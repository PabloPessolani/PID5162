BITACORA DVS - 20200720

Objetivos:
1) reemplazar el campo d_subver del DVS por d_flags para controlar el tipo de DVK disponibles para utilizar (IOCTL-IPC)
2) incorporar nombre al DVS
3) Si el IPC esta instalado, el modulo IOCTL utiliza esas funciones del kernel. 
4) Permitir la utilizacion de las dvk_calls directamente desde el kernel
	PROBLEMA QUE VAMOS A ENFRETAR: Como copiar datos desde el kernel de un proceso al modo usuario de otro ?????

COMPILAR EL KERNEL LINUX
=======================

sudo apt-get update
sudo apt-get install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc
apt-get install linux-headers-686-pae

cp /boot/config-$(uname -r) .config
make menuconfig
sudo make -j 4 && sudo make modules_install -j 4 && sudo make install -j 4
update-initramfs -c -k 4.9.88
update-grub  
update-grub2

  x Symbol: DVKIPC [=n]                                                                                x
  x Type  : boolean                                                                                    x
  x Prompt: Distributed Virtualization Kernel IPC                                                      x
  x   Location:                                                                                        x
  x     -> General setup                                                                               x
  x (1)   -> System V IPC (SYSVIPC [=y])                                                               x
  x Prompt: DVS's DVK embedded in kernel using IPC                                                     x
  x   Location:                                                                                        x
  x (2) -> Processor type and features                                                                 x
  x   Defined at init/Kconfig:254                                                                      x
  x   Depends on: SYSVIPC [=y]                                                                         x
  x                             
 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EN EL MAN DE IPC 
       On some architectures—for example x86-64 and ARM—there is no ipc()
       system call !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

===============================================================================================================
20200720:
    1) reemplazar el campo d_subver del DVS por d_flags para controlar el tipo de DVK disponibles para utilizar (IOCTL-IPC)
	2) incorporar nombre al DVS

MIS_BIT_KTHREAD: indica que el proceso es un Kernel Thread o está haciendo una invocación desde el kernel.

	Se cambio la estructura dvs_usr_t para que contenga el nombre del DVS Cluster y los flags que indica que 
	tipo de interfaces estan disponibles.
	
	Se cambiaron, por ahora, las dos versiones de dvk_hyper.c new_dc_init para que contemple las modificaciones
	Tambien se cambio el dvk_procfs.c de ambas versiones.
	
version IOCTL:
		Initialiting DVS: Enter para continuar... 
		DEBUG 651:dvk_open:106: Open dvk device file /dev/dvk
		ERROR: 651:dvk_open:109: rcode=0
		Initializing DVS. Local node ID 0... 
		DEBUG 651:dvk_dvs_init:542: nodeid=0
		DEBUG 651:dvk_dvs_init:554: ioctl ret=0 errno=0
		DEBUG 651:dvk_dvs_init:559: ioctl ret=0
		d_name=DVS_CLUSTER1 d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64  <<<< DVS_CLUSTER1
		d_max_copybuf=65536 d_max_copylen=1048576
		d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
		Get DVS info
		DEBUG 651:dvk_getdvsinfo:248: 
		DEBUG 651:dvk_getdvsinfo:257: ioctl ret=0 errno=0
		DEBUG 651:dvk_getdvsinfo:262: ioctl ret=0
		local node ID 0... 
		d_name=DVS_CLUSTER1 d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		d_max_copybuf=65536 d_max_copylen=1048576
		d_dbglvl=FFFFFF version=5 flags=2 sizeof(proc)=512 <<<<<<<<<<< flags=2 => BIT_IOCTL
		
		root@node0:/usr/src/dvs/scripts# cd /proc/dvs/
		root@node0:/proc/dvs# ls -l
		total 0
		-r--r--r-- 1 root root 0 jul 20 14:44 info
		-r--r--r-- 1 root root 0 jul 20 14:44 nodes
		dr-xr-xr-x 2 root root 0 jul 20 14:44 proxies
		root@node0:/proc/dvs# more info 
		name=DVS_CLUSTER1 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
		nodeid=0
		nr_dcs=32
		nr_nodes=32
		max_nr_procs=221
		max_nr_tasks=35
		max_sys_procs=64
		max_copy_buf=65536
		max_copy_len=1048576
		dbglvl=FFFFFF
		version=5 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 
		flags=2 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
		sizeof(proc)=504
		sizeof(proc) aligned=512
		sizeof(dc)=156
		sizeof(node)=92

PARA HOMOGENEIZAR UN UNICO CODIGO 
		Traslade el directorio dvk-mod debajo del /usr/src/linux/ipc 
		De tal modo que el IPC utilice esos objetos creados con el modulo
		
	    AL COMPILAR EL KERNEL 
		root@node0:/usr/src/linux/ipc/dvk-mod# ls -lt *.o
			Estos son los programas que compila el KERNEL 
				-rw-r--r-- 1 root root  218172 jul 20 18:35 dvk_sproxy.o
				-rw-r--r-- 1 root root  181844 jul 20 18:35 dvk_acks.o
				-rw-r--r-- 1 root root  244928 jul 20 18:35 dvk_utils.o
				-rw-r--r-- 1 root root  200500 jul 20 18:35 dvk_procfs.o
				-rw-r--r-- 1 root root  187068 jul 20 18:35 dvk_debugfs.o
				-rw-r--r-- 1 root root  209712 jul 20 18:35 dvk_migrate.o
				-rw-r--r-- 1 root root  427380 jul 20 18:35 dvk_hyper.o
				-rw-r--r-- 1 root root  311872 jul 20 18:35 dvk_ipc.o
				-rw-r--r-- 1 root root  233148 jul 20 18:35 dvk_rproxy.o
			Estos son los programas que compila el MODULO  		
				-rw-r--r-- 1 root root   71484 jul 20 18:29 dvk.mod.o
				-rw-r--r-- 1 root root 2918280 jul 20 18:29 dvk.o
				-rw-r--r-- 1 root root  178824 jul 20 18:28 reljmp_core.o
				-rw-r--r-- 1 root root  207048 jul 20 18:28 dvk_decode.o
				-rw-r--r-- 1 root root  172168 jul 20 18:28 dvk_newcall.o
				-rw-r--r-- 1 root root  197056 jul 20 18:28 main.o
		Asi que en teoria, hay que compilar el kernel con CONFIG_DVKIPC  y automaticamente 
		se compilan algunos programas del directorio del dvk-mod 
		Si solo hay que compilar el MODULO, entonces ejecutar make en el directorio dvk-mod 

===============================================================================================================
20200721:
		Se modifico le main.c de ipc/dvk-mod que verifica si ya existe la estructura dvs en el kernel
		Si existe, entonces se setea dvs_ptr hacia la variable ya existente en el kernel

		Se creo test_dvs_info linkeado con libdvkipc.o para ver si ya el kernel tiene configurado 
		
PROBLEMA: Cuando se quiere cambiar algo del DVK se tiene que recompilar el kernel COMPLETO!! 
	SOLUCION: Proximas versiones compilarsas sin soporte de CONFIG_DVKIPC.
	
			root@node0:/usr/src/dvs/dvk-tests# ./test_dvs_info 
			DEBUG 610:dvk_open:102: 
			Get DVS info
			DEBUG 610:dvk_getdvsinfo:248: 
			DEBUG 610:dvk_getdvsinfo:252: ipc ret=-1
			ERROR: 610:dvk_getdvsinfo:253: rcode=-1
			ERROR: 610:dvk_getdvsinfo:264: rcode=-1
			local node ID -1... 
			d_name=DVS_IPC d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64  <<<<<<<<<<< nombre DVS_IPC
			d_max_copybuf=65536 d_max_copylen=1048576
			d_dbglvl=7F306 version=5 flags=1 sizeof(proc)=0											<<<<<<<<<<< flags=1 => BIT_IPC			
			root@node0:/usr/src/dvs/dvk-tests# cd ..
			root@node0:/usr/src/dvs# cd ..
			root@node0:/usr/src# cd dvs/scripts/
			root@node0:/usr/src/dvs/scripts# ./rtest.sh t 0
			lcl_nodeid=0
			TCP proxies selected
			./rtest.sh: lÃ­nea 94:   616 Abortado        /usr/local/sbin/spread -c /etc/spread.conf > spread.txt
			dvk                   258048  0
			Initialiting DVS: Enter para continuar... 
			DEBUG 624:dvk_open:102: 
			Initializing DVS. Local node ID 0... 
			DEBUG 624:dvk_dvs_init:542: nodeid=0
			DEBUG 624:dvk_dvs_init:546: ipc ipc_op=71680 ret=0
			d_name=DVS_CLUSTER1 d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			d_max_copybuf=65536 d_max_copylen=1048576
			d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
			Get DVS info
			DEBUG 624:dvk_getdvsinfo:248: 
			DEBUG 624:dvk_getdvsinfo:252: ipc ret=0
			local node ID 0...  
			d_name=DVS_CLUSTER1 d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 <<<<<<<<<<<< nombre DVS_CLUSTER1
			d_max_copybuf=65536 d_max_copylen=1048576
			d_dbglvl=FFFFFF version=5 flags=101 sizeof(proc)=512					<<<<<<<<<<< flags=0x0101 => (BIT_IPC | BIT_INITIALIZED)


===============================================================================================================
20200722:
	privilegios
	     Los procesos de usuario >= NR_SYSPROCS, por default no tienen privilegios de enviar a ningun proceso
		 Los procesos de systema < NR_SYSPROCS, por default pueden enviarle a todos los procesos.

EJECUTANDO:
		root@node0:~# cd /usr/src/dvs/dvk-tests/
		root@node0:/usr/src/dvs/dvk-tests# ./tests.sh 0 0
		lcl_nodeid=0 dcid=0
		Enter para continuar... 
		Spread Enter para continuar... 
		./tests.sh: lÃ­nea 17:   629 Abortado                /usr/local/sbin/spread -c /etc/spread.conf > /dev/shm/spread.txt  (dir ahora: /usr/src/dvs/dvk-tests)
		mount Enter para continuar... 
		partition 5
		mount Enter para continuar... 
		local_nodeid=0 Enter para continuar... 
		DEBUG 632:dvk_open:102: 
		Initializing DVS. Local node ID 0... 
		DEBUG 632:dvk_dvs_init:542: nodeid=0
		DEBUG 632:dvk_dvs_init:546: ipc ipc_op=71680 ret=0
		d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		d_max_copybuf=65536 d_max_copylen=1048576
		d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
		Get DVS info
		DEBUG 632:dvk_getdvsinfo:248: 
		DEBUG 632:dvk_getdvsinfo:252: ipc ret=0
		local node ID 0... 
		d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		d_max_copybuf=65536 d_max_copylen=1048576
		d_dbglvl=FFFFFF version=5 flags=101 sizeof(proc)=512
		DC0 Enter para continuar... 
		TCP PROXY Enter para continuar... ^C
		root@node0:/usr/src/dvs/dvk-tests# . /dev/shm/DC0.sh 
		root@node0:/usr/src/dvs/dvk-tests# ./test_sendrec 
		Usage: ./test_sendrec <dcid> <parent_nr> <child_nr>  
		root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_sendrec 0 11 12
		DEBUG 2:dvk_open:102: 
		DEBUG 3:dvk_bind_X:1199: cmd=0 dcid=0 pid=-1 endpoint=12 nodeid=-1
		DEBUG 2:dvk_bind_X:1199: cmd=0 dcid=0 pid=-1 endpoint=11 nodeid=-1
		DEBUG 2:dvk_bind_X:1203: ipc ret=11 errno=0
		PARENT BIND dcid=0 parent_pid=2 parent_nr=11 parent_ep=11 m_ptr=0x1127008
		PARENT pause before SENDREC
		DEBUG 3:dvk_bind_X:1203: ipc ret=12 errno=0
		CHILD BIND dcid=0 child_pid=3 child_nr=12 child_ep=12 m_ptr=0x1127008
		CHILD FIRST RECEIVE
		DEBUG 3:dvk_receive_T:876: endpoint=31438 timeout=-1
		PARENT FIRST SENDREC msg:source=0 type=10 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		DEBUG 2:dvk_sendrec_T:910: endpoint=12 timeout=-1
		DEBUG 2:dvk_sendrec_T:914: ipc ret=-1
		ERROR: 2:dvk_sendrec_T:915: rcode=-110
		ERROR: 2:dvk_sendrec_T:934: rcode=-110
		ERROR: test_sendrec.c:main:55: rcode=-110
		PARENT FIRST REPLY msg:source=0 type=10 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		PARENT SECOND SENDREC msg:source=0 type=11 m1i1=5 m1i2=6 m1i3=7 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		DEBUG 2:dvk_sendrec_T:910: endpoint=12 timeout=-1
		DEBUG 2:dvk_sendrec_T:914: ipc ret=-1
		ERROR: 2:dvk_sendrec_T:915: rcode=-110
		ERROR: 2:dvk_sendrec_T:934: rcode=-110
		ERROR: test_sendrec.c:main:65: rcode=-110
		PARENT SECOND REPLY msg:source=0 type=11 m1i1=5 m1i2=6 m1i3=7 m1p1=(nil) m1p2=(nil) m1p3=(nil) 

EL DMESG
		[  106.660052] DEBUG 647:SYSC_ipc:163: ipc_bind: call=2304 first=0 second=0 third=-1 fifth=-1 
		[  106.660057] DEBUG 647:ipc_bind:185: oper=0 dcid=0 param_pid=-1 endpoint=11 nodeid=-1
		[  106.660061] DEBUG new_bind:1394: oper=0 dcid=0 param_pid=-1 endpoint=11 nodeid=-1
		[  106.660063] DEBUG new_bind:1416: dc_ptr=cca754a0
		[  106.660064] DEBUG new_bind:1418: RLOCK_DC dc=0 count=0
		[  106.660066] DEBUG new_bind:1431: proc_ptr=e79a5a00
		[  106.660067] DEBUG new_bind:1432: WLOCK_PROC ep=11 count=0
		[  106.660069] DEBUG init_proc_desc:16: p_name=$noname dcid=0
		[  106.660070] DEBUG init_proc_desc:27: Clearing Privileges
		[  106.660071] DEBUG init_proc_desc:35: Clearing Process fields
		[  106.660075] DEBUG new_bind:1468: param_pid=-1 lpid=647 vpid=2 tid=647
		[  106.660076] DEBUG new_bind:1492: SELF_BIND param_pid=-1 lpid=647 vpid=2 tid=647
		[  106.660078] DEBUG new_bind:1514: WUNLOCK_PROC ep=11 count=0
		[  106.660079] DEBUG new_bind:1518: WLOCK_TASK pid=647 count=0
		[  106.660080] DEBUG new_bind:1519: WLOCK_PROC ep=11 count=0
		[  106.660081] DEBUG new_bind:1556: increment the reference count of the task struct=647 count=2
		[  106.660083] DEBUG new_bind:1567: process p_name=test_sendrec *p_name_ptr=test_sendrec
		[  106.660084] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 647:new_bind:1630:proc_ptr->p_priv.priv_usr.priv_ipc_to:
		[  106.660086] DEBUG new_bind:1630: FF.
		[  106.660087] DEBUG new_bind:1630: FF.
		[  106.660088] DEBUG new_bind:1630: FF.
		[  106.660089] DEBUG new_bind:1630: FF.
		[  106.660089] DEBUG new_bind:1630: 
		[  106.660091] DEBUG new_bind:1631: PRINT_DVK_MAP: 647:new_bind:1631:proc_ptr->p_priv.priv_usr.priv_dvk_allowed:
		[  106.660092] DEBUG new_bind:1631: FF.
		[  106.660093] DEBUG new_bind:1631: FF.
		[  106.660093] DEBUG new_bind:1631: FF.
		[  106.660094] DEBUG new_bind:1631: 
		[  106.660097] DEBUG new_bind:1635: nr=11 endp=11 dcid=0 flags=0 misc=20 lpid=647 vpid=2 nodeid=0 name=test_sendrec 
		[  106.660099] DEBUG new_bind:1636: nr=11 endp=11 dcid=0 lpid=647 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[  106.660100] DEBUG new_bind:1650: WUNLOCK_TASK pid=647 count=0
		[  106.660101] DEBUG new_bind:1656: WUNLOCK_PROC ep=11 count=0
		[  106.660102] DEBUG new_bind:1658: DC_INCREF counter=1
		[  106.660104] DEBUG new_bind:1659: RUNLOCK_DC dc=0 count=0
		[  106.660113] DEBUG 648:SYSC_ipc:163: ipc_bind: call=2304 first=0 second=0 third=-1 fifth=-1 
		[  106.660118] DEBUG 648:ipc_bind:185: oper=0 dcid=0 param_pid=-1 endpoint=12 nodeid=-1
		[  106.660122] DEBUG new_bind:1394: oper=0 dcid=0 param_pid=-1 endpoint=12 nodeid=-1
		[  106.660124] DEBUG new_bind:1416: dc_ptr=cca754a0
		[  106.660126] DEBUG new_bind:1418: RLOCK_DC dc=0 count=0
		[  106.660128] DEBUG new_bind:1431: proc_ptr=e79a5c00
		[  106.660129] DEBUG new_bind:1432: WLOCK_PROC ep=12 count=0
		[  106.660131] DEBUG init_proc_desc:16: p_name=$noname dcid=0
		[  106.660132] DEBUG init_proc_desc:27: Clearing Privileges
		[  106.660133] DEBUG init_proc_desc:35: Clearing Process fields
		[  106.660136] DEBUG new_bind:1468: param_pid=-1 lpid=648 vpid=3 tid=648
		[  106.660138] DEBUG new_bind:1492: SELF_BIND param_pid=-1 lpid=648 vpid=3 tid=648
		[  106.660139] DEBUG new_bind:1514: WUNLOCK_PROC ep=12 count=0
		[  106.660141] DEBUG new_bind:1518: WLOCK_TASK pid=648 count=0
		[  106.660142] DEBUG new_bind:1519: WLOCK_PROC ep=12 count=0
		[  106.660143] DEBUG new_bind:1556: increment the reference count of the task struct=648 count=2
		[  106.660144] DEBUG new_bind:1567: process p_name=test_sendrec *p_name_ptr=test_sendrec
		[  106.660146] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 648:new_bind:1630:proc_ptr->p_priv.priv_usr.priv_ipc_to:
		[  106.660148] DEBUG new_bind:1630: FF.
		[  106.660149] DEBUG new_bind:1630: FF.
		[  106.660150] DEBUG new_bind:1630: FF.
		[  106.660151] DEBUG new_bind:1630: FF.
		[  106.660151] DEBUG new_bind:1630: 
		[  106.660153] DEBUG new_bind:1631: PRINT_DVK_MAP: 648:new_bind:1631:proc_ptr->p_priv.priv_usr.priv_dvk_allowed:
		[  106.660154] DEBUG new_bind:1631: FF.
		[  106.660155] DEBUG new_bind:1631: FF.
		[  106.660156] DEBUG new_bind:1631: FF.
		[  106.660156] DEBUG new_bind:1631: 
		[  106.660159] DEBUG new_bind:1635: nr=12 endp=12 dcid=0 flags=0 misc=20 lpid=648 vpid=3 nodeid=0 name=test_sendrec 
		[  106.660161] DEBUG new_bind:1636: nr=12 endp=12 dcid=0 lpid=648 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[  106.660162] DEBUG new_bind:1650: WUNLOCK_TASK pid=648 count=0
		[  106.660163] DEBUG new_bind:1656: WUNLOCK_PROC ep=12 count=0
		[  106.660164] DEBUG new_bind:1658: DC_INCREF counter=2
		[  106.660165] DEBUG new_bind:1659: RUNLOCK_DC dc=0 count=0
		[  106.660254] DEBUG 648:SYSC_ipc:163: ipc_mini_receive: call=768 first=31438 second=0 third=0 fifth=-1 
		[  106.660256] DEBUG 648:ipc_mini_receive:74: src_ep=31438 timeout_ms=-1
		[  106.660257] DEBUG new_mini_receive:303: src_ep=31438
		[  106.660259] DEBUG check_caller:533: caller_pid=648 caller_tgid=648
		[  106.660260] DEBUG check_caller:569: WLOCK_PROC ep=12 count=0
		[  106.660261] DEBUG check_caller:599: WUNLOCK_PROC ep=12 count=0
		[  106.660262] DEBUG check_caller:602: dcid=0
		[  106.660264] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[  106.660265] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[  106.660266] DEBUG check_caller:616: caller_pid=648 
		[  106.660267] DEBUG new_mini_receive:314: RLOCK_PROC ep=12 count=0
		[  106.660269] DEBUG new_mini_receive:318: caller_nr=12 caller_ep=12 src_ep=31438 
		[  106.660270] DEBUG new_mini_receive:322: dcid=0
		[  106.660271] DEBUG new_mini_receive:330: RUNLOCK_PROC ep=12 count=0
		[  106.660272] DEBUG new_mini_receive:332: RLOCK_DC dc=0 count=0
		[  106.660273] DEBUG new_mini_receive:335: RUNLOCK_DC dc=0 count=0
		[  106.660274] DEBUG new_mini_receive:385: WLOCK_PROC ep=12 count=0
		[  106.660276] DEBUG new_mini_receive:490: Any suitable message from 31438 was not found.
		[  106.660277] DEBUG sleep_proc:341: timeout=-1
		[  106.660278] DEBUG sleep_proc:352: BEFORE DOWN lpid=648 p_sem=0 timeout=-1
		[  106.660279] DEBUG sleep_proc:354: endpoint=12 flags=8
		[  106.660280] DEBUG sleep_proc:358: WUNLOCK_PROC ep=12 count=0
		
		[  109.661328] DEBUG 647:SYSC_ipc:163: ipc_mini_sendrec: call=1280 first=12 second=0 third=0 fifth=-1 
		[  109.661335] DEBUG 647:ipc_mini_sendrec:106: srcdst_ep=12 timeout_ms=-1
		[  109.661337] DEBUG new_mini_sendrec:527: srcdst_ep=12
		[  109.661340] DEBUG check_caller:533: caller_pid=647 caller_tgid=647
		[  109.661343] DEBUG check_caller:569: WLOCK_PROC ep=11 count=0
		[  109.661345] DEBUG check_caller:599: WUNLOCK_PROC ep=11 count=0
		[  109.661346] DEBUG check_caller:602: dcid=0
		[  109.661349] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[  109.661351] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[  109.661353] DEBUG check_caller:616: caller_pid=647 
		[  109.661355] DEBUG new_mini_sendrec:538: RLOCK_PROC ep=11 count=0
		[  109.661357] DEBUG new_mini_sendrec:543: caller_nr=11 caller_ep=11 srcdst_ep=12 
		[  109.661359] DEBUG new_mini_sendrec:550: RUNLOCK_PROC ep=11 count=0
		[  109.661361] DEBUG new_mini_sendrec:552: dcid=0
		[  109.661363] DEBUG new_mini_sendrec:556: RLOCK_DC dc=0 count=0
		[  109.661364] DEBUG new_mini_sendrec:560: RUNLOCK_DC dc=0 count=0
		[  109.661366] DEBUG new_mini_sendrec:575: RLOCK_PROC ep=11 count=0
		[  109.661368] DEBUG new_mini_sendrec:579: RUNLOCK_PROC ep=11 count=0
		[  109.661371] ERROR: 647:new_mini_sendrec:579: rcode=-110
		
		[  109.661533] DEBUG 647:SYSC_ipc:163: ipc_mini_sendrec: call=1280 first=12 second=0 third=0 fifth=-1 
		[  109.661536] DEBUG 647:ipc_mini_sendrec:106: srcdst_ep=12 timeout_ms=-1
		[  109.661538] DEBUG new_mini_sendrec:527: srcdst_ep=12
		[  109.661540] DEBUG check_caller:533: caller_pid=647 caller_tgid=647
		[  109.661542] DEBUG check_caller:569: WLOCK_PROC ep=11 count=0
		[  109.661544] DEBUG check_caller:599: WUNLOCK_PROC ep=11 count=0
		[  109.661545] DEBUG check_caller:602: dcid=0
		[  109.661547] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[  109.661549] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[  109.661550] DEBUG check_caller:616: caller_pid=647 
		[  109.661552] DEBUG new_mini_sendrec:538: RLOCK_PROC ep=11 count=0
		[  109.661554] DEBUG new_mini_sendrec:543: caller_nr=11 caller_ep=11 srcdst_ep=12 
		[  109.661556] DEBUG new_mini_sendrec:550: RUNLOCK_PROC ep=11 count=0
		[  109.661558] DEBUG new_mini_sendrec:552: dcid=0
		[  109.661559] DEBUG new_mini_sendrec:556: RLOCK_DC dc=0 count=0
		[  109.661561] DEBUG new_mini_sendrec:560: RUNLOCK_DC dc=0 count=0
		[  109.661563] DEBUG new_mini_sendrec:575: RLOCK_PROC ep=11 count=0
		[  109.661565] DEBUG new_mini_sendrec:579: RUNLOCK_PROC ep=11 count=0
		[  109.661567] ERROR: 647:new_mini_sendrec:579: rcode=-110 <<<<<  EDVSTRAPDENIED


===============================================================================================================
20200723:

EJECUTANDO:
		Root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_sendrec 0 11 12
		DEBUG 2:dvk_open:102: 
		DEBUG 2:dvk_bind_X:1199: cmd=0 dcid=0 pid=-1 endpoint=11 nodeid=-1
		DEBUG 2:dvk_bind_X:1203: ipc ret=11 errno=0
		PARENT BIND dcid=0 parent_pid=2 parent_nr=11 parent_ep=11 m_ptr=0x15f4008
		PARENT pause before SENDREC
		DEBUG 3:dvk_bind_X:1199: cmd=0 dcid=0 pid=-1 endpoint=12 nodeid=-1
		DEBUG 3:dvk_bind_X:1203: ipc ret=12 errno=0
		CHILD BIND dcid=0 child_pid=3 child_nr=12 child_ep=12 m_ptr=0x15f4008
		CHILD FIRST RECEIVE
		DEBUG 3:dvk_receive_T:876: endpoint=31438 timeout=-1
		PARENT FIRST SENDREC msg:source=0 type=10 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		DEBUG 2:dvk_sendrec_T:910: endpoint=12 timeout=-1
		DEBUG 3:dvk_receive_T:880: ipc ret=0
		CHILD RECEIVE msg:source=11 type=10 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		CHILD FIRST SEND
		DEBUG 3:dvk_send_T:843: endpoint=11 timeout=-1
		DEBUG 3:dvk_send_T:847: ipc ret=76
		DEBUG 2:dvk_sendrec_T:914: ipc ret=0
		PARENT FIRST REPLY msg:source=12 type=254 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		PARENT SECOND SENDREC msg:source=12 type=11 m1i1=5 m1i2=6 m1i3=7 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		DEBUG 2:dvk_sendrec_T:910: endpoint=12 timeout=-1
		CHILD SECOND RECEIVE
		DEBUG 3:dvk_receive_T:876: endpoint=31438 timeout=-1
		DEBUG 3:dvk_receive_T:880: ipc ret=0
		CHILD RECEIVE msg:source=11 type=11 m1i1=5 m1i2=6 m1i3=7 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		CHILD SECOND SEND
		DEBUG 3:dvk_send_T:843: endpoint=11 timeout=-1
		DEBUG 3:dvk_send_T:847: ipc ret=76
		DEBUG 2:dvk_sendrec_T:914: ipc ret=0
		PARENT SECOND REPLY msg:source=12 type=239 m1i1=5 m1i2=6 m1i3=7 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		DEBUG 2:dvk_getprocinfo:1156: dcid=0 p_nr=11 
		DEBUG 2:dvk_getprocinfo:1160: ipc ret=0
		nr=11 endp=11 dcid=0 flags=0 misc=20 lpid=658 vpid=2 nodeid=0 name=test_sendrec 
		endp=11 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
		nr=11 endp=11 dcid=0 p_lclsent=2 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 

DMESG 
		[   62.066811] DEBUG 658:SYSC_ipc:163: ipc_bind: call=2304 first=0 second=0 third=-1 fifth=-1 
		[   62.066816] DEBUG 658:ipc_bind:185: oper=0 dcid=0 param_pid=-1 endpoint=11 nodeid=-1
		[   62.066820] DEBUG new_bind:1394: oper=0 dcid=0 param_pid=-1 endpoint=11 nodeid=-1
		[   62.066822] DEBUG new_bind:1416: dc_ptr=d7a754a0
		[   62.066824] DEBUG new_bind:1418: RLOCK_DC dc=0 count=0
		[   62.066825] DEBUG new_bind:1431: proc_ptr=e7925a00
		[   62.066826] DEBUG new_bind:1432: WLOCK_PROC ep=11 count=0
		[   62.066828] DEBUG init_proc_desc:16: p_name=$noname dcid=0
		[   62.066830] DEBUG init_proc_desc:27: Clearing Privileges
		[   62.066830] DEBUG init_proc_desc:35: Clearing Process fields
		[   62.066833] DEBUG new_bind:1468: param_pid=-1 lpid=658 vpid=2 tid=658
		[   62.066835] DEBUG new_bind:1492: SELF_BIND param_pid=-1 lpid=658 vpid=2 tid=658
		[   62.066837] DEBUG new_bind:1514: WUNLOCK_PROC ep=11 count=0
		[   62.066838] DEBUG new_bind:1518: WLOCK_TASK pid=658 count=0
		[   62.066839] DEBUG new_bind:1519: WLOCK_PROC ep=11 count=0
		[   62.066840] DEBUG new_bind:1556: increment the reference count of the task struct=658 count=2
		[   62.066842] DEBUG new_bind:1567: process p_name=test_sendrec *p_name_ptr=test_sendrec
		[   62.066843] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 658:new_bind:1630:proc_ptr->p_priv.priv_usr.priv_ipc_to:
		[   62.066845] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 658:new_bind:1630:4:
		[   62.066846] DEBUG new_bind:1630: FFFF.
		[   62.066848] DEBUG new_bind:1630: FFFF.
		[   62.066848] DEBUG new_bind:1630: FFFF.
		[   62.066849] DEBUG new_bind:1630: FFFF.
		[   62.066850] DEBUG new_bind:1630: 
		[   62.066852] DEBUG new_bind:1631: PRINT_DVK_MAP: 658:new_bind:1631:proc_ptr->p_priv.priv_usr.priv_dvk_allowed:
		[   62.066853] DEBUG new_bind:1631: PRINT_IPCTO_MAP: 658:new_bind:1631:3:
		[   62.066854] DEBUG new_bind:1631: FF.
		[   62.066855] DEBUG new_bind:1631: FF.
		[   62.066856] DEBUG new_bind:1631: FF.
		[   62.066857] DEBUG new_bind:1631: 
		[   62.066859] DEBUG new_bind:1635: nr=11 endp=11 dcid=0 flags=0 misc=20 lpid=658 vpid=2 nodeid=0 name=test_sendrec 
		[   62.066861] DEBUG new_bind:1636: nr=11 endp=11 dcid=0 lpid=658 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   62.066862] DEBUG new_bind:1650: WUNLOCK_TASK pid=658 count=0
		[   62.066863] DEBUG new_bind:1656: WUNLOCK_PROC ep=11 count=0
		[   62.066864] DEBUG new_bind:1658: DC_INCREF counter=1
		[   62.066865] DEBUG new_bind:1659: RUNLOCK_DC dc=0 count=0
		[   62.067359] DEBUG 659:SYSC_ipc:163: ipc_bind: call=2304 first=0 second=0 third=-1 fifth=-1 
		[   62.067403] DEBUG 659:ipc_bind:185: oper=0 dcid=0 param_pid=-1 endpoint=12 nodeid=-1
		[   62.067408] DEBUG new_bind:1394: oper=0 dcid=0 param_pid=-1 endpoint=12 nodeid=-1
		[   62.067410] DEBUG new_bind:1416: dc_ptr=d7a754a0
		[   62.067412] DEBUG new_bind:1418: RLOCK_DC dc=0 count=0
		[   62.067413] DEBUG new_bind:1431: proc_ptr=e7925c00
		[   62.067415] DEBUG new_bind:1432: WLOCK_PROC ep=12 count=0
		[   62.067417] DEBUG init_proc_desc:16: p_name=$noname dcid=0
		[   62.067418] DEBUG init_proc_desc:27: Clearing Privileges
		[   62.067418] DEBUG init_proc_desc:35: Clearing Process fields
		[   62.067422] DEBUG new_bind:1468: param_pid=-1 lpid=659 vpid=3 tid=659
		[   62.067424] DEBUG new_bind:1492: SELF_BIND param_pid=-1 lpid=659 vpid=3 tid=659
		[   62.067425] DEBUG new_bind:1514: WUNLOCK_PROC ep=12 count=0
		[   62.067427] DEBUG new_bind:1518: WLOCK_TASK pid=659 count=0
		[   62.067428] DEBUG new_bind:1519: WLOCK_PROC ep=12 count=0
		[   62.067429] DEBUG new_bind:1556: increment the reference count of the task struct=659 count=2
		[   62.067430] DEBUG new_bind:1567: process p_name=test_sendrec *p_name_ptr=test_sendrec
		[   62.067432] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 659:new_bind:1630:proc_ptr->p_priv.priv_usr.priv_ipc_to:
		[   62.067434] DEBUG new_bind:1630: PRINT_IPCTO_MAP: 659:new_bind:1630:4:
		[   62.067435] DEBUG new_bind:1630: FFFF.
		[   62.067436] DEBUG new_bind:1630: FFFF.
		[   62.067437] DEBUG new_bind:1630: FFFF.
		[   62.067438] DEBUG new_bind:1630: FFFF.
		[   62.067439] DEBUG new_bind:1630: 
		[   62.067440] DEBUG new_bind:1631: PRINT_DVK_MAP: 659:new_bind:1631:proc_ptr->p_priv.priv_usr.priv_dvk_allowed:
		[   62.067441] DEBUG new_bind:1631: PRINT_IPCTO_MAP: 659:new_bind:1631:3:
		[   62.067442] DEBUG new_bind:1631: FF.
		[   62.067443] DEBUG new_bind:1631: FF.
		[   62.067444] DEBUG new_bind:1631: FF.
		[   62.067445] DEBUG new_bind:1631: 
		[   62.067447] DEBUG new_bind:1635: nr=12 endp=12 dcid=0 flags=0 misc=20 lpid=659 vpid=3 nodeid=0 name=test_sendrec 
		[   62.067449] DEBUG new_bind:1636: nr=12 endp=12 dcid=0 lpid=659 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   62.067450] DEBUG new_bind:1650: WUNLOCK_TASK pid=659 count=0
		[   62.067452] DEBUG new_bind:1656: WUNLOCK_PROC ep=12 count=0
		[   62.067453] DEBUG new_bind:1658: DC_INCREF counter=2
		[   62.067454] DEBUG new_bind:1659: RUNLOCK_DC dc=0 count=0
		[   62.067514] DEBUG 659:SYSC_ipc:163: ipc_mini_receive: call=768 first=31438 second=0 third=0 fifth=-1 
		[   62.067516] DEBUG 659:ipc_mini_receive:74: src_ep=31438 timeout_ms=-1
		[   62.067518] DEBUG new_mini_receive:303: src_ep=31438
		[   62.067550] DEBUG check_caller:533: caller_pid=659 caller_tgid=659
		[   62.067552] DEBUG check_caller:569: WLOCK_PROC ep=12 count=0
		[   62.067553] DEBUG check_caller:599: WUNLOCK_PROC ep=12 count=0
		[   62.067554] DEBUG check_caller:602: dcid=0
		[   62.067556] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   62.067557] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   62.067558] DEBUG check_caller:616: caller_pid=659 
		[   62.067559] DEBUG new_mini_receive:314: RLOCK_PROC ep=12 count=0
		[   62.067561] DEBUG new_mini_receive:318: caller_nr=12 caller_ep=12 src_ep=31438 
		[   62.067562] DEBUG new_mini_receive:322: dcid=0
		[   62.067563] DEBUG new_mini_receive:330: RUNLOCK_PROC ep=12 count=0
		[   62.067564] DEBUG new_mini_receive:332: RLOCK_DC dc=0 count=0
		[   62.067565] DEBUG new_mini_receive:335: RUNLOCK_DC dc=0 count=0
		[   62.067566] DEBUG new_mini_receive:385: WLOCK_PROC ep=12 count=0
		[   62.067568] DEBUG new_mini_receive:490: Any suitable message from 31438 was not found.
		[   62.067569] DEBUG sleep_proc:341: timeout=-1
		[   62.067571] DEBUG sleep_proc:352: BEFORE DOWN lpid=659 p_sem=0 timeout=-1
		[   62.067572] DEBUG sleep_proc:354: endpoint=12 flags=8
		[   62.067573] DEBUG sleep_proc:358: WUNLOCK_PROC ep=12 count=0
		[   65.068118] DEBUG 658:SYSC_ipc:163: ipc_mini_sendrec: call=1280 first=12 second=0 third=0 fifth=-1 
		[   65.068125] DEBUG 658:ipc_mini_sendrec:106: srcdst_ep=12 timeout_ms=-1
		[   65.068128] DEBUG new_mini_sendrec:527: srcdst_ep=12
		[   65.068131] DEBUG check_caller:533: caller_pid=658 caller_tgid=658
		[   65.068134] DEBUG check_caller:569: WLOCK_PROC ep=11 count=0
		[   65.068136] DEBUG check_caller:599: WUNLOCK_PROC ep=11 count=0
		[   65.068137] DEBUG check_caller:602: dcid=0
		[   65.068140] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   65.068142] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   65.068144] DEBUG check_caller:616: caller_pid=658 
		[   65.068146] DEBUG new_mini_sendrec:538: RLOCK_PROC ep=11 count=0
		[   65.068148] DEBUG new_mini_sendrec:543: caller_nr=11 caller_ep=11 srcdst_ep=12 
		[   65.068150] DEBUG new_mini_sendrec:550: RUNLOCK_PROC ep=11 count=0
		[   65.068152] DEBUG new_mini_sendrec:552: dcid=0
		[   65.068154] DEBUG new_mini_sendrec:556: RLOCK_DC dc=0 count=0
		[   65.068155] DEBUG new_mini_sendrec:560: RUNLOCK_DC dc=0 count=0
		[   65.068157] DEBUG new_mini_sendrec:575: RLOCK_PROC ep=11 count=0
		[   65.068159] DEBUG new_mini_sendrec:586: RUNLOCK_PROC ep=11 count=0
		[   65.068161] DEBUG new_mini_sendrec:592: WLOCK_PROC ep=12 count=0
		[   65.068163] DEBUG new_mini_sendrec:594: srcdst_nr=12 srcdst_ep=12
		[   65.068165] DEBUG new_mini_sendrec:614: srcdst_ptr->p_usr.p_nodeid=0
		[   65.068167] DEBUG new_mini_sendrec:618: RLOCK_DC dc=0 count=0
		[   65.068169] DEBUG new_mini_sendrec:622: RUNLOCK_DC dc=0 count=0
		[   65.068171] DEBUG new_mini_sendrec:637: dcid=0 caller_pid=658 caller_nr=11 srcdst_ep=12 
		[   65.068173] DEBUG new_mini_sendrec:642: SENDING HALF
		[   65.068174] DEBUG new_mini_sendrec:726: destination is waiting. Copy the message and wakeup destination
		[   65.068178] DEBUG copy_usr2usr:884: rqtr_ep=11 src_ep=11 src_lpid=658 src_vpid=2 src_addr=015f4008
		[   65.068182] DEBUG copy_usr2usr:887: dst_ep=12 dst_lpid=659 dst_vpid=3 dst_addr=015f4008 bytes=76
		[   65.068183] DEBUG copy_usr2usr:891: task_pid_nr(current)=658
		[   65.068185] DEBUG copy_usr2usr:894: WRITE
		[   65.068187] DEBUG copy_usr2usr:899: task_pid_nr(dst_ptr->p_task)=659
		[   65.068191] DEBUG dvk_vm_rw:442: pid=659 liovcnt=1 riovcnt=1 flags=0 vm_write=1
		[   65.068193] DEBUG dvk_check_iovect:1280: type=1 nr_segs=1
		[   65.068195] DEBUG dvk_rw_check_kvector:862: type=1 nr_segs=1
		[   65.068197] DEBUG dvk_rw_check_kvector:862: type=-1 nr_segs=1
		[   65.068200] DEBUG dvk_vm_rw_core:277: pid=659 riovcnt=1 flags=0 vm_write=1
		[   65.068207] DEBUG dvk_vm_rw_core:331: i=0 rc=0
		[   65.068227] DEBUG dvk_vm_rw_core:349: rc=76
		[   65.068229] DEBUG dvk_vm_rw_core:356: rc=76
		[   65.068231] DEBUG copy_usr2usr:952: len=76
		[   65.068233] DEBUG inherit_cpu:285: cpuid=1 vpid=3
		[   65.068245] DEBUG inherit_cpu:293: nr=12 endp=12 dcid=0 lpid=659 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   65.068247] DEBUG new_mini_sendrec:732: BEFORE UP lpid=659 p_sem=-1 rcode=76
		[   65.068263] DEBUG new_mini_sendrec:740: WUNLOCK_PROC ep=12 count=0
		[   65.068265] DEBUG sleep_proc:341: timeout=-1
		[   65.068267] DEBUG sleep_proc:352: BEFORE DOWN lpid=658 p_sem=0 timeout=-1
		[   65.068269] DEBUG sleep_proc:354: endpoint=11 flags=8
		[   65.068270] DEBUG sleep_proc:358: WUNLOCK_PROC ep=11 count=1
		[   65.068284] DEBUG sleep_proc:365: endpoint=12 ret=0 p_rcode=76
		[   65.068286] DEBUG sleep_proc:366: endpoint=12 flags=0 cpuid=1
		[   65.068288] DEBUG sleep_proc:367: WLOCK_PROC ep=12 count=0
		[   65.068291] DEBUG sleep_proc:414: nr=12 endp=12 dcid=0 lpid=659 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   65.068293] DEBUG sleep_proc:416: someone wakeups me: sem=0 p_rcode=0
		[   65.068295] DEBUG new_mini_receive:507: WUNLOCK_PROC ep=12 count=0
		[   65.068430] DEBUG 659:SYSC_ipc:163: ipc_mini_send: call=512 first=11 second=0 third=0 fifth=-1 
		[   65.068433] DEBUG 659:ipc_mini_send:52: dst_ep=11 timeout_ms=-1
		[   65.068434] DEBUG new_mini_send:32: dst_ep=11
		[   65.068437] DEBUG check_caller:533: caller_pid=659 caller_tgid=659
		[   65.068439] DEBUG check_caller:569: WLOCK_PROC ep=12 count=0
		[   65.068440] DEBUG check_caller:599: WUNLOCK_PROC ep=12 count=0
		[   65.068442] DEBUG check_caller:602: dcid=0
		[   65.068444] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   65.068446] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   65.068447] DEBUG check_caller:616: caller_pid=659 
		[   65.068449] DEBUG new_mini_send:44: RLOCK_PROC ep=12 count=0
		[   65.068452] DEBUG new_mini_send:48: caller_nr=12 caller_ep=12 dst_ep=11 
		[   65.068453] DEBUG new_mini_send:53: RUNLOCK_PROC ep=12 count=0
		[   65.068455] DEBUG new_mini_send:55: dcid=0
		[   65.068457] DEBUG new_mini_send:59: RLOCK_DC dc=0 count=0
		[   65.068458] DEBUG new_mini_send:64: RUNLOCK_DC dc=0 count=0
		[   65.068460] DEBUG new_mini_send:79: RLOCK_PROC ep=12 count=0
		[   65.068462] DEBUG new_mini_send:90: RUNLOCK_PROC ep=12 count=0
		[   65.068464] DEBUG new_mini_send:93: WUNLOCK_PROC ep=12 count=1
		[   65.068466] DEBUG new_mini_send:93: WLOCK_PROC ep=11 count=1
		[   65.068468] DEBUG new_mini_send:93: WLOCK_PROC ep=12 count=1
		[   65.068470] DEBUG new_mini_send:98: dst_nr=11 dst_ep=11
		[   65.068471] DEBUG new_mini_send:117: dst_ptr->p_usr.p_nodeid=0
		[   65.068473] DEBUG new_mini_send:121: RLOCK_DC dc=0 count=0
		[   65.068475] DEBUG new_mini_send:125: RUNLOCK_DC dc=0 count=0
		[   65.068477] DEBUG new_mini_send:140: dcid=0 caller_pid=659 caller_nr=12 dst_ep=11 
		[   65.068479] DEBUG new_mini_send:222: destination is waiting. Copy the message and wakeup destination
		[   65.068482] DEBUG copy_usr2usr:884: rqtr_ep=12 src_ep=12 src_lpid=659 src_vpid=3 src_addr=015f4008
		[   65.068485] DEBUG copy_usr2usr:887: dst_ep=11 dst_lpid=658 dst_vpid=2 dst_addr=015f4008 bytes=76
		[   65.068486] DEBUG copy_usr2usr:891: task_pid_nr(current)=659
		[   65.068488] DEBUG copy_usr2usr:894: WRITE
		[   65.068489] DEBUG copy_usr2usr:899: task_pid_nr(dst_ptr->p_task)=658
		[   65.068492] DEBUG dvk_vm_rw:442: pid=658 liovcnt=1 riovcnt=1 flags=0 vm_write=1
		[   65.068494] DEBUG dvk_check_iovect:1280: type=1 nr_segs=1
		[   65.068496] DEBUG dvk_rw_check_kvector:862: type=1 nr_segs=1
		[   65.068498] DEBUG dvk_rw_check_kvector:862: type=-1 nr_segs=1
		[   65.068500] DEBUG dvk_vm_rw_core:277: pid=658 riovcnt=1 flags=0 vm_write=1
		[   65.068505] DEBUG dvk_vm_rw_core:331: i=0 rc=0
		[   65.068518] DEBUG dvk_vm_rw_core:349: rc=76
		[   65.068520] DEBUG dvk_vm_rw_core:356: rc=76
		[   65.068522] DEBUG copy_usr2usr:952: len=76
		[   65.068524] DEBUG inherit_cpu:285: cpuid=1 vpid=2
		[   65.068531] DEBUG inherit_cpu:293: nr=11 endp=11 dcid=0 lpid=658 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   65.068533] DEBUG new_mini_send:233: BEFORE UP lpid=658 p_sem=-1 rcode=76
		[   65.068541] DEBUG new_mini_send:234: WUNLOCK_PROC ep=11 count=1
		[   65.068543] DEBUG new_mini_send:272: WUNLOCK_PROC ep=12 count=1
		[   65.068584] DEBUG sleep_proc:365: endpoint=11 ret=0 p_rcode=76
		[   65.068586] DEBUG sleep_proc:366: endpoint=11 flags=0 cpuid=1
		[   65.068588] DEBUG sleep_proc:367: WLOCK_PROC ep=11 count=1
		[   65.068591] DEBUG sleep_proc:414: nr=11 endp=11 dcid=0 lpid=658 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   65.068593] DEBUG sleep_proc:416: someone wakeups me: sem=0 p_rcode=0
		[   65.068595] DEBUG new_mini_sendrec:813: WUNLOCK_PROC ep=11 count=1
		[   65.068635] DEBUG 658:SYSC_ipc:163: ipc_mini_sendrec: call=1280 first=12 second=0 third=0 fifth=-1 
		[   65.068637] DEBUG 658:ipc_mini_sendrec:106: srcdst_ep=12 timeout_ms=-1
		[   65.068639] DEBUG new_mini_sendrec:527: srcdst_ep=12
		[   65.068641] DEBUG check_caller:533: caller_pid=658 caller_tgid=658
		[   65.068643] DEBUG check_caller:569: WLOCK_PROC ep=11 count=1
		[   65.068645] DEBUG check_caller:599: WUNLOCK_PROC ep=11 count=1
		[   65.068646] DEBUG check_caller:602: dcid=0
		[   65.068648] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   65.068650] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   65.068651] DEBUG check_caller:616: caller_pid=658 
		[   65.068653] DEBUG new_mini_sendrec:538: RLOCK_PROC ep=11 count=1
		[   65.068656] DEBUG new_mini_sendrec:543: caller_nr=11 caller_ep=11 srcdst_ep=12 
		[   65.068657] DEBUG new_mini_sendrec:550: RUNLOCK_PROC ep=11 count=1
		[   65.068659] DEBUG new_mini_sendrec:552: dcid=0
		[   65.068661] DEBUG new_mini_sendrec:556: RLOCK_DC dc=0 count=0
		[   65.068663] DEBUG new_mini_sendrec:560: RUNLOCK_DC dc=0 count=0
		[   65.068665] DEBUG new_mini_sendrec:575: RLOCK_PROC ep=11 count=1
		[   65.068721] DEBUG new_mini_sendrec:586: RUNLOCK_PROC ep=11 count=1
		[   65.068729] DEBUG new_mini_sendrec:592: WLOCK_PROC ep=12 count=1
		[   65.068732] DEBUG new_mini_sendrec:594: srcdst_nr=12 srcdst_ep=12
		[   65.068734] DEBUG new_mini_sendrec:614: srcdst_ptr->p_usr.p_nodeid=0
		[   65.068737] DEBUG new_mini_sendrec:618: RLOCK_DC dc=0 count=0
		[   65.068739] DEBUG new_mini_sendrec:622: RUNLOCK_DC dc=0 count=0
		[   65.068743] DEBUG new_mini_sendrec:637: dcid=0 caller_pid=658 caller_nr=11 srcdst_ep=12 
		[   65.068744] DEBUG new_mini_sendrec:642: SENDING HALF
		[   65.068746] DEBUG new_mini_sendrec:775: destination is not waiting to receive srcdst_ptr-flags=0. Enqueue at TAIL.
		[   65.068751] DEBUG sleep_proc2:718: BEFORE DOWN lpid=658 p_sem=0 timeout=-1
		[   65.068753] DEBUG sleep_proc2:720: endpoint=11 flags=C
		[   65.068755] DEBUG sleep_proc2:724: WUNLOCK_PROC ep=11 count=2
		[   65.068757] DEBUG sleep_proc2:724: WUNLOCK_PROC ep=12 count=1
		[   65.068759] DEBUG sleep_proc2:725: endpoint=11 flags=C
		[   75.071018] DEBUG 659:SYSC_ipc:163: ipc_mini_receive: call=768 first=31438 second=0 third=0 fifth=-1 
		[   75.071023] DEBUG 659:ipc_mini_receive:74: src_ep=31438 timeout_ms=-1
		[   75.071025] DEBUG new_mini_receive:303: src_ep=31438
		[   75.071027] DEBUG check_caller:533: caller_pid=659 caller_tgid=659
		[   75.071029] DEBUG check_caller:569: WLOCK_PROC ep=12 count=1
		[   75.071030] DEBUG check_caller:599: WUNLOCK_PROC ep=12 count=1
		[   75.071031] DEBUG check_caller:602: dcid=0
		[   75.071033] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   75.071034] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   75.071035] DEBUG check_caller:616: caller_pid=659 
		[   75.071036] DEBUG new_mini_receive:314: RLOCK_PROC ep=12 count=1
		[   75.071038] DEBUG new_mini_receive:318: caller_nr=12 caller_ep=12 src_ep=31438 
		[   75.071038] DEBUG new_mini_receive:322: dcid=0
		[   75.071040] DEBUG new_mini_receive:330: RUNLOCK_PROC ep=12 count=1
		[   75.071041] DEBUG new_mini_receive:332: RLOCK_DC dc=0 count=0
		[   75.071042] DEBUG new_mini_receive:335: RUNLOCK_DC dc=0 count=0
		[   75.071043] DEBUG new_mini_receive:385: WLOCK_PROC ep=12 count=1
		[   75.071045] DEBUG new_mini_receive:434: WUNLOCK_PROC ep=12 count=1
		[   75.071046] DEBUG new_mini_receive:435: WLOCK_PROC ep=11 count=2
		[   75.071047] DEBUG new_mini_receive:436: WLOCK_PROC ep=12 count=1
		[   75.071048] DEBUG new_mini_receive:440: Found acceptable message from 11. Copy it and update status.
		[   75.071049] DEBUG new_mini_receive:443: LIST_DEL
		[   75.071052] DEBUG copy_usr2usr:884: rqtr_ep=11 src_ep=11 src_lpid=658 src_vpid=2 src_addr=015f4008
		[   75.071054] DEBUG copy_usr2usr:887: dst_ep=12 dst_lpid=659 dst_vpid=3 dst_addr=015f4008 bytes=76
		[   75.071055] DEBUG copy_usr2usr:891: task_pid_nr(current)=659
		[   75.071055] DEBUG copy_usr2usr:902: READ
		[   75.071057] DEBUG copy_usr2usr:907: task_pid_nr(src_ptr->p_task)=658
		[   75.071059] DEBUG dvk_vm_rw:442: pid=658 liovcnt=1 riovcnt=1 flags=0 vm_write=0
		[   75.071061] DEBUG dvk_check_iovect:1280: type=0 nr_segs=1
		[   75.071062] DEBUG dvk_rw_check_kvector:862: type=0 nr_segs=1
		[   75.071063] DEBUG dvk_rw_check_kvector:862: type=-1 nr_segs=1
		[   75.071065] DEBUG dvk_vm_rw_core:277: pid=658 riovcnt=1 flags=0 vm_write=0
		[   75.071072] DEBUG dvk_vm_rw_core:331: i=0 rc=0
		[   75.071086] DEBUG dvk_vm_rw_core:349: rc=76
		[   75.071087] DEBUG dvk_vm_rw_core:356: rc=76
		[   75.071089] DEBUG copy_usr2usr:952: len=76
		[   75.071090] DEBUG new_mini_receive:475: WUNLOCK_PROC ep=11 count=2
		[   75.071092] DEBUG new_mini_receive:476: WUNLOCK_PROC ep=12 count=1
		[   75.071144] DEBUG 659:SYSC_ipc:163: ipc_mini_send: call=512 first=11 second=0 third=0 fifth=-1 
		[   75.071145] DEBUG 659:ipc_mini_send:52: dst_ep=11 timeout_ms=-1
		[   75.071146] DEBUG new_mini_send:32: dst_ep=11
		[   75.071148] DEBUG check_caller:533: caller_pid=659 caller_tgid=659
		[   75.071149] DEBUG check_caller:569: WLOCK_PROC ep=12 count=1
		[   75.071150] DEBUG check_caller:599: WUNLOCK_PROC ep=12 count=1
		[   75.071151] DEBUG check_caller:602: dcid=0
		[   75.071152] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   75.071153] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   75.071154] DEBUG check_caller:616: caller_pid=659 
		[   75.071155] DEBUG new_mini_send:44: RLOCK_PROC ep=12 count=1
		[   75.071157] DEBUG new_mini_send:48: caller_nr=12 caller_ep=12 dst_ep=11 
		[   75.071158] DEBUG new_mini_send:53: RUNLOCK_PROC ep=12 count=1
		[   75.071159] DEBUG new_mini_send:55: dcid=0
		[   75.071160] DEBUG new_mini_send:59: RLOCK_DC dc=0 count=0
		[   75.071161] DEBUG new_mini_send:64: RUNLOCK_DC dc=0 count=0
		[   75.071162] DEBUG new_mini_send:79: RLOCK_PROC ep=12 count=1
		[   75.071163] DEBUG new_mini_send:90: RUNLOCK_PROC ep=12 count=1
		[   75.071165] DEBUG new_mini_send:93: WUNLOCK_PROC ep=12 count=2
		[   75.071166] DEBUG new_mini_send:93: WLOCK_PROC ep=11 count=2
		[   75.071167] DEBUG new_mini_send:93: WLOCK_PROC ep=12 count=2
		[   75.071168] DEBUG new_mini_send:98: dst_nr=11 dst_ep=11
		[   75.071169] DEBUG new_mini_send:117: dst_ptr->p_usr.p_nodeid=0
		[   75.071170] DEBUG new_mini_send:121: RLOCK_DC dc=0 count=0
		[   75.071171] DEBUG new_mini_send:125: RUNLOCK_DC dc=0 count=0
		[   75.071172] DEBUG new_mini_send:140: dcid=0 caller_pid=659 caller_nr=12 dst_ep=11 
		[   75.071174] DEBUG new_mini_send:222: destination is waiting. Copy the message and wakeup destination
		[   75.071175] DEBUG copy_usr2usr:884: rqtr_ep=12 src_ep=12 src_lpid=659 src_vpid=3 src_addr=015f4008
		[   75.071177] DEBUG copy_usr2usr:887: dst_ep=11 dst_lpid=658 dst_vpid=2 dst_addr=015f4008 bytes=76
		[   75.071178] DEBUG copy_usr2usr:891: task_pid_nr(current)=659
		[   75.071179] DEBUG copy_usr2usr:894: WRITE
		[   75.071180] DEBUG copy_usr2usr:899: task_pid_nr(dst_ptr->p_task)=658
		[   75.071181] DEBUG dvk_vm_rw:442: pid=658 liovcnt=1 riovcnt=1 flags=0 vm_write=1
		[   75.071182] DEBUG dvk_check_iovect:1280: type=1 nr_segs=1
		[   75.071184] DEBUG dvk_rw_check_kvector:862: type=1 nr_segs=1
		[   75.071185] DEBUG dvk_rw_check_kvector:862: type=-1 nr_segs=1
		[   75.071186] DEBUG dvk_vm_rw_core:277: pid=658 riovcnt=1 flags=0 vm_write=1
		[   75.071188] DEBUG dvk_vm_rw_core:331: i=0 rc=0
		[   75.071197] DEBUG dvk_vm_rw_core:349: rc=76
		[   75.071198] DEBUG dvk_vm_rw_core:356: rc=76
		[   75.071199] DEBUG copy_usr2usr:952: len=76
		[   75.071201] DEBUG inherit_cpu:285: cpuid=1 vpid=2
		[   75.071207] DEBUG inherit_cpu:293: nr=11 endp=11 dcid=0 lpid=658 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   75.071208] DEBUG new_mini_send:233: BEFORE UP lpid=658 p_sem=-1 rcode=76
		[   75.071218] DEBUG new_mini_send:234: WUNLOCK_PROC ep=11 count=2
		[   75.071220] DEBUG new_mini_send:272: WUNLOCK_PROC ep=12 count=2
		[   75.071927] DEBUG sleep_proc2:733: endpoint=11 ret=0 p_rcode=76
		[   75.071931] DEBUG sleep_proc2:734: endpoint=11 flags=0 cpuid=1
		[   75.071932] DEBUG sleep_proc2:736: WLOCK_PROC ep=11 count=2
		[   75.071934] DEBUG sleep_proc2:736: WLOCK_PROC ep=12 count=2
		[   75.071936] DEBUG sleep_proc2:783: nr=11 endp=11 dcid=0 lpid=658 p_cpumask=FFFFFFFF nodemap=1 name=test_sendrec 
		[   75.071937] DEBUG sleep_proc2:785: someone wakeups me: sem=0 p_rcode=0
		[   75.071940] DEBUG new_mini_sendrec:808: WUNLOCK_PROC ep=12 count=2
		[   75.071941] DEBUG new_mini_sendrec:813: WUNLOCK_PROC ep=11 count=2
		[   75.072000] DEBUG 658:SYSC_ipc:163: ipc_getprocinfo: call=3840 first=0 second=11 third=0 fifth=0 
		[   75.072002] DEBUG 658:ipc_getprocinfo:283: dcid=0 p_nr=11
		[   75.072004] DEBUG new_getprocinfo:1938: dcid=0 p_nr=11
		[   75.072007] DEBUG check_caller:533: caller_pid=658 caller_tgid=658
		[   75.072008] DEBUG check_caller:569: WLOCK_PROC ep=11 count=2
		[   75.072009] DEBUG check_caller:599: WUNLOCK_PROC ep=11 count=2
		[   75.072010] DEBUG check_caller:602: dcid=0
		[   75.072012] DEBUG check_caller:606: RLOCK_DC dc=0 count=0
		[   75.072013] DEBUG check_caller:610: RUNLOCK_DC dc=0 count=0
		[   75.072014] DEBUG check_caller:616: caller_pid=658 
		[   75.072016] DEBUG new_getprocinfo:1955: RLOCK_DC dc=0 count=0
		[   75.072017] DEBUG new_getprocinfo:1960: RUNLOCK_DC dc=0 count=0
		[   75.072018] DEBUG new_getprocinfo:1962: RLOCK_PROC ep=11 count=2
		[   75.072019] DEBUG new_getprocinfo:1963: lpid=658 name=test_sendrec
		[   75.072021] DEBUG new_getprocinfo:1965: RUNLOCK_PROC ep=11 count=2

Si cambiamos a:
		root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_sendrec 0 13 53
			DONDE endpoint=53 es SERVER que hace RECEIVE 
			ERROR: test_sendrec.c:main:104: rcode=-102 <<<< EDVSBADCALL
			Producto de verificar si puede realizar el dvkcall RECEIVE 
			if( ! get_sys_bit(caller_ptr->p_priv.priv_usr.priv_dvk_allowed, RECEIVE )){
					ERROR_RUNLOCK_PROC(caller_ptr,EDVSBADCALL);
			} 
			
Si cambiamos a:
		root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_sendrec 0  54 14
			DONDE endpoint=14 es SERVER que hace RECEIVE y endpoint 54 cliente que hace SENDREC 
			[  673.141653] ERROR: 22093:new_mini_sendrec:548: rcode=-102
			if( ! get_sys_bit(caller_ptr->p_priv.priv_usr.priv_dvk_allowed, SENDREC )){
				ERROR_RUNLOCK_PROC(caller_ptr,EDVSBADCALL);
			}	 
	
	Por Default, todo proceso tendra acceso al SYSCALL (SENDREC, GETEP, WAIT4BIND, UNBIND)

	
		Se modifico dvk_debug.h 
				#if defined CONFIG_DVKIPC || defined CONFIG_DVSIOCTL

	
		Cambie en el Linux y en DVS 
			CONFIG_DVS_DVK => CONFIG_DVKIOCTL
		usando https://www.internalpointers.com/post/linux-find-and-replace-text-multiple-files
		
		grep -RiIl 'CONFIG_DVK' | xargs sed -i 's/CONFIG_DVK/CONFIG_DVS_DVK/g'

		grep -RiIl 'CONFIG_DVS_DVK' | xargs sed -i 's/CONFIG_DVS_DVK/CONFIG_DVKIOCTL/g'
		
		grep -RiIl 'DVS_DVK' | xargs sed -i 's/DVS_DVK/DVKIOCTL/g'
		
		PARA ENCONTRAR ARCHIVOS MODIFICADOS EN /usr/src/linux 
		find . -type f -newermt '2020-07-23' | grep -v ".order" | grep -v ".builtin" | grep -v ".mod" | grep -v ".cmd" | grep -v ".o"

		Se implementaron control de privilegios de PROXIES 
			get2rmt equivalente a RECEIVE
			put2lcl equivalente a SEND 
			
		Luego cuando se tiene un Mensaje a entregar por el PROXY_RECEIVER a traves de put2lcl 
			Se deben controlar los permisos del endpoint emisor y receptor.
			
		En el proxy sender no deberia haber problemas porque ya el IPC controla eso.

 
		------------------------------------------------------------------------------------------------	
		COMPILE EL KERNEL SIN CONFIG_DVKIPC Y LUEGO COMPILE EL MODULO 
		RECOMPILE TODOS LOS PROGRAMAS PARA QUE USEN LA LIBRERIA DE IOCTL
		EL MODULO CARGA 
			root@node0:/usr/src/dvs/dvk-tests# dmesg
			root@node0:/usr/src/dvs/dvk-tests# lsmod | grep dvk
			dvk                   262144  0

		PERO NO ESTAN RESUELTAS LAS DVK CALLS 
			root@node0:/usr/src/dvs/dvk-tests# ./test_dvs_info 
			DEBUG 697:dvk_open:102: 
			Get DVS info
			DEBUG 697:dvk_getdvsinfo:248: 
			DEBUG 697:dvk_getdvsinfo:252: ipc ret=-1
			ERROR: 697:dvk_getdvsinfo:253: rcode=-38
			ERROR: 697:dvk_getdvsinfo:264: rcode=-38
			local node ID -38... 
			d_name= d_nr_dcs=0 d_nr_nodes=0 d_nr_procs=0 d_nr_tasks=0 d_nr_sysprocs=0 
			d_max_copybuf=0 d_max_copylen=0
			d_dbglvl=0 version=0 flags=0 sizeof(proc)=0


		grep -RiIl 'CONFIG_DVKIOCTLIPC' | xargs sed -i 's/CONFIG_DVKIOCTLIPC/CONFIG_DVKIPC/g'

		FUNCIONARON BIEN LAS PRUEBAS LOCALES!!!!!
		
		ATENCION:
			No olvidar de recompilar dvk-lib 
			No olvidar de modificar los Makefile de todos los programas 
				dvk-test 
				dvk-loops
				dvk-proxies
				dvs-apps/dc_init 
		
===============================================================================================================
20200726:
		Se modificarone errores de dvk_utils al inicializar los proxies referentes a los privilegios.
		
		Se creo el utilitario test_priv_info para imprimir los privilegios de un proceso 

===============================================================================================================
20200727 :
		Agregue utilitario test_set_priv  
		usage: ./test_set_priv 
				[-w warn_ep] [-l level] [-I ipc_to] [-N ipc_not] [-A Allowed] [-D Deny] -d dcid -p p_nr

		Para probar se bindea un proceso PRIVILEGIADO (6)
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_bind 0 5

		y el resultado es 
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -D put2lcl -d 0 -p 5
			DEBUG 5:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 5:dvk_getdvsinfo:246: 
			DEBUG 5:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 5:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			deny_dvk=20
			dcid=0
			DEBUG 5:dvk_getdcinfo:347: dcid=0
			DEBUG 5:dvk_getdcinfo:359: ioctl ret=0 errno=0
			DEBUG 5:dvk_getdcinfo:364: ioctl ret=0
			dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			flags=0 dc_nodes=3 dc_pid=827 dc_name=DC0
			dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
			DEBUG 5:dvk_getprocinfo:1154: dcid=0 p_nr=5 
			DEBUG 5:dvk_getprocinfo:1167: ioctl ret=0 errno=0
			DEBUG 5:dvk_getprocinfo:1172: ioctl ret=0
			nr=5 endp=5 dcid=0 flags=0 misc=20 lpid=865 vpid=2 nodeid=0 name=test_bind 
			endp=5 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
			nr=5 endp=5 dcid=0 p_lclsent=0 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 
			DEBUG 5:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 5:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 5:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			BEFORE priv_id=39 priv_warn=27342 priv_level=0
			BEFORE 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[mini_send(2)]->[mini_receive(3)]->[mini_notify(4)]->[mini_sendrec(5)]->[mini_rcvrqst(6)]->[mini_reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			DEBUG 5:dvk_setpriv:1009: dcid=0 endpoint=5 
			DEBUG 5:dvk_setpriv:1022: ioctl ret=0 errno=0
			DEBUG 5:dvk_setpriv:1027: ioctl ret=0
			DEBUG 5:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 5:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 5:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			AFTER priv_id=39 priv_warn=27342 priv_level=0
			AFTER 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[mini_send(2)]->[mini_receive(3)]->[mini_notify(4)]->[mini_sendrec(5)]->[mini_rcvrqst(6)]->[mini_reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFEF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->

		
SEGUN EL KERN.LOG   
		Jul 27 11:02:31 node0 kernel: [ 1376.037400] DEBUG dvk_ioctl:342: cmd=4004E30C arg=BFC13060
		Jul 27 11:02:31 node0 kernel: [ 1376.037404] DEBUG dvk_ioctl:362: DVK_CALL=12 (io_setpriv) 
		Jul 27 11:02:31 node0 kernel: [ 1376.037405] DEBUG io_setpriv:131: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037409] DEBUG new_setpriv:2457: RLOCK_DC dc=0 count=0
		Jul 27 11:02:31 node0 kernel: [ 1376.037410] DEBUG new_setpriv:2462: dcid=0 proc_nr=5 proc_ep=5
		Jul 27 11:02:31 node0 kernel: [ 1376.037412] DEBUG new_setpriv:2470: WLOCK_PROC ep=5 count=0
		Jul 27 11:02:31 node0 kernel: [ 1376.037415] DEBUG new_setpriv:2474: priv_id=39 priv_warn=27342 priv_level=0
		Jul 27 11:02:31 node0 kernel: [ 1376.037417] DEBUG new_setpriv:2475: PRINT_DVKALLOWED_MAP: 870:new_setpriv:2475:proc_ptr->p_priv.priv_usr.priv_dvk_allowed:
		Jul 27 11:02:31 node0 kernel: [ 1376.037419] DEBUG new_setpriv:2475: PRINT_DVKALLOWED_MAP: 870:new_setpriv:2475:3:
		Jul 27 11:02:31 node0 kernel: [ 1376.037420] DEBUG new_setpriv:2475: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037420] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037421] DEBUG new_setpriv:2475: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037421] FFEF: <<<<<<<<<<<<<<<<<<<<<<<<<<<<< FFEF: 1111111111101111
		Jul 27 11:02:31 node0 kernel: [ 1376.037422] DEBUG new_setpriv:2475: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037422] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037423] DEBUG new_setpriv:2475: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037424] DEBUG new_setpriv:2476: PRINT_SYS_MAP: 870:new_setpriv:2476:proc_ptr->p_priv.priv_usr.priv_ipc_to:
		Jul 27 11:02:31 node0 kernel: [ 1376.037426] DEBUG new_setpriv:2476: PRINT_SYS_MAP: 870:new_setpriv:2476:4:
		Jul 27 11:02:31 node0 kernel: [ 1376.037427] DEBUG new_setpriv:2476: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037427] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037428] DEBUG new_setpriv:2476: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037428] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037429] DEBUG new_setpriv:2476: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037429] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037430] DEBUG new_setpriv:2476: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037430] FFFF:
		Jul 27 11:02:31 node0 kernel: [ 1376.037431] DEBUG new_setpriv:2476: 
		Jul 27 11:02:31 node0 kernel: [ 1376.037432] DEBUG new_setpriv:2477: WUNLOCK_PROC ep=5 count=0
		Jul 27 11:02:31 node0 kernel: [ 1376.037433] DEBUG new_setpriv:2478: RUNLOCK_DC dc=0 count=0	


OTRA PRUEBA:  QUITAR PERMISOS PARA IPC AL PROCESO (-34)
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -N -34  -d 0 -p 5
			DEBUG 32:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 32:dvk_getdvsinfo:246: 
			DEBUG 32:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 32:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			dcid=0
			DEBUG 32:dvk_getdcinfo:347: dcid=0
			DEBUG 32:dvk_getdcinfo:359: ioctl ret=0 errno=0
			DEBUG 32:dvk_getdcinfo:364: ioctl ret=0
			dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			flags=0 dc_nodes=3 dc_pid=827 dc_name=DC0
			dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
			DEBUG 32:dvk_getprocinfo:1154: dcid=0 p_nr=5 
			DEBUG 32:dvk_getprocinfo:1167: ioctl ret=0 errno=0
			DEBUG 32:dvk_getprocinfo:1172: ioctl ret=0
			nr=5 endp=5 dcid=0 flags=0 misc=20 lpid=952 vpid=29 nodeid=0 name=test_bind 
			endp=5 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
			nr=5 endp=5 dcid=0 p_lclsent=0 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 
			DEBUG 32:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 32:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 32:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			BEFORE priv_id=39 priv_warn=27342 priv_level=0
			BEFORE 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF:
			FFFF:
			FFFF:
			FFFF:
			DEBUG 32:dvk_setpriv:1009: dcid=0 endpoint=5 
			DEBUG 32:dvk_setpriv:1022: ioctl ret=0 errno=0
			DEBUG 32:dvk_setpriv:1027: ioctl ret=0
			DEBUG 32:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 32:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 32:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			AFTER priv_id=39 priv_warn=27342 priv_level=0
			AFTER 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFE: <<<<<<<<<<<<<<<<<< AQUI MODIFICO 
			FFFF:
			FFFF:
			FFFF:

OTRA PRUEBA:  VOLVER A DAR PERMISOS PARA IPC AL PROCESO (-34)

			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -I -34  -d 0 -p 5
			DEBUG 33:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 33:dvk_getdvsinfo:246: 
			DEBUG 33:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 33:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			dcid=0
			DEBUG 33:dvk_getdcinfo:347: dcid=0
			DEBUG 33:dvk_getdcinfo:359: ioctl ret=0 errno=0
			DEBUG 33:dvk_getdcinfo:364: ioctl ret=0
			dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			flags=0 dc_nodes=3 dc_pid=827 dc_name=DC0
			dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
			DEBUG 33:dvk_getprocinfo:1154: dcid=0 p_nr=5 
			DEBUG 33:dvk_getprocinfo:1167: ioctl ret=0 errno=0
			DEBUG 33:dvk_getprocinfo:1172: ioctl ret=0
			nr=5 endp=5 dcid=0 flags=0 misc=20 lpid=952 vpid=29 nodeid=0 name=test_bind 
			endp=5 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
			nr=5 endp=5 dcid=0 p_lclsent=0 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 
			DEBUG 33:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 33:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 33:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			BEFORE priv_id=39 priv_warn=27342 priv_level=0
			BEFORE 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFE: <<<<<<<<<<<<<<<<<<<< ESTADO ANTERIOR
			FFFF:
			FFFF:
			FFFF:
			DEBUG 33:dvk_setpriv:1009: dcid=0 endpoint=5 
			DEBUG 33:dvk_setpriv:1022: ioctl ret=0 errno=0
			DEBUG 33:dvk_setpriv:1027: ioctl ret=0
			DEBUG 33:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 33:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 33:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			AFTER priv_id=39 priv_warn=27342 priv_level=0
			AFTER 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF: <<<<<<<<<<<<<<<<<<< ESTADO NUEVO 
			FFFF:
			FFFF:
			FFFF:

OTRA PRUEBA: CAMBIAR EL WARN ENDPOINT Y EL LEVEL 
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -w -33 -l 3  -d 0 -p 5
			DEBUG 37:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 37:dvk_getdvsinfo:246: 
			DEBUG 37:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 37:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			dcid=0
			DEBUG 37:dvk_getdcinfo:347: dcid=0
			DEBUG 37:dvk_getdcinfo:359: ioctl ret=0 errno=0
			DEBUG 37:dvk_getdcinfo:364: ioctl ret=0
			dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			flags=0 dc_nodes=3 dc_pid=827 dc_name=DC0
			dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
			DEBUG 37:dvk_getprocinfo:1154: dcid=0 p_nr=5 
			DEBUG 37:dvk_getprocinfo:1167: ioctl ret=0 errno=0
			DEBUG 37:dvk_getprocinfo:1172: ioctl ret=0
			nr=5 endp=5 dcid=0 flags=0 misc=20 lpid=971 vpid=34 nodeid=0 name=test_bind 
			endp=5 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
			nr=5 endp=5 dcid=0 p_lclsent=0 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 
			DEBUG 37:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 37:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 37:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			BEFORE priv_id=39 priv_warn=27342 priv_level=0 <<<<<<<<<<<<<<<<<<<<<<<<< ORIGINAL 
			BEFORE 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF:
			FFFF:
			FFFF:
			FFFF:
			DEBUG 37:dvk_setpriv:1009: dcid=0 endpoint=5 
			DEBUG 37:dvk_setpriv:1022: ioctl ret=0 errno=0
			DEBUG 37:dvk_setpriv:1027: ioctl ret=0
			DEBUG 37:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 37:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 37:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52 
			AFTER priv_id=39 priv_warn=-33 priv_level=3  <<<<<<<<<<<<<<<<<<<<<<<<< MODIFICADO 
			AFTER 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF:
			FFFF:
			FFFF:
			FFFF:


	dvk_hyper.c:  controle los parametros que pasa el dvk_setpriv 

	PRUEBA DE CAMBIAR 
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -w -33 -l 3  -d 0 -p 5
			DEBUG 5:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 5:dvk_getdvsinfo:246: 
			DEBUG 5:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 5:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			dcid=0
			DEBUG 5:dvk_getdcinfo:347: dcid=0
			DEBUG 5:dvk_getdcinfo:359: ioctl ret=0 errno=0
			DEBUG 5:dvk_getdcinfo:364: ioctl ret=0
			dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			flags=0 dc_nodes=3 dc_pid=658 dc_name=DC0
			dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
			DEBUG 5:dvk_getprocinfo:1154: dcid=0 p_nr=5 
			DEBUG 5:dvk_getprocinfo:1167: ioctl ret=0 errno=0
			DEBUG 5:dvk_getprocinfo:1172: ioctl ret=0
			nr=5 endp=5 dcid=0 flags=0 misc=20 lpid=695 vpid=2 nodeid=0 name=test_bind 
			endp=5 dcid=0 flags=0 p_getfrom=27342 p_sendto=27342 p_waitmigr=27342 p_waitunbind=27342 p_proxy=27342
			nr=5 endp=5 dcid=0 p_lclsent=0 p_rmtsent=0 p_lclcopy=0 p_rmtcopy=0 
			DEBUG 5:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 5:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 5:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			BEFORE priv_id=39 priv_warn=27342 priv_level=0 <<<<<<<<<<<<<<<<<<<<<<<<<<< VALORES ORIGINALES
			BEFORE 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF:
			FFFF:
			FFFF:
			FFFF:
			DEBUG 5:dvk_setpriv:1009: dcid=0 endpoint=5 
			DEBUG 5:dvk_setpriv:1022: ioctl ret=0 errno=0
			DEBUG 5:dvk_setpriv:1027: ioctl ret=0
			DEBUG 5:dvk_getpriv:1037: dcid=0 endpoint=5 
			DEBUG 5:dvk_getpriv:1050: ioctl ret=0 errno=0
			DEBUG 5:dvk_getpriv:1055: ioctl ret=0
			sizeof(priv_usr_t)=52
			AFTER priv_id=39 priv_warn=-33 priv_level=3 <<<<<<<<<<<<<<<<<<<<<<<<<<< VALORES NUEVOS
			AFTER 
			NR_DVK_CHUNKS=3
			BITCHUNK_BITS=16

			FFFF:[void0(0)]->[dc_init(1)]->[send(2)]->[receive(3)]->[notify(4)]->[sendrec(5)]->[rcvrqst(6)]->[reply(7)]->[dc_end(8)]->[bind(9)]->[unbind(10)]->[getpriv(11)]->[setpriv(12)]->[vcopy(13)]->[getdcinfo(14)]->[getprocinfo(15)]->
			FFFF:[mini_relay(16)]->[proxies_bind(17)]->[proxies_unbind(18)]->[getnodeinfo(19)]->[put2lcl(20)]->[get2rmt(21)]->[add_node(22)]->[del_node(23)]->[dvs_init(24)]->[dvs_end(25)]->[getep(26)]->[getdvsinfo(27)]->[proxy_conn(28)]->[wait4bind(29)]->[migrate(30)]->[node_up(31)]->
			FFFF:[node_down(32)]->[getproxyinfo(33)]->[wakeup(34)]->
			NR_SYS_CHUNKS=4
			BITCHUNK_BITS=16

			FFFF:
			FFFF:
			FFFF:
			FFFF:

		PROVOCANDO ERRORES 
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -w -55 -l 3  -d 0 -p 5 <<<< INVALID warn_ep
			DEBUG 17:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 17:dvk_getdvsinfo:246: 
			DEBUG 17:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 17:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			Invalid warn_ep [-34-28]
			root@node0:/usr/src/dvs/dvk-tests# make
			gcc  -MMD -MP -ldl -lm -lrt -lnsl -pthread -rdynamic -o test_set_priv test_set_priv.c  ../dvk-lib//stub_dvkcall.o -I..
			
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -w -55 -l 3  -d 0 -p 5 <<<<< INVALID level 
			DEBUG 21:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 21:dvk_getdvsinfo:246: 
			DEBUG 21:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 21:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			Invalid warn_ep [-34,28]
			root@node0:/usr/src/dvs/dvk-tests# nsenter -p -t$DC0 ./test_set_priv -w -5 -l 33  -d 0 -p 5
			DEBUG 22:dvk_open:105: Open dvk device file /dev/dvk
			DEBUG 22:dvk_getdvsinfo:246: 
			DEBUG 22:dvk_getdvsinfo:255: ioctl ret=0 errno=0
			DEBUG 22:dvk_getdvsinfo:260: ioctl ret=0
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			local_nodeid=0
			Invalid level [0-4]

===============================================================================================================
20200816:
		Se agregaron a /proc 
			DCx/ipcto
				-nr id --wr- lv 3210987654321098765432109876543210987654321098765432109876543210
				 11 46 27342  0 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
				 80 64 27342  0 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
			DCx/dvkcalls 
				-nr id --wr- lv 765432109876543210987654321098765432109876543210
				 11 46 27342  0 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
				 80 64 27342  0 ------------------X--X---------------X----X-----
			proxies/ipcto
				ID Type 3210987654321098765432109876543210987654321098765432109876543210
				 0 send ----------------------------------------------------------------
				 0 recv ----------------------------------------------------------------
			proxies/dvkcalls 
				ID Type 765432109876543210987654321098765432109876543210
				 0 send --------------------------X--XX-----------------
				 0 recv ---------------------------X-XX-----------------
			
		Que muestran el estado de los privilegios de los procesos 


		Se reprogramo INFORMACION ESTADISTICA AL PROXY VIA WEB 
			El SENDER proxy reporta en el puerto (4000+nodeid)
				SENDER Proxy Web Server .
				---snode--	---dnode--	---dcid---	--nr_msgs-	--nr_data-	--nr_cmd--
						0			1			0			15		655360		25
			El RECEIVER proxy reporta en el puerto (5000+nodeid)
				RECEIVER Proxy Web Server .
				---snode--	---dnode--	---dcid---	--nr_msgs-	--nr_data-	--nr_cmd--
						1			0			0			15			0		40			

		
		
		
======================================================================================================

Aug 18 16:50:08 node0 kernel: [  352.545296] DEBUG 999:SYSC_ipc:165: ipc_mini_sendrec: call=1280 first=1 second=0 third=0 fifth=-1 
Aug 18 16:50:08 node0 kernel: [  352.545297] DEBUG 999:ipc_mini_sendrec:115: srcdst_ep=1 timeout_ms=-1
Aug 18 16:50:08 node0 kernel: [  352.545299] DEBUG 999:new_mini_sendrec:527: srcdst_ep=1
Aug 18 16:50:08 node0 kernel: [  352.545302] DEBUG 999:check_caller:546: caller_pid=999 caller_tgid=999
Aug 18 16:50:08 node0 kernel: [  352.545303] DEBUG 999:check_caller:582: WLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545305] DEBUG 999:check_caller:612: WUNLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545306] DEBUG 999:check_caller:615: dcid=0
Aug 18 16:50:08 node0 kernel: [  352.545308] DEBUG 999:check_caller:619: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545309] DEBUG 999:check_caller:623: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545311] DEBUG 999:check_caller:629: caller_pid=999 
Aug 18 16:50:08 node0 kernel: [  352.545312] DEBUG 999:new_mini_sendrec:538: RLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545314] DEBUG 999:new_mini_sendrec:543: caller_nr=10 caller_ep=10 srcdst_ep=1 
Aug 18 16:50:08 node0 kernel: [  352.545316] DEBUG 999:new_mini_sendrec:550: RUNLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545317] DEBUG 999:new_mini_sendrec:552: dcid=0
Aug 18 16:50:08 node0 kernel: [  352.545319] DEBUG 999:new_mini_sendrec:556: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545320] DEBUG 999:new_mini_sendrec:560: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545322] DEBUG 999:new_mini_sendrec:575: RLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545324] DEBUG 999:new_mini_sendrec:586: RUNLOCK_PROC ep=10 count=4
Aug 18 16:50:08 node0 kernel: [  352.545325] DEBUG 999:new_mini_sendrec:592: WUNLOCK_PROC ep=10 count=5
Aug 18 16:50:08 node0 kernel: [  352.545327] DEBUG 999:new_mini_sendrec:592: WLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.545328] DEBUG 999:new_mini_sendrec:592: WLOCK_PROC ep=10 count=5
Aug 18 16:50:08 node0 kernel: [  352.545329] DEBUG 999:new_mini_sendrec:594: srcdst_nr=1 srcdst_ep=1
Aug 18 16:50:08 node0 kernel: [  352.545331] DEBUG 999:new_mini_sendrec:614: srcdst_ptr->p_usr.p_nodeid=0
Aug 18 16:50:08 node0 kernel: [  352.545332] DEBUG 999:new_mini_sendrec:618: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545334] DEBUG 999:new_mini_sendrec:622: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.545336] DEBUG 999:new_mini_sendrec:637: dcid=0 caller_pid=999 caller_nr=10 srcdst_ep=1 
Aug 18 16:50:08 node0 kernel: [  352.545337] DEBUG 999:new_mini_sendrec:642: SENDING HALF
Aug 18 16:50:08 node0 kernel: [  352.545338] DEBUG 999:new_mini_sendrec:775: destination is not waiting to receive srcdst_ptr-flags=0. Enqueue at TAIL.
Aug 18 16:50:08 node0 kernel: [  352.545341] DEBUG 999:sleep_proc2:731: BEFORE DOWN lpid=999 p_sem=0 timeout=-1
Aug 18 16:50:08 node0 kernel: [  352.545342] DEBUG 999:sleep_proc2:733: endpoint=10 flags=C
Aug 18 16:50:08 node0 kernel: [  352.545344] DEBUG 999:sleep_proc2:737: WUNLOCK_PROC ep=10 count=5
Aug 18 16:50:08 node0 kernel: [  352.545592] DEBUG 999:sleep_proc2:737: WUNLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.545632] DEBUG 999:sleep_proc2:738: endpoint=10 flags=C

Aug 18 16:50:08 node0 kernel: [  352.546320] DEBUG 644:SYSC_ipc:165: ipc_mini_send: call=512 first=10 second=0 third=0 fifth=30000 
Aug 18 16:50:08 node0 kernel: [  352.546323] DEBUG 644:ipc_mini_send:61: dst_ep=10 timeout_ms=30000
Aug 18 16:50:08 node0 kernel: [  352.546325] DEBUG 644:new_mini_send:32: dst_ep=10
Aug 18 16:50:08 node0 kernel: [  352.546327] DEBUG 644:check_caller:546: caller_pid=644 caller_tgid=644
Aug 18 16:50:08 node0 kernel: [  352.546329] DEBUG 644:check_caller:582: WLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546330] DEBUG 644:check_caller:612: WUNLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546331] DEBUG 644:check_caller:615: dcid=0
Aug 18 16:50:08 node0 kernel: [  352.546333] DEBUG 644:check_caller:619: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546335] DEBUG 644:check_caller:623: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546336] DEBUG 644:check_caller:629: caller_pid=644 
Aug 18 16:50:08 node0 kernel: [  352.546338] DEBUG 644:new_mini_send:44: RLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546339] DEBUG 644:new_mini_send:48: caller_nr=1 caller_ep=1 dst_ep=10 
Aug 18 16:50:08 node0 kernel: [  352.546340] DEBUG 644:new_mini_send:53: RUNLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546342] DEBUG 644:new_mini_send:55: dcid=0
Aug 18 16:50:08 node0 kernel: [  352.546343] DEBUG 644:new_mini_send:59: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546344] DEBUG 644:new_mini_send:64: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546346] DEBUG 644:new_mini_send:79: RLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546347] DEBUG 644:new_mini_send:90: RUNLOCK_PROC ep=1 count=3
Aug 18 16:50:08 node0 kernel: [  352.546349] DEBUG 644:new_mini_send:93: WLOCK_PROC ep=10 count=5
Aug 18 16:50:08 node0 kernel: [  352.546350] DEBUG 644:new_mini_send:98: dst_nr=10 dst_ep=10
Aug 18 16:50:08 node0 kernel: [  352.546351] DEBUG 644:new_mini_send:117: dst_ptr->p_usr.p_nodeid=0
Aug 18 16:50:08 node0 kernel: [  352.546352] DEBUG 644:new_mini_send:121: RLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546354] DEBUG 644:new_mini_send:125: RUNLOCK_DC dc=0 count=0
Aug 18 16:50:08 node0 kernel: [  352.546356] DEBUG 644:new_mini_send:140: dcid=0 caller_pid=644 caller_nr=1 dst_ep=10 
Aug 18 16:50:08 node0 kernel: [  352.546357] DEBUG 644:new_mini_send:238: destination is not waiting dst_flags=C. Enqueue at the TAIL.
Aug 18 16:50:08 node0 kernel: [  352.546359] DEBUG 644:sleep_proc2:731: BEFORE DOWN lpid=644 p_sem=0 timeout=30000
Aug 18 16:50:08 node0 kernel: [  352.546360] DEBUG 644:sleep_proc2:733: endpoint=1 flags=4
Aug 18 16:50:08 node0 kernel: [  352.546361] DEBUG 644:sleep_proc2:737: WUNLOCK_PROC ep=1 count=4
Aug 18 16:50:08 node0 kernel: [  352.546363] DEBUG 644:sleep_proc2:737: WUNLOCK_PROC ep=10 count=5
Aug 18 16:50:08 node0 kernel: [  352.546364] DEBUG 644:sleep_proc2:738: endpoint=1 flags=4		

SE MODIFICARON LOS sleep_proc() 
	Se ejecuto ./rhostfsd rhostfs.cfg  para que quede en un dvk_receive() esperando
	y se le hizo kill -SIGUSR1 <PID>
	
[  197.331425] DEBUG 954:sleep_proc:379: sig[0]:0x00000100, sig[1]:0x00000000
[  197.331430] DEBUG 954:sleep_proc:382: endpoint=1 ret=-512 p_rcode=0
[  197.331432] DEBUG 954:sleep_proc:383: endpoint=1 flags=8 cpuid=2
[  197.331434] DEBUG 954:sleep_proc:384: WLOCK_PROC ep=1 count=0
[  197.331436] DEBUG 954:sleep_proc:408: pid=954 ret=-4
[  197.331438] DEBUG 954:sleep_proc:433: nr=1 endp=1 dcid=0 lpid=954 p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
[  197.331440] DEBUG 954:sleep_proc:435: someone wakeups me: sem=0 p_rcode=-4
[  197.331442] DEBUG 954:new_mini_receive:503: WUNLOCK_PROC ep=1 count=0
[  197.331443] ERROR: 954:new_mini_receive:504: rcode=-4
[  197.331450] do_exit: local_nodeid:0

ERROR: 
	Deberia haber detectado el SIGUSR1 
	if( sigismember(&current->pending.signal, SIGUSR1)	)
		DVKDEBUG(INTERNAL,"SIGUSR1 received\n");
	Los numeros de SIGNALS ESTAN EN /arch/x86/include/uapi/asm/signal.h

tambien inclui el bind tipo UNIKERNEL_BIND que cambia el flag p_misc_flags
		root@node0:~# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		 0  11    11  1789/2      0    0  420 27342 27342 27342 27342 test_ukbind   <<< 0x0420 = 0000 0100 0010 0000
		donde 
			MIS_BIT_UNIKERNEL	10
			MIS_BIT_GRPLEADER	5

SOLUCION: Ahora se arreglo lo del SIGUSR1
			if( sigismember(&current->pending.signal,  SIGUSR1) ||				<<< EN ESTA SOLA NO DETECTABA
				sigismember(&current->signal->shared_pending.signal, SIGUSR1)) <<< ESTA IMPRIMIENDO POR ESTA
				DVKDEBUG(INTERNAL,"SIGUSR1 received\n")
		
		root@node0:/usr/src/dvs/vos/uml/rhostfsd# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		 0   1     1   730/730    0    8   20 31438 27342 27342 27342 rhostfsd       
		root@node0:/usr/src/dvs/vos/uml/rhostfsd# dmesg -c > /dev/null
		root@node0:/usr/src/dvs/vos/uml/rhostfsd# kill -SIGUSR1 730
		root@node0:/usr/src/dvs/vos/uml/rhostfsd# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		[1]+  SeÃ±al definida por el usuario 1        ./rhostfsd rhostfs.cfg
		root@node0:/usr/src/dvs/vos/uml/rhostfsd# dmesg | grep SIGUSR 
		[ 1464.205878] DEBUG 730:sleep_proc:382: SIGUSR1 received

TODO: REMOVER LO DEL SIGUSR1  Y LUEGO PROBAR QUE PASA CON UML 

ESTE ES EL PRIMERO QUE APARECIO 
Aug 24 23:53:11 node0 kernel: [  197.280044] DEBUG 683:sleep_proc2:799: shared_pending sig[0]:0x00002000, sig[1]:0x00000000
SIGNAL 14 = #define SIGALRM		14		

Aug 24 23:53:11 node0 kernel: [  197.706772] DEBUG 659:sleep_proc2:796: pending: sig[0]:0x00000100, sig[1]:0x00000000
SIGNAL 9 = #define SIGKILL		 9		

Aug 24 23:53:11 node0 kernel: [  197.275254] DEBUG 683:sleep_proc2:799: shared_pending sig[0]:0x10000000, sig[1]:0x00000000
SIGNAL 29 = #define SIGIO		29

Aug 24 23:53:11 node0 kernel: [  197.706776] DEBUG 659:sleep_proc2:799: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
SIGNAL 2 = #define SIGINT		 2		 		

Aug 24 23:53:07 node0 kernel: [  193.690104] DEBUG 683:sleep_proc2:799: shared_pending sig[0]:0x10002000, sig[1]:0x00000000
SIGNAL 29 = #define SIGIO		29
SIGNAL 14 = #define SIGALRM		14		

Aug 24 23:53:00 node0 kernel: [  186.290151] ERROR: 683:new_mini_sendrec:806: rcode=-4

DESCRIPCION DEL PROBLEMA:
========================

		SERVER					CLIENT
1		recvrqst()		
2								sendrec() => p_rts_flags=0x08 (RECEIVING)
3		process_rqst()			
4								SIGALARM
5								Sale con p_rts_flags=0
6								UML_reitera
7								sendrec() => Server no esta esperando => p_rts_flags=0x0C
8		reply()   encuentra a CLIENT en estado erroneo. (deberia estar en 0x08)
		SE PUDRE TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!
		
POSIBLE SOLUCION 
	DVKDEBUG(INTERNAL,"pending: sig[0]:0x%08x, sig[1]:0x%08x\n", 
		current->pending.signal.sig[0], current->pending.signal.sig[1]);
	DVKDEBUG(INTERNAL,"shared_pending sig[0]:0x%08x, sig[1]:0x%08x\n",
		current->signal->shared_pending.signal.sig[0], 
		current->signal->shared_pending.signal.sig[1]);	
	if(test_bit(MIS_BIT_UNIKERNEL, &proc->p_usr.p_misc_flags)){
		if( sigismember(&current->pending.signal,  SIGALRM	) ||
			sigismember(&current->signal->shared_pending.signal, SIGALRM	)){
			DVKDEBUG(INTERNAL,"SIGALRM	 received\n");
			continue;
		}
	}
EL PROBLEMA SIGUE ESTANDO AUN CUANDO SE SUPRIMA LA SIGALRM 
	Aug 25 14:49:51 node0 kernel: [  262.825712] DEBUG 681:sleep_proc:354: timeout=-1
	Aug 25 14:49:51 node0 kernel: [  262.825713] DEBUG 681:sleep_proc:365: BEFORE DOWN lpid=681 p_sem=0 timeout=-1
	Aug 25 14:49:51 node0 kernel: [  262.825714] DEBUG 681:sleep_proc:367: endpoint=10 flags=8
	Aug 25 14:49:51 node0 kernel: [  262.825716] DEBUG 681:sleep_proc:371: WUNLOCK_PROC ep=10 count=9
	Aug 25 14:49:51 node0 kernel: [  262.825717] DEBUG 681:sleep_proc:379: pending: sig[0]:0x00000000, sig[1]:0x00000000
	Aug 25 14:49:51 node0 kernel: [  262.825719] DEBUG 681:sleep_proc:382: shared_pending sig[0]:0x00002000, sig[1]:0x00000000
	Aug 25 14:49:51 node0 kernel: [  262.825720] DEBUG 681:sleep_proc:386: SIGALRM	 received
	Aug 25 14:49:51 node0 kernel: [  262.825721] DEBUG 681:sleep_proc:371: WUNLOCK_PROC ep=10 count=10
	Aug 25 14:49:51 node0 kernel: [  262.825722] DEBUG 681:sleep_proc:379: pending: sig[0]:0x00000000, sig[1]:0x00000000
	Aug 25 14:49:51 node0 kernel: [  262.825723] DEBUG 681:sleep_proc:382: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
	Aug 25 14:49:51 node0 kernel: [  262.825725] DEBUG 681:sleep_proc:397: endpoint=10 ret=-512 p_rcode=0
	Aug 25 14:49:51 node0 kernel: [  262.825726] DEBUG 681:sleep_proc:398: endpoint=10 flags=8 cpuid=1
	Aug 25 14:49:51 node0 kernel: [  262.825727] DEBUG 681:sleep_proc:399: WLOCK_PROC ep=10 count=10
	Aug 25 14:49:51 node0 kernel: [  262.825729] DEBUG 681:sleep_proc:423: pid=681 ret=-4

===============================================================================================================
20200825:
		DA ERROR EL UNBIND 
		[ 1455.424182] DEBUG 746:new_exit_unbind:266: code=-1015034154
		[ 1455.424183] DEBUG 746:new_exit_unbind:270: WLOCK_TASK pid=746 count=0
		[ 1455.424185] DEBUG 746:new_exit_unbind:276: RLOCK_PROC ep=10 count=4125
		[ 1455.424188] DEBUG 746:new_exit_unbind:277: nr=10 endp=10 dcid=0 flags=0 misc=420 lpid=746 vpid=2 nodeid=0 name=uml.linux 
		[ 1455.424190] DEBUG 746:new_exit_unbind:310:  Exiting endpoint=10 lpid=746
		[ 1455.424192] DEBUG 746:new_exit_unbind:312: RUNLOCK_PROC ep=10 count=4125
		[ 1455.424194] DEBUG 746:new_exit_unbind:313: WLOCK_DC dc=0 count=0
		[ 1455.424195] DEBUG 746:new_exit_unbind:314: WLOCK_PROC ep=10 count=4125
		[ 1455.424197] DEBUG 746:new_exit_unbind:321:  endpoint=10 lpid=746
		[ 1455.424200] DEBUG 746:do_unbind:731: nr=10 endp=10 dcid=0 flags=0 misc=420 lpid=746 vpid=2 nodeid=0 name=uml.linux 
		[ 1455.424203] DEBUG 746:do_unbind:760: Caller nr=10 endp=10 dcid=0 flags=0 misc=420 lpid=746 vpid=2 nodeid=0 name=uml.linux 
		[ 1455.424204] DEBUG 746:do_unbind:799: wakeup with error those processes trying to send a message to the proc
		[ 1455.424206] DEBUG 746:do_unbind:805: WUNLOCK_PROC ep=10 count=4125
		[ 1455.424207] DEBUG 746:do_unbind:806: WLOCK_PROC ep=1 count=352
		[ 1455.424209] DEBUG 746:do_unbind:807: WLOCK_PROC ep=10 count=4125
		[ 1455.424210] DEBUG 746:do_unbind:810: LIST_DEL
		[ 1455.424211] DEBUG 746:do_unbind:813: Find process 1 trying to send a message to 10
		[ 1455.424212] DEBUG 746:do_unbind:817: Wakeup SENDER with error ep=1  pid=-1
		[ 1455.424215] DEBUG 746:inherit_cpu:298: cpuid=2 vpid=-1
		[ 1455.424217] ERROR: 746:inherit_cpu:302: rcode=-3
		[ 1455.424220] DEBUG 746:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=-1 p_cpumask=FFFFFFFF nodemap=0 name=$noname 
		[ 1455.424221] DEBUG 746:do_unbind:818: BEFORE UP lpid=-1 p_sem=0 rcode=-109
		[ 1455.424223] DEBUG 746:do_unbind:823: WUNLOCK_PROC ep=1 count=352
		[ 1455.424224] DEBUG 746:do_unbind:805: WUNLOCK_PROC ep=10 count=4125
		[ 1455.424225] DEBUG 746:do_unbind:806: WLOCK_PROC ep=1 count=352
		[ 1455.424226] DEBUG 746:do_unbind:807: WLOCK_PROC ep=10 count=4125
		[ 1455.424227] DEBUG 746:do_unbind:810: LIST_DEL
		[ 1455.424229] ------------[ cut here ]------------
		[ 1455.424238] WARNING: CPU: 2 PID: 746 at lib/list_debug.c:53 __list_del_entry+0x60/0x100
		[ 1455.424240] list_del corruption, e78e46b4->next is LIST_POISON1 (00000100)
		[ 1455.424241] Modules linked in: iptable_filter fuse vmwgfx snd_ens1371 snd_ac97_codec ac97_bus gameport snd_rawmidi snd_seq_device snd_pcm snd_timer snd joydev ttm drm_kms_helper vmw_balloon serio_raw pcspkr evdev drm sg soundcore shpchp vmw_vmci ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq sr_mod cdrom sd_mod ata_generic hid_generic usbhid hid uhci_hcd ehci_pci ehci_hcd ata_piix libata xhci_pci psmouse xhci_hcd usbcore pcnet32 mii mptspi scsi_transport_spi mptscsih mptbase scsi_mod i2c_piix4
		[ 1455.424299] CPU: 2 PID: 746 Comm: uml.linux Tainted: G        W       4.9.88 #83
		[ 1455.424300] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
		[ 1455.424302]  e8e4be10 c332e222 e8e4be24 00000000 c3067a3a c382dd50 e8e4be44 000002ea
		[ 1455.424307]  c382d634 00000035 c334cf40 00000035 00000009 c334cf40 e78e46b4 e78e47d8
		[ 1455.424312]  0000004c e8e4be30 c3067aa6 00000009 00000000 e8e4be24 c382dd50 e8e4be44
		[ 1455.424316] Call Trace:
		[ 1455.424323]  [<c332e222>] ? dump_stack+0x55/0x73
		[ 1455.424328]  [<c3067a3a>] ? __warn+0xea/0x110
		[ 1455.424330]  [<c334cf40>] ? __list_del_entry+0x60/0x100
		[ 1455.424332]  [<c334cf40>] ? __list_del_entry+0x60/0x100
		[ 1455.424335]  [<c3067aa6>] ? warn_slowpath_fmt+0x46/0x60
		[ 1455.424337]  [<c334cf40>] ? __list_del_entry+0x60/0x100
		[ 1455.424339]  [<c334cfe8>] ? list_del+0x8/0x20
		[ 1455.424343]  [<c32693ed>] ? do_unbind+0x1dd/0x20c0
		[ 1455.424414]  [<c30c32f7>] ? vprintk_default+0x37/0x40
		[ 1455.424419]  [<c326d505>] ? new_exit_unbind+0x395/0x1210
		[ 1455.424421]  [<c306c5f2>] ? do_exit+0xa42/0xa50
		[ 1455.424424]  [<c306c677>] ? do_group_exit+0x37/0x90
		[ 1455.424426]  [<c306c6e6>] ? SyS_exit_group+0x16/0x20
		[ 1455.424428]  [<c3003728>] ? do_fast_syscall_32+0x98/0x160
		[ 1455.424432]  [<c363e282>] ? sysenter_past_esp+0x47/0x75
		[ 1455.424434] ---[ end trace 99bb2494cf383978 ]---
		[ 1455.424436] DEBUG 746:do_unbind:813: Find process 1 trying to send a message to 10
		[ 1455.424438] DEBUG 746:do_unbind:817: Wakeup SENDER with error ep=1  pid=-1
		
	
Aug 24 23:52:40 node0 kernel: [  165.990199] list_del corruption, e79058b4->next is LIST_POISON1 (00000100)
	
ERROR: 	El funcionamiento del dvk_rcvrqst es erroneo frente al dvk_receive!!
				BUSCAR DIFF
		POR LAS DUDAS comprobar dvk_reply() vs dvk_send()

	CLIENT->sendrec()
			DEBUG CLIENT:SYSC_ipc:165: ipc_mini_sendrec: call=1280 first=1 second=0 third=0 fifth=-1 
			DEBUG CLIENT:ipc_mini_sendrec:115: srcdst_ep=1 timeout_ms=-1
			DEBUG CLIENT:new_mini_sendrec:523: srcdst_ep=1
			DEBUG CLIENT:check_caller:609: caller_pid=CLIENT caller_tgid=CLIENT
			DEBUG CLIENT:check_caller:645: WLOCK_PROC ep=10 count=702
			DEBUG CLIENT:check_caller:675: WUNLOCK_PROC ep=10 count=702
			DEBUG CLIENT:check_caller:678: dcid=0
			DEBUG CLIENT:check_caller:682: RLOCK_DC dc=0 count=0
			DEBUG CLIENT:check_caller:686: RUNLOCK_DC dc=0 count=0
			DEBUG CLIENT:check_caller:692: caller_pid=CLIENT 
			DEBUG CLIENT:new_mini_sendrec:534: RLOCK_PROC ep=10 count=702
			DEBUG CLIENT:new_mini_sendrec:539: caller_nr=10 caller_ep=10 srcdst_ep=1 
			DEBUG CLIENT:new_mini_sendrec:546: RUNLOCK_PROC ep=10 count=702
			DEBUG CLIENT:new_mini_sendrec:548: dcid=0
			DEBUG CLIENT:new_mini_sendrec:552: RLOCK_DC dc=0 count=0
			DEBUG CLIENT:new_mini_sendrec:556: RUNLOCK_DC dc=0 count=0
			DEBUG CLIENT:new_mini_sendrec:571: RLOCK_PROC ep=10 count=702
			DEBUG CLIENT:new_mini_sendrec:578: RUNLOCK_PROC ep=10 count=702
			DEBUG CLIENT:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=703
			DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=1 count=702
			DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=10 count=703
			DEBUG CLIENT:new_mini_sendrec:586: srcdst_nr=1 srcdst_ep=1
			DEBUG CLIENT:new_mini_sendrec:606: srcdst_ptr->p_usr.p_nodeid=0
			DEBUG CLIENT:new_mini_sendrec:610: RLOCK_DC dc=0 count=0
			DEBUG CLIENT:new_mini_sendrec:614: RUNLOCK_DC dc=0 count=0
			DEBUG CLIENT:new_mini_sendrec:629: dcid=0 caller_pid=CLIENT caller_nr=10 srcdst_ep=1 
			DEBUG CLIENT:new_mini_sendrec:634: SENDING HALF
			DEBUG CLIENT:new_mini_sendrec:718: destination is waiting. Copy the message and wakeup destination
			DEBUG CLIENT:copy_usr2usr:1021: rqtr_ep=10 src_ep=10 src_lpid=CLIENT src_vpid=2 src_addr=48c91c58
			DEBUG CLIENT:copy_usr2usr:1024: dst_ep=1 dst_lpid=SERVER dst_vpid=SERVER dst_addr=004bf200 bytes=76
			DEBUG CLIENT:copy_usr2usr:1028: task_pid_nr(current)=CLIENT
			DEBUG CLIENT:copy_usr2usr:1031: WRITE
			DEBUG CLIENT:copy_usr2usr:1036: task_pid_nr(dst_ptr->p_task)=SERVER
			DEBUG CLIENT:dvk_vm_rw:451: pid=SERVER liovcnt=1 riovcnt=1 flags=0 vm_write=1
			DEBUG CLIENT:dvk_check_iovect:1283: type=1 nr_segs=1
			DEBUG CLIENT:dvk_rw_check_kvector:865: type=1 nr_segs=1
			DEBUG CLIENT:dvk_rw_check_kvector:865: type=-1 nr_segs=1
			DEBUG CLIENT:dvk_vm_rw_core:286: pid=SERVER riovcnt=1 flags=0 vm_write=1
			DEBUG CLIENT:dvk_vm_rw_core:340: i=0 rc=0
			DEBUG CLIENT:dvk_vm_rw_core:358: rc=76
			DEBUG CLIENT:dvk_vm_rw_core:365: rc=76
			DEBUG CLIENT:copy_usr2usr:1090: len=76
			DEBUG CLIENT:inherit_cpu:298: cpuid=2 vpid=SERVER
			ERROR: CLIENT:inherit_cpu:302: rcode=-3
			DEBUG CLIENT:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=SERVER p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
			DEBUG CLIENT:new_mini_sendrec:724: BEFORE UP lpid=SERVER p_sem=-1 rcode=76
					Se supone que todavia NO HIZO el 
					WUNLOCK_PROC(srcdst_ptr);
			
		SERVER rcvrqst()	
			DEBUG SERVER:sleep_proc:389: pending: sig[0]:0x00000000, sig[1]:0x00000000
			DEBUG SERVER:sleep_proc:392: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			DEBUG SERVER:sleep_proc:398: endpoint=1 ret=0 p_rcode=76
			DEBUG SERVER:sleep_proc:399: endpoint=1 flags=0 cpuid=3
			DEBUG SERVER:sleep_proc:400: WLOCK_PROC ep=1 count=701 
			<<<<<<<<<<<<<< PORQUE PUDO HACER ESTO SI SE SUPONE QUE ESTA BLOQUEADO AUN!! 
			DEBUG SERVER:sleep_proc:453: nr=1 endp=1 dcid=0 lpid=SERVER p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
			DEBUG SERVER:sleep_proc:455: someone wakeups me: sem=0 p_rcode=0
			DEBUG SERVER:new_mini_rcvrqst:1771: WUNLOCK_PROC ep=1 count=701
			
		SERVER vcopy()	- FALLA el estado de CLIENT no es correcto 
			DEBUG SERVER:SYSC_ipc:165: ipc_vcopy: call=3328 first=10 second=1115267072 third=1 fifth=4096 
			DEBUG SERVER:ipc_vcopy:259: src_ep=10 dst_ep=1 bytes=4096
			DEBUG SERVER:new_vcopy:1124: src_ep=10 dst_ep=1 bytes=4096
			DEBUG SERVER:check_caller:609: caller_pid=SERVER caller_tgid=SERVER
			DEBUG SERVER:check_caller:645: WLOCK_PROC ep=1 count=701
			DEBUG SERVER:check_caller:675: WUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:check_caller:678: dcid=0
			DEBUG SERVER:check_caller:682: RLOCK_DC dc=0 count=0
			DEBUG SERVER:check_caller:686: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:check_caller:692: caller_pid=SERVER 
			DEBUG SERVER:new_vcopy:1138: RLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1159: dcid=0
			DEBUG SERVER:new_vcopy:1169: RUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1171: RLOCK_DC dc=0 count=0
			DEBUG SERVER:new_vcopy:1194: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:new_vcopy:1197: RLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1216: (src_nr+dc_max_nr_tasks)=44
			DEBUG SERVER:new_vcopy:1222: RUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1227: RLOCK_DC dc=0 count=0
			DEBUG SERVER:new_vcopy:1228: LOCK PROCESSES IN ASCENDENT ORDER
			DEBUG SERVER:new_vcopy:1235: WLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1235: WLOCK_PROC ep=10 count=702
			DEBUG SERVER:new_vcopy:1239: CHECK FOR SOURCE/DESTINATION STATUS
			DEBUG SERVER:new_vcopy:1322: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:new_vcopy:1331: WUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_vcopy:1331: WUNLOCK_PROC ep=10 count=702
			ERROR: SERVER:new_vcopy:1334: rcode=-324
			
		SE PUDRE TODO 	
			DEBUG SERVER:SYSC_ipc:165: ipc_mini_reply: call=1792 first=10 second=0 third=0 fifth=30000 
			DEBUG SERVER:ipc_mini_reply:152: dst_ep=10 timeout_ms=30000
			DEBUG SERVER:new_mini_reply:1793: dst_ep=10
			DEBUG SERVER:check_caller:609: caller_pid=SERVER caller_tgid=SERVER
			DEBUG SERVER:check_caller:645: WLOCK_PROC ep=1 count=701
			DEBUG SERVER:check_caller:675: WUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:check_caller:678: dcid=0
			DEBUG SERVER:check_caller:682: RLOCK_DC dc=0 count=0
			DEBUG SERVER:check_caller:686: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:check_caller:692: caller_pid=SERVER 
			DEBUG SERVER:new_mini_reply:1803: RLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_mini_reply:1807: caller_nr=1 caller_ep=1 dst_ep=10 
			DEBUG SERVER:new_mini_reply:1814: RUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_mini_reply:1816: dcid=0
			DEBUG SERVER:new_mini_reply:1820: RLOCK_DC dc=0 count=0
			DEBUG SERVER:new_mini_reply:1825: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:new_mini_reply:1845: RLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_mini_reply:1852: RUNLOCK_PROC ep=1 count=701
			DEBUG SERVER:new_mini_reply:1855: WLOCK_PROC ep=10 count=702
			DEBUG SERVER:new_mini_reply:1860: dst_nr=10 dst_ep=10
			DEBUG SERVER:new_mini_reply:1879: dst_ptr->p_usr.p_nodeid=0
			DEBUG SERVER:new_mini_reply:1883: RLOCK_DC dc=0 count=0
			DEBUG SERVER:new_mini_reply:1887: RUNLOCK_DC dc=0 count=0
			DEBUG SERVER:new_mini_reply:1902: dcid=0 caller_pid=SERVER caller_nr=1 dst_ep=10 
			DEBUG SERVER:new_mini_reply:1996: destination is not waiting dst_flags=0. Enqueue at the TAIL.
			DEBUG SERVER:new_mini_reply:2002: WUNLOCK_PROC ep=10 count=702
			DEBUG SERVER:new_mini_reply:2004: WUNLOCK_PROC ep=1 count=702
			ERROR: SERVER:new_mini_reply:2005: rcode=-324
			DEBUG CLIENT:new_mini_sendrec:732: WUNLOCK_PROC ep=1 count=703
			DEBUG CLIENT:sleep_proc:355: timeout=-1
			DEBUG CLIENT:sleep_proc:366: BEFORE DOWN lpid=CLIENT p_sem=0 timeout=-1
			DEBUG CLIENT:sleep_proc:368: endpoint=10 flags=8
			DEBUG CLIENT:sleep_proc:375: new_set.sig[0]=0x10002000 old_set.sig[0]=0x00001000
			DEBUG CLIENT:sleep_proc:381: WUNLOCK_PROC ep=10 count=703

TAMBIEN PASA CON dvk_receive() O EL PROBLEMA ESTARA EN dvk_sendrec() ?????????????
		CLIENT dvk_sendrec() 
			Aug 26 19:41:57 node0 kernel: [32724.226140] DEBUG CLIENT:new_mini_sendrec:805: WUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226391] DEBUG CLIENT:SYSC_ipc:165: ipc_mini_sendrec: call=1280 first=1 second=0 third=0 fifth=-1 
			Aug 26 19:41:57 node0 kernel: [32724.226393] DEBUG CLIENT:ipc_mini_sendrec:115: srcdst_ep=1 timeout_ms=-1
			Aug 26 19:41:57 node0 kernel: [32724.226395] DEBUG CLIENT:new_mini_sendrec:523: srcdst_ep=1
			Aug 26 19:41:57 node0 kernel: [32724.226397] DEBUG CLIENT:check_caller:609: caller_pid=CLIENT caller_tgid=CLIENT
			Aug 26 19:41:57 node0 kernel: [32724.226399] DEBUG CLIENT:check_caller:645: WLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226400] DEBUG CLIENT:check_caller:675: WUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226401] DEBUG CLIENT:check_caller:678: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.226403] DEBUG CLIENT:check_caller:682: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226404] DEBUG CLIENT:check_caller:686: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226405] DEBUG CLIENT:check_caller:692: caller_pid=CLIENT 
			Aug 26 19:41:57 node0 kernel: [32724.226407] DEBUG CLIENT:new_mini_sendrec:534: RLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226408] DEBUG CLIENT:new_mini_sendrec:539: caller_nr=10 caller_ep=10 srcdst_ep=1 
			Aug 26 19:41:57 node0 kernel: [32724.226410] DEBUG CLIENT:new_mini_sendrec:546: RUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226411] DEBUG CLIENT:new_mini_sendrec:548: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.226412] DEBUG CLIENT:new_mini_sendrec:552: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226414] DEBUG CLIENT:new_mini_sendrec:556: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226415] DEBUG CLIENT:new_mini_sendrec:571: RLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226417] DEBUG CLIENT:new_mini_sendrec:578: RUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.226418] DEBUG CLIENT:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1647
			Aug 26 19:41:57 node0 kernel: [32724.226420] DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=1 count=1510
			Aug 26 19:41:57 node0 kernel: [32724.226421] DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1647
			Aug 26 19:41:57 node0 kernel: [32724.226422] DEBUG CLIENT:new_mini_sendrec:586: srcdst_nr=1 srcdst_ep=1
			Aug 26 19:41:57 node0 kernel: [32724.226424] DEBUG CLIENT:new_mini_sendrec:606: srcdst_ptr->p_usr.p_nodeid=0
			Aug 26 19:41:57 node0 kernel: [32724.226425] DEBUG CLIENT:new_mini_sendrec:610: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226426] DEBUG CLIENT:new_mini_sendrec:614: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.226428] DEBUG CLIENT:new_mini_sendrec:629: dcid=0 caller_pid=CLIENT caller_nr=10 srcdst_ep=1 
			Aug 26 19:41:57 node0 kernel: [32724.226429] DEBUG CLIENT:new_mini_sendrec:634: SENDING HALF
			Aug 26 19:41:57 node0 kernel: [32724.226430] DEBUG CLIENT:new_mini_sendrec:718: destination is waiting. Copy the message and wakeup destination
			Aug 26 19:41:57 node0 kernel: [32724.226433] DEBUG CLIENT:copy_usr2usr:1021: rqtr_ep=10 src_ep=10 src_lpid=CLIENT src_vpid=440 src_addr=47eb9c58
			Aug 26 19:41:57 node0 kernel: [32724.226435] DEBUG CLIENT:copy_usr2usr:1024: dst_ep=1 dst_lpid=SERVER dst_vpid=SERVER dst_addr=00428200 bytes=76
			Aug 26 19:41:57 node0 kernel: [32724.226436] DEBUG CLIENT:copy_usr2usr:1028: task_pid_nr(current)=CLIENT
			Aug 26 19:41:57 node0 kernel: [32724.226437] DEBUG CLIENT:copy_usr2usr:1031: WRITE
			Aug 26 19:41:57 node0 kernel: [32724.226439] DEBUG CLIENT:copy_usr2usr:1036: task_pid_nr(dst_ptr->p_task)=SERVER
			Aug 26 19:41:57 node0 kernel: [32724.226441] DEBUG CLIENT:dvk_vm_rw:451: pid=SERVER liovcnt=1 riovcnt=1 flags=0 vm_write=1
			Aug 26 19:41:57 node0 kernel: [32724.226443] DEBUG CLIENT:dvk_check_iovect:1283: type=1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.226445] DEBUG CLIENT:dvk_rw_check_kvector:865: type=1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.226447] DEBUG CLIENT:dvk_rw_check_kvector:865: type=-1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.226448] DEBUG CLIENT:dvk_vm_rw_core:286: pid=SERVER riovcnt=1 flags=0 vm_write=1
			Aug 26 19:41:57 node0 kernel: [32724.226455] DEBUG CLIENT:dvk_vm_rw_core:340: i=0 rc=0
			Aug 26 19:41:57 node0 kernel: [32724.226467] DEBUG CLIENT:dvk_vm_rw_core:358: rc=76
			Aug 26 19:41:57 node0 kernel: [32724.226468] DEBUG CLIENT:dvk_vm_rw_core:365: rc=76
			Aug 26 19:41:57 node0 kernel: [32724.226469] DEBUG CLIENT:copy_usr2usr:1090: len=76
			Aug 26 19:41:57 node0 kernel: [32724.226471] DEBUG CLIENT:inherit_cpu:298: cpuid=2 vpid=SERVER
			Aug 26 19:41:57 node0 kernel: [32724.226473] ERROR: CLIENT:inherit_cpu:302: rcode=-3
			Aug 26 19:41:57 node0 kernel: [32724.226475] DEBUG CLIENT:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=SERVER p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
			Aug 26 19:41:57 node0 kernel: [32724.226477] DEBUG CLIENT:new_mini_sendrec:724: BEFORE UP lpid=SERVER p_sem=-1 rcode=76
		DESPERTO AL SERVER PERO TODAVIA NO LO LIBERO!!

		SERVER despierta del dvk_receive()
			Aug 26 19:41:57 node0 kernel: [32724.229355] DEBUG SERVER:sleep_proc:389: pending: sig[0]:0x00000000, sig[1]:0x00000000
			Aug 26 19:41:57 node0 kernel: [32724.229360] DEBUG SERVER:sleep_proc:392: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			Aug 26 19:41:57 node0 kernel: [32724.229361] DEBUG SERVER:sleep_proc:398: endpoint=1 ret=0 p_rcode=76
			Aug 26 19:41:57 node0 kernel: [32724.229363] DEBUG SERVER:sleep_proc:399: endpoint=1 flags=0 cpuid=0
			
		PORQUE PUEDE HACER EL LOCK !!!!!!????????	QUE CARAJO ES count????
			Aug 26 19:41:57 node0 kernel: [32724.229365] DEBUG SERVER:sleep_proc:400: WLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229368] DEBUG SERVER:sleep_proc:453: nr=1 endp=1 dcid=0 lpid=SERVER p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
			Aug 26 19:41:57 node0 kernel: [32724.229370] DEBUG SERVER:sleep_proc:455: someone wakeups me: sem=0 p_rcode=0
			Aug 26 19:41:57 node0 kernel: [32724.229372] DEBUG SERVER:new_mini_receive:503: WUNLOCK_PROC ep=1 count=1509
			
			Aug 26 19:41:57 node0 kernel: [32724.229455] DEBUG SERVER:SYSC_ipc:165: ipc_vcopy: call=3328 first=10 second=1166204928 third=1 fifth=4096 
			Aug 26 19:41:57 node0 kernel: [32724.229457] DEBUG SERVER:ipc_vcopy:259: src_ep=10 dst_ep=1 bytes=4096
			Aug 26 19:41:57 node0 kernel: [32724.229459] DEBUG SERVER:new_vcopy:1124: src_ep=10 dst_ep=1 bytes=4096
			Aug 26 19:41:57 node0 kernel: [32724.229463] DEBUG SERVER:check_caller:609: caller_pid=SERVER caller_tgid=SERVER
			Aug 26 19:41:57 node0 kernel: [32724.229464] DEBUG SERVER:check_caller:645: WLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229466] DEBUG SERVER:check_caller:675: WUNLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229467] DEBUG SERVER:check_caller:678: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.229469] DEBUG SERVER:check_caller:682: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229470] DEBUG SERVER:check_caller:686: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229471] DEBUG SERVER:check_caller:692: caller_pid=SERVER 
			Aug 26 19:41:57 node0 kernel: [32724.229473] DEBUG SERVER:new_vcopy:1138: RLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229474] DEBUG SERVER:new_vcopy:1159: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.229475] DEBUG SERVER:new_vcopy:1169: RUNLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229477] DEBUG SERVER:new_vcopy:1171: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229478] DEBUG SERVER:new_vcopy:1194: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229480] DEBUG SERVER:new_vcopy:1197: RLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229481] DEBUG SERVER:new_vcopy:1216: (src_nr+dc_max_nr_tasks)=44
			Aug 26 19:41:57 node0 kernel: [32724.229482] DEBUG SERVER:new_vcopy:1222: RUNLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229484] DEBUG SERVER:new_vcopy:1227: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229486] DEBUG SERVER:new_vcopy:1228: LOCK PROCESSES IN ASCENDENT ORDER
			Aug 26 19:41:57 node0 kernel: [32724.229487] DEBUG SERVER:new_vcopy:1235: WLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229489] DEBUG SERVER:new_vcopy:1235: WLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.229490] DEBUG SERVER:new_vcopy:1239: CHECK FOR SOURCE/DESTINATION STATUS
			Aug 26 19:41:57 node0 kernel: [32724.229494] DEBUG SERVER:new_vcopy:1322: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.229495] DEBUG SERVER:new_vcopy:1331: WUNLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.229496] DEBUG SERVER:new_vcopy:1331: WUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.229498] ERROR: SERVER:new_vcopy:1334: rcode=-324

				
				
		ESTE ES EL FUNCIONAMIENTO CUANDO ANDA BIEN 
		CLIENT dvk_sendrec()
			Aug 26 19:41:57 node0 kernel: [32724.221359] DEBUG CLIENT:SYSC_ipc:165: ipc_mini_sendrec: call=1280 first=1 second=0 third=0 fifth=-1 
			Aug 26 19:41:57 node0 kernel: [32724.221363] DEBUG CLIENT:ipc_mini_sendrec:115: srcdst_ep=1 timeout_ms=-1
			Aug 26 19:41:57 node0 kernel: [32724.221366] DEBUG CLIENT:new_mini_sendrec:523: srcdst_ep=1
			Aug 26 19:41:57 node0 kernel: [32724.221370] DEBUG CLIENT:check_caller:609: caller_pid=CLIENT caller_tgid=CLIENT
			Aug 26 19:41:57 node0 kernel: [32724.221372] DEBUG CLIENT:check_caller:645: WLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221373] DEBUG CLIENT:check_caller:675: WUNLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221375] DEBUG CLIENT:check_caller:678: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.221377] DEBUG CLIENT:check_caller:682: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221378] DEBUG CLIENT:check_caller:686: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221379] DEBUG CLIENT:check_caller:692: caller_pid=CLIENT 
			Aug 26 19:41:57 node0 kernel: [32724.221381] DEBUG CLIENT:new_mini_sendrec:534: RLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221382] DEBUG CLIENT:new_mini_sendrec:539: caller_nr=10 caller_ep=10 srcdst_ep=1 
			Aug 26 19:41:57 node0 kernel: [32724.221384] DEBUG CLIENT:new_mini_sendrec:546: RUNLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221385] DEBUG CLIENT:new_mini_sendrec:548: dcid=0
			Aug 26 19:41:57 node0 kernel: [32724.221386] DEBUG CLIENT:new_mini_sendrec:552: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221387] DEBUG CLIENT:new_mini_sendrec:556: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221389] DEBUG CLIENT:new_mini_sendrec:571: RLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221390] DEBUG CLIENT:new_mini_sendrec:578: RUNLOCK_PROC ep=10 count=1645
			Aug 26 19:41:57 node0 kernel: [32724.221391] DEBUG CLIENT:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.221393] DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.221394] DEBUG CLIENT:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1646
			Aug 26 19:41:57 node0 kernel: [32724.221396] DEBUG CLIENT:new_mini_sendrec:586: srcdst_nr=1 srcdst_ep=1
			Aug 26 19:41:57 node0 kernel: [32724.221397] DEBUG CLIENT:new_mini_sendrec:606: srcdst_ptr->p_usr.p_nodeid=0
			Aug 26 19:41:57 node0 kernel: [32724.221398] DEBUG CLIENT:new_mini_sendrec:610: RLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221400] DEBUG CLIENT:new_mini_sendrec:614: RUNLOCK_DC dc=0 count=0
			Aug 26 19:41:57 node0 kernel: [32724.221401] DEBUG CLIENT:new_mini_sendrec:629: dcid=0 caller_pid=CLIENT caller_nr=10 srcdst_ep=1 
			Aug 26 19:41:57 node0 kernel: [32724.221402] DEBUG CLIENT:new_mini_sendrec:634: SENDING HALF
			Aug 26 19:41:57 node0 kernel: [32724.221404] DEBUG CLIENT:new_mini_sendrec:718: destination is waiting. Copy the message and wakeup destination
			Aug 26 19:41:57 node0 kernel: [32724.221407] DEBUG CLIENT:copy_usr2usr:1021: rqtr_ep=10 src_ep=10 src_lpid=CLIENT src_vpid=440 src_addr=47eb9c58
			Aug 26 19:41:57 node0 kernel: [32724.221409] DEBUG CLIENT:copy_usr2usr:1024: dst_ep=1 dst_lpid=13314 dst_vpid=13314 dst_addr=00428200 bytes=76
			Aug 26 19:41:57 node0 kernel: [32724.221410] DEBUG CLIENT:copy_usr2usr:1028: task_pid_nr(current)=CLIENT
			Aug 26 19:41:57 node0 kernel: [32724.221411] DEBUG CLIENT:copy_usr2usr:1031: WRITE
			Aug 26 19:41:57 node0 kernel: [32724.221413] DEBUG CLIENT:copy_usr2usr:1036: task_pid_nr(dst_ptr->p_task)=13314
			Aug 26 19:41:57 node0 kernel: [32724.221416] DEBUG CLIENT:dvk_vm_rw:451: pid=13314 liovcnt=1 riovcnt=1 flags=0 vm_write=1
			Aug 26 19:41:57 node0 kernel: [32724.221418] DEBUG CLIENT:dvk_check_iovect:1283: type=1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.221419] DEBUG CLIENT:dvk_rw_check_kvector:865: type=1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.221422] DEBUG CLIENT:dvk_rw_check_kvector:865: type=-1 nr_segs=1
			Aug 26 19:41:57 node0 kernel: [32724.221424] DEBUG CLIENT:dvk_vm_rw_core:286: pid=13314 riovcnt=1 flags=0 vm_write=1
			Aug 26 19:41:57 node0 kernel: [32724.221430] DEBUG CLIENT:dvk_vm_rw_core:340: i=0 rc=0
			Aug 26 19:41:57 node0 kernel: [32724.221445] DEBUG CLIENT:dvk_vm_rw_core:358: rc=76
			Aug 26 19:41:57 node0 kernel: [32724.221446] DEBUG CLIENT:dvk_vm_rw_core:365: rc=76
			Aug 26 19:41:57 node0 kernel: [32724.221447] DEBUG CLIENT:copy_usr2usr:1090: len=76
			Aug 26 19:41:57 node0 kernel: [32724.221449] DEBUG CLIENT:inherit_cpu:298: cpuid=2 vpid=13314
			Aug 26 19:41:57 node0 kernel: [32724.221451] ERROR: CLIENT:inherit_cpu:302: rcode=-3
			Aug 26 19:41:57 node0 kernel: [32724.221454] DEBUG CLIENT:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=13314 p_cpumask=FFFFFFFF nodemap=1 name=rhostfsd 
			Aug 26 19:41:57 node0 kernel: [32724.221456] DEBUG CLIENT:new_mini_sendrec:724: BEFORE UP lpid=13314 p_sem=-1 rcode=76
		AQUI LIBERA AL SERVER!!!! ESTA BIEN 
			Aug 26 19:41:57 node0 kernel: [32724.221491] DEBUG CLIENT:new_mini_sendrec:732: WUNLOCK_PROC ep=1 count=1509
			Aug 26 19:41:57 node0 kernel: [32724.221493] DEBUG CLIENT:sleep_proc:355: timeout=-1
			Aug 26 19:41:57 node0 kernel: [32724.221494] DEBUG CLIENT:sleep_proc:366: BEFORE DOWN lpid=CLIENT p_sem=0 timeout=-1
			Aug 26 19:41:57 node0 kernel: [32724.221496] DEBUG CLIENT:sleep_proc:368: endpoint=10 flags=8
			Aug 26 19:41:57 node0 kernel: [32724.221497] DEBUG CLIENT:sleep_proc:375: new_set.sig[0]=0x10002000 old_set.sig[0]=0x00001000
			Aug 26 19:41:57 node0 kernel: [32724.221499] DEBUG CLIENT:sleep_proc:381: WUNLOCK_PROC ep=10 count=1646


	ERROR!! Hay un WUNLOCK_PROC adicional 

	root@node0:/var/log# grep "WLOCK_PROC ep=10" kern.log | wc 
>>>>>  7537   82907  774089
	root@node0:/var/log# grep "WUNLOCK_PROC ep=10" kern.log | wc 
>>>>>  9045   99495  953368

	root@node0:/var/log# grep "WLOCK_PROC ep=1 " kern.log | wc 
>>>>>  10427  114697 1062920
	root@node0:/var/log# grep "WUNLOCK_PROC ep=1 " kern.log | wc 
>>>>   11936  131296 1242511

Aug 26 20:11:04 node0 kernel: [34471.573473] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1942
Aug 26 20:11:15 node0 kernel: [34482.042370] DEBUG 13328:check_caller:645: WLOCK_PROC ep=10 count=1942
Aug 26 20:11:15 node0 kernel: [34482.042393] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1943
Aug 26 20:11:16 node0 kernel: [34483.350879] DEBUG 13328:check_caller:645: WLOCK_PROC ep=10 count=1943
Aug 26 20:11:16 node0 kernel: [34483.350901] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1944
Aug 26 20:11:20 node0 kernel: [34486.897860] DEBUG 13328:check_caller:645: WLOCK_PROC ep=10 count=1944
Aug 26 20:11:20 node0 kernel: [34486.897883] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1945
Aug 26 20:11:20 node0 kernel: [34487.179489] DEBUG 13328:check_caller:645: WLOCK_PROC ep=10 count=1945
Aug 26 20:11:20 node0 kernel: [34487.179512] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=1946
Aug 26 20:11:23 node0 kernel: [34489.947227] DEBUG 13328:new_exit_unbind:314: WLOCK_PROC ep=10 count=1946

Aug 26 20:11:04 node0 kernel: [34471.573475] DEBUG 13328:new_mini_sendrec:617: WUNLOCK_PROC ep=10 count=1942
Aug 26 20:11:15 node0 kernel: [34482.042371] DEBUG 13328:check_caller:675: WUNLOCK_PROC ep=10 count=1942
Aug 26 20:11:15 node0 kernel: [34482.042390] DEBUG 13328:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1943
Aug 26 20:11:15 node0 kernel: [34482.042395] DEBUG 13328:new_mini_sendrec:617: WUNLOCK_PROC ep=10 count=1943
Aug 26 20:11:16 node0 kernel: [34483.350880] DEBUG 13328:check_caller:675: WUNLOCK_PROC ep=10 count=1943
Aug 26 20:11:16 node0 kernel: [34483.350898] DEBUG 13328:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1944
Aug 26 20:11:16 node0 kernel: [34483.350904] DEBUG 13328:new_mini_sendrec:617: WUNLOCK_PROC ep=10 count=1944
Aug 26 20:11:20 node0 kernel: [34486.897862] DEBUG 13328:check_caller:675: WUNLOCK_PROC ep=10 count=1944
Aug 26 20:11:20 node0 kernel: [34486.897880] DEBUG 13328:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1945
Aug 26 20:11:20 node0 kernel: [34486.897885] DEBUG 13328:new_mini_sendrec:617: WUNLOCK_PROC ep=10 count=1945
Aug 26 20:11:20 node0 kernel: [34487.179490] DEBUG 13328:check_caller:675: WUNLOCK_PROC ep=10 count=1945
Aug 26 20:11:20 node0 kernel: [34487.179509] DEBUG 13328:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=1946
Aug 26 20:11:20 node0 kernel: [34487.179514] DEBUG 13328:new_mini_sendrec:617: WUNLOCK_PROC ep=10 count=1946
Aug 26 20:11:23 node0 kernel: [34489.947282] DEBUG 13328:new_exit_unbind:385: WUNLOCK_PROC ep=10 count=1946
		


Aug 26 19:38:49 node0 kernel: [32536.500245] DEBUG 13328:SYSC_ipc:165: ipc_mini_sendrec: call=1280 first=1 second=0 third=0 fifth=-1 

Aug 26 19:38:49 node0 kernel: [32536.500251] DEBUG 13328:check_caller:645: WLOCK_PROC ep=10 count=955
Aug 26 19:38:49 node0 kernel: [32536.500253] DEBUG 13328:check_caller:675: WUNLOCK_PROC ep=10 count=955

Aug 26 19:38:49 node0 kernel: [32536.500259] DEBUG 13328:new_mini_sendrec:534: RLOCK_PROC ep=10 count=955
Aug 26 19:38:49 node0 kernel: [32536.500262] DEBUG 13328:new_mini_sendrec:546: RUNLOCK_PROC ep=10 count=955

AQUI ESTA EL PROBLEMA!!! 
Aug 26 19:38:49 node0 kernel: [32536.500267] DEBUG 13328:new_mini_sendrec:571: RLOCK_PROC ep=10 count=955
Aug 26 19:38:49 node0 kernel: [32536.500268] DEBUG 13328:new_mini_sendrec:578: RUNLOCK_PROC ep=10 count=955
Aug 26 19:38:49 node0 kernel: [32536.500269] DEBUG 13328:new_mini_sendrec:584: WUNLOCK_PROC ep=10 count=956

Aug 26 19:38:49 node0 kernel: [32536.500272] DEBUG 13328:new_mini_sendrec:584: WLOCK_PROC ep=10 count=956
Aug 26 19:38:49 node0 kernel: [32536.500343] DEBUG 13328:sleep_proc:381: WUNLOCK_PROC ep=10 count=956
===============================================================================================================
20200827:
	
SE MODIFICARON VARIOS ARCHIVOS DEL DVK-MOD 
ERRORES IMPORTANTES DE LOCK/UNLOCK 
TODO IPC AFECTADO 

SE  PROBARON LOS IPC BASICOS EN UML 
	
Se modifico test_dvs_init para que se pueda setear que esta en USERSPACE
TODOS LOS PROCESOS LOCALES BINDEADOS EN UN DVS USERSPACE DEBEN SETEARSE CON EL BIT_USERSPACE
	
EN UML 
	name=TEST_CLUSTER
	nodeid=1
	nr_dcs=32
	nr_nodes=32
	max_nr_procs=221
	max_nr_tasks=35
	max_sys_procs=64
	max_copy_buf=65536
	max_copy_len=1048576
	dbglvl=FFFFFF
	version=5
	flags=105 <<<<<<<<<<<<<<<<<<<<<<< USERMODE=KERNEL DE UML 
	sizeof(proc)=488
	sizeof(proc) aligned=512
	sizeof(dc)=152
	sizeof(node)=80

EL HOST
	name=TEST_CLUSTER
	nodeid=0
	nr_dcs=32
	nr_nodes=32
	max_nr_procs=221
	max_nr_tasks=35
	max_sys_procs=64
	max_copy_buf=65536
	max_copy_len=1048576
	dbglvl=FFFFFF
	version=5
	flags=101 <<<<<<<<<<<<<<<<<<<<<<<<< KERNEL DE HOST 
	sizeof(proc)=504
	sizeof(proc) aligned=512
	sizeof(dc)=164
	sizeof(node)=92

===============================================================================================================
20200830:
	Se hicierno MUCHAS modificaciones en el uso de DC (dc_ptr) y todo lo relacionado con el LOCK.
	Para probar parcialmente estas modificaciones, primero se prueban en UML 
	Al ejecutar test_uml.sh y tratar de ejecutar dc_init() se queda bloqueado.
	Aparentemente debe haber un LOCK previo en el DC.

		d_dbglvl=FFFFFF version=5 flags=101 sizeof(proc)=512
		do_exit: local_nodeid:0
		DC0 Enter para continuar... 
		dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=35 dc_nr_sysprocs=64 dc_nr_nodes=32
		flags=0 dc_nodes=0 dc_pid=0 dc_name=DC0
		dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
		DEBUG 723:dvk_open:101: 
		before dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=35 dc_nr_sysprocs=64 dc_nr_nodes=32
		before flags=0 dc_nodes=0 dc_pid=0 dc_name=DC0
		before dc_dcid=0 dc_warn2proc=27342 dc_warnmsg=-1
		DEBUG 723:dvk_dc_init:177: 
		DEBUG SYSC_ipc:165: ipc_dc_init: call=256 first=0 second=0 third=0 fifth=0 
		DEBUG new_dc_init:425: Current user=0 
		DEBUG new_dc_init:441: This process is running on cpu_id=0 
		DEBUG new_dc_init:446: dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=35 dc_nr_sysprocs=64 dc_nr_nodes=32
		DEBUG new_dc_init:447: flags=0 dc_nodes=0 dc_pid=0 dc_name=DC0

	EN kern.log de UML 
			Aug 30 23:20:13 node1 kernel: DEBUG SYSC_ipc:165: ipc_dvs_init: call=6144 first=1 second=0 third=0 fifth=0 
			Aug 30 23:20:13 node1 kernel: ERROR: 718:new_dvs_init:35: rcode=-317  <<< EDVSDVSBUSY
			Aug 30 23:20:13 node1 kernel: ERROR: 718:SYSC_ipc:167: rcode=-317
	
			Aug 30 23:20:13 node1 kernel: DEBUG SYSC_ipc:165: ipc_getdvsinfo: call=6912 first=0 second=0 third=0 fifth=0 
			Aug 30 23:20:13 node1 kernel: DEBUG new_getdvsinfo:1977: local_nodeid=0 
			Aug 30 23:20:13 node1 kernel: ERROR: 718:SYSC_ipc:167: rcode=0
			
			Aug 30 23:20:13 node1 kernel: do_exit: local_nodeid:0 <<<<<<<<<< PORQUE LOCALNODEID = 0 SI YO PUSE 1 EN EL INIT_DVS !!!!
			Aug 30 23:20:14 node1 kernel: do_exit: local_nodeid:0
			Aug 30 23:20:26 node1 kernel: DEBUG SYSC_ipc:165: ipc_dc_init: call=256 first=0 second=0 third=0 fifth=0 
			Aug 30 23:20:26 node1 kernel: DEBUG new_dc_init:425: Current user=0 
			Aug 30 23:20:26 node1 kernel: DEBUG new_dc_init:441: This process is running on cpu_id=0 
			Aug 30 23:20:26 node1 kernel: DEBUG new_dc_init:446: dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=35 dc_nr_sysprocs=64 dc_nr_nodes=32
			Aug 30 23:20:26 node1 kernel: DEBUG new_dc_init:447: flags=0 dc_nodes=0 dc_pid=0 dc_name=DC0
	
	HABRA ALGUN CONFLICTO ENTRE EL local_nodeid DEL KERNEL con el local_nodeid del DVK en USERSPACE ??
	
	
===============================================================================================================
20200901:
		Cuando trate de hacer test_unbind de un uml.linux colgado da esto.
		
		Sep  1 14:10:29 node0 kernel: [12388.834839] DEBUG 17916:SYSC_ipc:165: ipc_unbind: call=2560 first=0 second=1 third=-1 fifth=0 
		Sep  1 14:10:29 node0 kernel: [12388.834844] DEBUG 17916:ipc_unbind:215: dcid=0 proc_ep=1 timeout_ms=-1
		Sep  1 14:10:29 node0 kernel: [12388.834846] DEBUG 17916:new_unbind:1811: dcid=0 proc_ep=1
		Sep  1 14:10:29 node0 kernel: [12388.834849] DEBUG 17916:new_unbind:1824: RLOCK_DC dc=0 count=0
		Sep  1 14:10:29 node0 kernel: [12388.834851] DEBUG 17916:new_unbind:1836: WLOCK_TASK pid=17916 count=0
		Sep  1 14:10:29 node0 kernel: [12388.834853] DEBUG 17916:new_unbind:1837: RLOCK_PROC ep=1 count=0
		Sep  1 14:10:29 node0 kernel: [12388.834854] DEBUG 17916:new_unbind:1838: RUNLOCK_DC dc=0 count=0
		
		Sep  1 14:10:29 node0 kernel: [12388.876700] BUG: unable to handle kernel NULL pointer dereference at 000001d8
		Sep  1 14:10:29 node0 kernel: [12388.877178] IP: [<c45f4042>] mutex_lock+0x12/0x30 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
		Sep  1 14:10:29 node0 kernel: [12388.888071] *pdpt = 00000000294ec001 *pde = 0000000000000000 
		Sep  1 14:10:29 node0 kernel: [12388.888136] 
		Sep  1 14:10:29 node0 kernel: [12388.888550] Oops: 0002 [#1] SMP
		Sep  1 14:10:29 node0 kernel: [12388.889064] Modules linked in: fuse iptable_filter vmwgfx ttm drm_kms_helper evdev vmw_balloon drm joydev vmw_vmci serio_raw pcspkr sg shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache sr_mod cdrom ata_generic hid_generic sd_mod usbhid hid ata_piix libata uhci_hcd xhci_pci psmouse xhci_hcd ehci_pci ehci_hcd usbcore pcnet32 mii mptspi scsi_transport_spi mptscsih mptbase scsi_mod i2c_piix4
		Sep  1 14:10:29 node0 kernel: [12388.893578] CPU: 2 PID: 17916 Comm: test_unbind Not tainted 4.9.88 #98
		Sep  1 14:10:29 node0 kernel: [12388.894680] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
		Sep  1 14:10:29 node0 kernel: [12388.897032] task: ebf41380 task.stack: e372a000
		Sep  1 14:10:29 node0 kernel: [12388.898288] EIP: 0060:[<c45f4042>] EFLAGS: 00010286 CPU: 2
		Sep  1 14:10:29 node0 kernel: [12388.899560] EIP is at mutex_lock+0x12/0x30
		Sep  1 14:10:29 node0 kernel: [12388.900897] EAX: 000001d8 EBX: 000001d8 ECX: 00000000 EDX: 80000000
		Sep  1 14:10:29 node0 kernel: [12388.902273] ESI: 00000000 EDI: ec384600 EBP: e372be94 ESP: e372be90
		Sep  1 14:10:29 node0 kernel: [12388.903727]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
		Sep  1 14:10:29 node0 kernel: [12388.905210] CR0: 80050033 CR2: 000001d8 CR3: 2a463ea0 CR4: 000006f0
		Sep  1 14:10:29 node0 kernel: [12388.906888] Stack:
		Sep  1 14:10:29 node0 kernel: [12388.908697]  000001d8 e372bf14 c4263a1b c47c20f4 000045fc c4616f38 0000072e 00000000
		Sep  1 14:10:29 node0 kernel: [12388.910415]  00000000 e372beec c40ba4fe 00000000 00000000 00000000 00000000 c49d8d00
		Sep  1 14:10:29 node0 kernel: [12388.912174]  0000003a 00000040 ebf41380 c4a16260 ec3847d8 c4a162a0 00000001 00000000
		Sep  1 14:10:29 node0 kernel: [12388.913986] Call Trace:
		Sep  1 14:10:29 node0 kernel: [12388.916010]  [<c4263a1b>] ? new_unbind+0x36b/0x18a0
		Sep  1 14:10:29 node0 kernel: [12388.920946]  [<c40ba4fe>] ? vprintk_emit+0x2ee/0x4f0
		Sep  1 14:10:29 node0 kernel: [12388.922917]  [<c40ba8e7>] ? vprintk_default+0x37/0x40
		Sep  1 14:10:29 node0 kernel: [12388.925222]  [<c4290e57>] ? ipc_unbind+0x27/0x80
		Sep  1 14:10:29 node0 kernel: [12388.927269]  [<c4290e30>] ? ipc_bind+0x90/0x90
		Sep  1 14:10:29 node0 kernel: [12388.929242]  [<c4257111>] ? SyS_ipc+0x61/0x3a0
		Sep  1 14:10:29 node0 kernel: [12388.931347]  [<c4003708>] ? do_fast_syscall_32+0x98/0x160
		Sep  1 14:10:29 node0 kernel: [12388.933390]  [<c45f71c2>] ? sysenter_past_esp+0x47/0x75
		Sep  1 14:10:29 node0 kernel: [12388.935444] Code: c3 90 8d b4 26 00 00 00 00 31 c0 5d c3 8d b6 00 00 00 00 8d bf 00 00 00 00 55 89 e5 53 0f 1f 44 00 00 89 c3 e8 a0 ef ff ff 89 d8 <f0> ff 08 79 05 e8 74 07 00 00 64 a1 48 f2 9a c4 89 43 10 5b 5d
		Sep  1 14:10:29 node0 kernel: [12388.942227] EIP: [<c45f4042>] 
		Sep  1 14:10:29 node0 kernel: [12388.942268] mutex_lock+0x12/0x30
		Sep  1 14:10:29 node0 kernel: [12388.944561]  SS:ESP 0068:e372be90
		Sep  1 14:10:29 node0 kernel: [12388.946858] CR2: 00000000000001d8
		Sep  1 14:10:29 node0 kernel: [12388.957511] ---[ end trace ef874f8c3fc272ff ]---
		Sep  1 14:10:29 node0 kernel: [12388.960205] do_exit: local_nodeid:0
		Sep  1 14:10:29 node0 kernel: [12388.963317] DEBUG 17916:new_exit_unbind:267: code=-998595402


	ATENCION MODIFIQUE SUSTANTIVAMENTE dvk_unbind() 


===============================================================================================================
20200902:

			LA VERSION DE LINUX KERNEL ESTA EN Makefile de /usr/src/linux 
				VERSION = 4
				PATCHLEVEL = 9
				SUBLEVEL = 88
				EXTRAVERSION = -DVK  <<<<<<<<<<<<<<<<<<< 
				NAME = Roaring Lionus
				
			root@node1:~# uname -a
			Linux node1 4.9.88-DVK #138 Wed Sep 2 01:56:05 -03 2020 i686 GNU/Linux

		TUVE QUE CAMBIAR LA OPCION DEFAULT DE GRUB 
		Para ver cuales son las entradas de menu:
			grep menuentry /boot/grub/grub.cfg
		Copiar la $menuentry_id_option 
		Luego editar 
			joe /etc/default/grub
		reemplazar la opcion GRUB_DEFAULT 
			GRUB_DEFAULT="gnulinux-4.9.88-DVK-advanced-903d63e4-e4db-43ec-acf6-9e6fed2a55d0"
		Luego ejecutar 
			update-grub
			update-grub2
		
===============================================================================================================
20200907:
		Modifique el lz4tcp_proxy_bat.c para que soporte envio del timestamp y de los PIDs de los procesos.
		
		YA RESUELTO:
			PROXY SENDER 
				 lz4tcp_proxy_bat.c:ps_start_serving:886:SPROXY: 593 cmd=0x1 dcid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=0
				 lz4tcp_proxy_bat.c:ps_start_serving:887:SPROXY: 593 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
				 lz4tcp_proxy_bat.c:ps_start_serving:891:SPROXY: 593 c_timestamp=1599484979.731862643
				DEBUG 593:dvk_getprocinfo:1165: dcid=0 p_nr=2 
				DEBUG 593:dvk_getprocinfo:1169: ipc ret=0
				DEBUG 593:dvk_getprocinfo:1165: dcid=0 p_nr=3 
				DEBUG 593:dvk_getprocinfo:1169: ipc ret=92
				 lz4tcp_proxy_bat.c:ps_start_serving:907:SPROXY: 593 c_flags=0x0 c_src_pid=619 c_dst_pid=92
				 lz4tcp_proxy_bat.c:ps_start_serving:918:SPROXY: source=2 type=0 m1i1=7 m1i2=7 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
				 lz4tcp_proxy_bat.c:ps_start_serving:924:SPROXY 593: Getting more messages
			PROXY RECEIVER 
				 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY: 594 c_timestamp=1599484979.740854273
				 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 594 c_flags=0x0 c_src_pid=638 c_dst_pid=92
			
	ATENCION: EL KERNEL SPROXY YA PONIA EL TIMESTAMP !!!
	
===============================================================================================================
20200908:
			Se compilo el kernel con ambas opciones: CONFIG_DVKIOCTL y CONFIG_DVKIPC
			Se compilaron las herramientas para library IOCTL.
			
			root@node0:/usr/src/dvs/dvk-mod# mknod /dev/dvk c 33 0
			root@node0:/usr/src/dvs/dvk-mod# cd /usr/src/dvs/dvk-mod/
			root@node0:/usr/src/dvs/dvk-mod# insmod dvk.ko dvk_major=33 dvk_minor=0 dvk_nr_devs=1 
			insmod: ERROR: could not insert module dvk.ko: Invalid module format

			DMESG:
			[  805.886264] dvk: disagrees about version of symbol module_layout
		
		
===============================================================================================================
20200909:		Ahora la version es 
				Linux node0 4.9.88-DVK #147 SMP Wed Sep 9 19:29:29 -03 2020 i686 GNU/Linux
				por lo que se cambio el main.c del modulo 
				MODULE_VERSION("4.9.88-DVK");

				Pero da este error 
				root@node0:/usr/src/dvs/dvk-tests# ./tests.sh 0 0
				lcl_nodeid=0 dcid=0
				Enter para continuar... 
				Spread Enter para continuar... 

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.442766] CPU: 2 PID: 1237 Comm: insmod Tainted: G           O    4.9.88-DVK #147

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.442852] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
				./tests.sh: lÃ­nea 20:  1237 Terminado (killed)      insmod dvk.ko dvk_major=33 dvk_minor=0 dvk_nr_devs=1

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443009] task: ed464080 task.stack: eb8a4000

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443536] Stack:

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443567]  d08a5c80 000002b8 d014e19b 00000000 d08a5ee8 00000000 13e38bdc 00000000

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443684]  eb8a5e2c d0002195 00000000 f7bf21e0 00000000 d08a5c80 f7bf2150 d08a5d28

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443799]  00000000 eb8a5e2c d019856f f0652e40 00000001 f7bf3b30 00000246 13e38bdc

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443916] Call Trace:

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.443954]  [<d014e19b>] ? free_pcppages_bulk+0x35b/0x460

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444020]  [<d0002195>] ? do_one_initcall+0x45/0x180

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444082]  [<d019856f>] ? kmem_cache_alloc_trace+0x31f/0x4f0

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444151]  [<d0146b1f>] ? do_init_module+0x21/0x1c3

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444212]  [<d0146b4e>] ? do_init_module+0x50/0x1c3

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444294]  [<d00e3d96>] ? load_module+0x2066/0x2690

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444357]  [<d00e45f1>] ? SyS_finit_module+0xb1/0x100

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444420]  [<d00036a8>] ? do_fast_syscall_32+0x98/0x160

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444532]  [<d05cba82>] ? sysenter_past_esp+0x47/0x75

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.444597] Code: 7f 85 db 0f 84 ac 00 00 00 c7 05 80 a1 c1 f8 01 00 00 00 89 1d a0 fb c0 f8 f6 05 8c f2 c0 f8 01 0f 85 c0 04 00 00 a1 a0 fb c0 f8 <f0> 80 48 34 02 f6 05 8c f2 c0 f8 01 0f 85 21 05 00 00 bb ef ff

				Message from syslogd@node0 at Sep  9 20:40:52 ...
				 kernel:[  296.451734] EIP: [<f8bcb405>] 
				dvk                   344064  1

		Si cambio el test_dvs_init a usar IOCTL, da este error 
			root@node0:/usr/src/dvs/dvk-tests# ./test_dvs_init -n 0  -D 16777215 -C TEST_CLUSTER
			DEBUG 812:dvk_open:108: Open dvk device file /dev/dvk
			ERROR: 812:dvk_open:111: rcode=-6
			ERROR: test_dvs_init.c:main:179: rcode=-6
			Initializing DVS. Local node ID 0... 
			DEBUG 812:dvk_dvs_init:552: nodeid=0
			DEBUG 812:dvk_dvs_init:564: ioctl ret=-1 errno=9
			ERROR: 812:dvk_dvs_init:566: rcode=-9
			DEBUG 812:dvk_dvs_init:569: ioctl ret=-9
			ERROR: 812:dvk_dvs_init:571: rcode=-9
			ERROR: test_dvs_init.c:main:183: rcode=-9
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			d_max_copybuf=65536 d_max_copylen=1048576
			d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
			Get DVS info
			DEBUG 812:dvk_getdvsinfo:258: 
			DEBUG 812:dvk_getdvsinfo:267: ioctl ret=-1 errno=9
			ERROR: 812:dvk_getdvsinfo:269: rcode=-9
			DEBUG 812:dvk_getdvsinfo:272: ioctl ret=-9
			ERROR: 812:dvk_getdvsinfo:274: rcode=-9
			local node ID -9... 
			d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			d_max_copybuf=65536 d_max_copylen=1048576
			d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0

root@node0:~# cd /usr/src/dvs/dvk-tests/
root@node0:/usr/src/dvs/dvk-tests# ./tests.sh 0 0 
lcl_nodeid=0 dcid=0
Enter para continuar... 
Spread Enter para continuar... 

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.267314] CPU: 3 PID: 622 Comm: insmod Tainted: G           O    4.9.88-DVK #149

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.267399] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.267520] task: e511a980 task.stack: e40b6000

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268065] Stack:

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268095]  ce8a5c80 000002d2 ce14e067 00000000 ce8a5ee8 00000000 15ff1eb9 00000000

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268214]  e40b7e2c ce002195 00000000 f7bf2588 00000000 ce8a5c80 f7bf2564 ce8a5d28

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268330]  00000000 e40b7e2c ce19856f ebe92e00 00000001 f781a3c0 00000246 15ff1eb9

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268446] Call Trace:

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268485]  [<ce14e067>] ? free_pcppages_bulk+0x227/0x460

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268567]  [<ce002195>] ? do_one_initcall+0x45/0x180

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268632]  [<ce19856f>] ? kmem_cache_alloc_trace+0x31f/0x4f0

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268701]  [<ce146b1f>] ? do_init_module+0x21/0x1c3

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268761]  [<ce146b4e>] ? do_init_module+0x50/0x1c3

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268822]  [<ce0e3d96>] ? load_module+0x2066/0x2690

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268883]  [<ce0e45f1>] ? SyS_finit_module+0xb1/0x100

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.268946]  [<ce0036a8>] ? do_fast_syscall_32+0x98/0x160

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.269011]  [<ce5cba82>] ? sysenter_past_esp+0x47/0x75

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.269071] Code: 7f 85 db 0f 84 ac 00 00 00 c7 05 80 61 8a f8 01 00 00 00 89 1d a0 bb 89 f8 f6 05 8c b2 89 f8 01 0f 85 c0 04 00 00 a1 a0 bb 89 f8 <f0> 80 48 34 02 f6 05 8c b2 89 f8 01 0f 85 21 05 00 00 bb ef ff

Message from syslogd@node0 at Sep 10 07:45:57 ...
 kernel:[  239.276689] EIP: [<f8857405>] 
./tests.sh: lÃ­nea 20:   622 Terminado (killed)      insmod dvk.ko dvk_major=33 dvk_minor=0 dvk_nr_devs=1
dvk                   344064  1


			root@node0:/usr/src/dvs/dvk-tests# dmesg 
			[  239.252341] dvk: loading out-of-tree module taints kernel.
			[  239.266357] BUG: unable to handle kernel paging request at ce268a34
			[  239.266456] IP: [<f8857405>] init_module+0x63/0x5d9 [dvk]
			[  239.266534] *pdpt = 000000000e974001 *pde = 000000000e2001e1 
			[  239.266650] Oops: 0003 [#1] SMP
			[  239.266690] Modules linked in: dvk(O+) iptable_filter fuse joydev vmw_balloon serio_raw vmwgfx pcspkr evdev ttm drm_kms_helper shpchp sg drm vmw_vmci ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sd_mod sr_mod cdrom ata_generic xhci_pci psmouse uhci_hcd ehci_pci xhci_hcd ehci_hcd mptspi scsi_transport_spi pcnet32 mptscsih mii usbcore i2c_piix4 ata_piix mptbase libata scsi_mod
			[  239.267314] CPU: 3 PID: 622 Comm: insmod Tainted: G           O    4.9.88-DVK #149
			[  239.267399] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
			[  239.267520] task: e511a980 task.stack: e40b6000
			[  239.267593] EIP: 0060:[<f8857405>] EFLAGS: 00010246 CPU: 3
			[  239.267664] EIP is at init_module+0x63/0x5d9 [dvk]
			[  239.267720] EAX: ce268a00 EBX: ce268a00 ECX: 00000000 EDX: f8892a53
			[  239.267791] ESI: f88573a2 EDI: ec3a2760 EBP: e40b7db8 ESP: e40b7d98
			[  239.267862]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
			[  239.267925] CR0: 80050033 CR2: ce268a34 CR3: 2b4562c0 CR4: 000006f0
			[  239.268065] Stack:
			[  239.268095]  ce8a5c80 000002d2 ce14e067 00000000 ce8a5ee8 00000000 15ff1eb9 00000000
			[  239.268214]  e40b7e2c ce002195 00000000 f7bf2588 00000000 ce8a5c80 f7bf2564 ce8a5d28
			[  239.268330]  00000000 e40b7e2c ce19856f ebe92e00 00000001 f781a3c0 00000246 15ff1eb9
			[  239.268446] Call Trace:
			[  239.268485]  [<ce14e067>] ? free_pcppages_bulk+0x227/0x460
			[  239.268567]  [<ce002195>] ? do_one_initcall+0x45/0x180
			[  239.268632]  [<ce19856f>] ? kmem_cache_alloc_trace+0x31f/0x4f0
			[  239.268701]  [<ce146b1f>] ? do_init_module+0x21/0x1c3
			[  239.268761]  [<ce146b4e>] ? do_init_module+0x50/0x1c3
			[  239.268822]  [<ce0e3d96>] ? load_module+0x2066/0x2690
			[  239.268883]  [<ce0e45f1>] ? SyS_finit_module+0xb1/0x100
			[  239.268946]  [<ce0036a8>] ? do_fast_syscall_32+0x98/0x160
			[  239.269011]  [<ce5cba82>] ? sysenter_past_esp+0x47/0x75
			[  239.269071] Code: 7f 85 db 0f 84 ac 00 00 00 c7 05 80 61 8a f8 01 00 00 00 89 1d a0 bb 89 f8 f6 05 8c b2 89 f8 01 0f 85 c0 04 00 00 a1 a0 bb 89 f8 <f0> 80 48 34 02 f6 05 8c b2 89 f8 01 0f 85 21 05 00 00 bb ef ff
			[  239.276689] EIP: [<f8857405>] 
			[  239.276744] init_module+0x63/0x5d9 [dvk]
			[  239.279136]  SS:ESP 0068:e40b7d98
			[  239.281698] CR2: 00000000ce268a34
			[  239.284094] ---[ end trace e9cad1ec388614a7 ]---
	
===============================================================================================================
20200910:
			Probando  copia de datos 
			root@node0:/usr/src/dvs/dvk-loops# nsenter -p -t$DC0 ./loop_dvk_copy1 10 65536

			[  578.489118] DEBUG 964:SYSC_ipc:166: ipc_unbind: call=2560 first=0 second=2 third=-1 fifth=0 
			[  578.489120] DEBUG 964:ipc_unbind:215: dcid=0 proc_ep=2 timeout_ms=-1
			[  578.489123] DEBUG 964:new_unbind:1818: dcid=0 proc_ep=2
			[  578.489126] DEBUG 964:check_caller:608: caller_pid=964 caller_tgid=964

			[  578.489128] DEBUG 964:check_caller:644: WLOCK_PROC ep=2 count=0
			[  578.489130] DEBUG 964:check_caller:674: WUNLOCK_PROC ep=2 count=0
			
			[  578.489132] DEBUG 964:check_caller:677: dcid=0
			
			[  578.489134] DEBUG 964:check_caller:681: RLOCK_DC dc=0 count=0
			[  578.489136] DEBUG 964:check_caller:685: RUNLOCK_DC dc=0 count=0
			
			[  578.489138] DEBUG 964:check_caller:691: caller_pid=964 
			
			[  578.489140] DEBUG 964:new_unbind:1834: RLOCK_DC dc=0 count=0
			[  578.489142] DEBUG 964:new_unbind:1846: RUNLOCK_DC dc=0 count=0
			
			[  578.489145] DEBUG 964:new_unbind:1848: RLOCK_PROC ep=2 count=0
			[  578.489147] DEBUG 964:new_unbind:1870: RUNLOCK_PROC ep=2 count=0
			
			[  578.489149] DEBUG 964:new_unbind:1873: RLOCK_PROC ep=2 count=0
			[  578.489151] DEBUG 964:new_unbind:1876: RUNLOCK_PROC ep=2 count=0
			
			[  578.489154] DEBUG 964:new_unbind:1888: WLOCK_TASK pid=964 count=0
			
			[  578.489156] DEBUG 964:new_unbind:1901: WLOCK_DC dc=0 count=0
			[  578.489158] DEBUG 964:new_unbind:1903: WLOCK_PROC ep=2 count=0 <<<<< EL PROCESO QUEDA BLOQUEADO, NO EL KERNEL 
			
			[  578.489851] DEBUG 965:sleep_proc2:823: pending: sig[0]:0x00000000, sig[1]:0x00000000
			[  578.489857] DEBUG 965:sleep_proc2:826: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			[  578.489860] DEBUG 965:sleep_proc2:833: endpoint=1 ret=0 p_rcode=76
			[  578.489863] DEBUG 965:sleep_proc2:834: endpoint=1 flags=0 cpuid=3
			[  578.489866] DEBUG 965:sleep_proc2:836: WLOCK_PROC ep=1 count=0 

		SOLUCIONADO en dvk_unbind()
			Sep 10 08:54:37 node0 kernel: [  104.468228] DEBUG 695:new_exit_unbind:267: code=-948491534
			Sep 10 08:54:37 node0 kernel: [  104.468232] DEBUG 695:new_exit_unbind:271: WLOCK_TASK pid=695 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468234] DEBUG 695:new_exit_unbind:277: RLOCK_PROC ep=1 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468238] DEBUG 695:new_exit_unbind:278: nr=1 endp=1 dcid=0 flags=1 misc=0 lpid=-1 vpid=-1 nodeid=-1 name=$noname 
			Sep 10 08:54:37 node0 kernel: [  104.468241] DEBUG 695:new_exit_unbind:311:  Exiting endpoint=1 lpid=-1
			Sep 10 08:54:37 node0 kernel: [  104.468243] DEBUG 695:new_exit_unbind:313: RUNLOCK_PROC ep=1 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468246] DEBUG 695:new_exit_unbind:314: WLOCK_DC dc=0 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468248] DEBUG 695:new_exit_unbind:315: WLOCK_PROC ep=1 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468250] DEBUG 695:new_exit_unbind:322:  endpoint=1 lpid=-1
			Sep 10 08:54:37 node0 kernel: [  104.468253] DEBUG 695:new_exit_unbind:386: WUNLOCK_PROC ep=1 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468255] DEBUG 695:new_exit_unbind:387: WUNLOCK_TASK pid=695 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468258] DEBUG 695:new_exit_unbind:388: WUNLOCK_DC dc=0 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468618] do_exit: local_nodeid:0

			Sep 10 08:54:37 node0 kernel: [  104.468696] DEBUG 694:new_exit_unbind:267: code=-948491534
			Sep 10 08:54:37 node0 kernel: [  104.468699] DEBUG 694:new_exit_unbind:271: WLOCK_TASK pid=694 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468701] DEBUG 694:new_exit_unbind:277: RLOCK_PROC ep=2 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468705] DEBUG 694:new_exit_unbind:278: nr=2 endp=2 dcid=0 flags=1 misc=0 lpid=-1 vpid=-1 nodeid=-1 name=$noname 
			Sep 10 08:54:37 node0 kernel: [  104.468707] DEBUG 694:new_exit_unbind:311:  Exiting endpoint=2 lpid=-1
			Sep 10 08:54:37 node0 kernel: [  104.468709] DEBUG 694:new_exit_unbind:313: RUNLOCK_PROC ep=2 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468711] DEBUG 694:new_exit_unbind:314: WLOCK_DC dc=0 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468714] DEBUG 694:new_exit_unbind:315: WLOCK_PROC ep=2 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468716] DEBUG 694:new_exit_unbind:322:  endpoint=2 lpid=-1
			Sep 10 08:54:37 node0 kernel: [  104.468718] DEBUG 694:new_exit_unbind:386: WUNLOCK_PROC ep=2 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468720] DEBUG 694:new_exit_unbind:387: WUNLOCK_TASK pid=694 count=0
			Sep 10 08:54:37 node0 kernel: [  104.468722] DEBUG 694:new_exit_unbind:388: WUNLOCK_DC dc=0 count=0

==============================================================================================================
20200910:
	EN TRANSFERENCIA REMOTA NODO0 AL TERMINAR QUEDA ESPERANDO POR EL DEL NODO1 
	
			Sep 11 07:49:08 node0 kernel: [  243.322470] DEBUG 769:SYSC_ipc:166: ipc_unbind: call=2560 first=0 second=3 third=-1 fifth=0 
			Sep 11 07:49:08 node0 kernel: [  243.322472] DEBUG 769:ipc_unbind:215: dcid=0 proc_ep=3 timeout_ms=-1
			Sep 11 07:49:08 node0 kernel: [  243.322475] DEBUG 769:new_unbind:1818: dcid=0 proc_ep=3
			Sep 11 07:49:08 node0 kernel: [  243.322478] DEBUG 769:check_caller:608: caller_pid=769 caller_tgid=769  
			
			Sep 11 07:49:08 node0 kernel: [  243.322479] DEBUG 769:check_caller:644: WLOCK_PROC ep=2 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322481] DEBUG 769:check_caller:674: WUNLOCK_PROC ep=2 count=0
			
			Sep 11 07:49:08 node0 kernel: [  243.322482] DEBUG 769:check_caller:677: dcid=0
			
			Sep 11 07:49:08 node0 kernel: [  243.322483] DEBUG 769:check_caller:681: RLOCK_DC dc=0 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322484] DEBUG 769:check_caller:685: RUNLOCK_DC dc=0 count=0
			
			Sep 11 07:49:08 node0 kernel: [  243.322485] DEBUG 769:check_caller:691: caller_pid=769 
			
			Sep 11 07:49:08 node0 kernel: [  243.322487] DEBUG 769:new_unbind:1834: RLOCK_DC dc=0 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322488] DEBUG 769:new_unbind:1846: RUNLOCK_DC dc=0 count=0
			
			Sep 11 07:49:08 node0 kernel: [  243.322491] DEBUG 769:new_unbind:1848: RLOCK_PROC ep=3 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322492] DEBUG 769:new_unbind:1870: RUNLOCK_PROC ep=3 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322493] DEBUG 769:new_unbind:1873: RLOCK_PROC ep=2 count=0
			Sep 11 07:49:08 node0 kernel: [  243.322495] DEBUG 769:new_unbind:1894: RUNLOCK_PROC ep=2 count=0
			
		Sep 11 07:49:08 node0 kernel: [  243.322497] DEBUG 769:new_unbind:1901: WLOCK_DC dc=0 count=0 <<<<<<<<<<<<
			
			Sep 11 07:49:08 node0 kernel: [  243.322498] DEBUG 769:new_unbind:1903: WLOCK_PROC ep=2 count=0  <<<< 2??
			Sep 11 07:49:08 node0 kernel: [  243.322500] DEBUG 769:new_unbind:1903: WLOCK_PROC ep=3 count=0  <<<< 3??
			
			Sep 11 07:49:08 node0 kernel: [  243.322503] DEBUG 769:do_unbind:738: nr=3 endp=3 dcid=0 flags=1000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=client3 
			Sep 11 07:49:08 node0 kernel: [  243.322506] DEBUG 769:do_unbind:767: Caller nr=2 endp=2 dcid=0 flags=0 misc=20 lpid=769 vpid=3 nodeid=0 name=loop_r-s_server 
			Sep 11 07:49:08 node0 kernel: [  243.322508] DEBUG 769:do_unbind:806: wakeup with error those processes trying to send a message to the proc
			Sep 11 07:49:08 node0 kernel: [  243.322509] DEBUG 769:do_unbind:833: delete notify messages bits sent by the proc

			Sep 11 07:49:08 node0 kernel: [  243.322521] DEBUG 769:do_unbind:881: WUNLOCK_PROC ep=0 count=0 <<<< 0??
			Sep 11 07:49:08 node0 kernel: [  243.322523] DEBUG 769:do_unbind:881: WUNLOCK_PROC ep=1 count=0 <<<< 1??
			
		FALTA LIBERAR AL DC !!!!! 
		
	LUEGO, CUANDO SE HACE EL 
			root@node0:/usr/src/dvs/dvk-proxies/test# cat /proc/dvs/DC0/procs 		
			QUEDA COLGADO
==============================================================================================================
20200911:		Probando loop_copy_server en NODE0

		Sep 11 12:11:15 node0 kernel: [  258.078789] DEBUG 668:SYSC_ipc:166: ipc_unbind: call=2560 first=0 second=2 third=-1 fifth=0 
		Sep 11 12:11:15 node0 kernel: [  258.078790] DEBUG 668:ipc_unbind:215: dcid=0 proc_ep=2 timeout_ms=-1
		Sep 11 12:11:15 node0 kernel: [  258.078791] DEBUG 668:new_unbind:1831: dcid=0 proc_ep=2
		Sep 11 12:11:15 node0 kernel: [  258.078793] DEBUG 668:check_caller:608: caller_pid=668 caller_tgid=668
		
		Sep 11 12:11:15 node0 kernel: [  258.078794] DEBUG 668:check_caller:644: WLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078795] DEBUG 668:check_caller:674: WUNLOCK_PROC ep=2 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078796] DEBUG 668:check_caller:677: dcid=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078798] DEBUG 668:check_caller:681: RLOCK_DC dc=0 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078799] DEBUG 668:check_caller:685: RUNLOCK_DC dc=0 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078800] DEBUG 668:check_caller:691: caller_pid=668 
		
		Sep 11 12:11:15 node0 kernel: [  258.078801] DEBUG 668:new_unbind:1847: RLOCK_DC dc=0 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078802] DEBUG 668:new_unbind:1859: RUNLOCK_DC dc=0 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078804] DEBUG 668:new_unbind:1861: RLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078805] DEBUG 668:new_unbind:1883: RUNLOCK_PROC ep=2 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078806] DEBUG 668:new_unbind:1886: RLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078807] DEBUG 668:new_unbind:1889: RUNLOCK_PROC ep=2 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078809] DEBUG 668:new_unbind:1901: WLOCK_TASK pid=668 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078810] DEBUG 668:new_unbind:1914: WLOCK_DC dc=0 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078811] DEBUG 668:new_unbind:1918: WLOCK_PROC ep=2 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078814] DEBUG 668:do_unbind:738: nr=2 endp=2 dcid=0 flags=0 misc=20 lpid=668 vpid=3 nodeid=0 name=loop_copy_serve 
		Sep 11 12:11:15 node0 kernel: [  258.078816] DEBUG 668:do_unbind:767: Caller nr=2 endp=2 dcid=0 flags=0 misc=20 lpid=668 vpid=3 nodeid=0 name=loop_copy_serve 
		Sep 11 12:11:15 node0 kernel: [  258.078817] DEBUG 668:do_unbind:806: wakeup with error those processes trying to send a message to the proc
		Sep 11 12:11:15 node0 kernel: [  258.078818] DEBUG 668:do_unbind:835: delete notify messages bits sent by the proc
		Sep 11 12:11:15 node0 kernel: [  258.078822] DEBUG 668:do_unbind:842: Skip, caller process
		Sep 11 12:11:15 node0 kernel: [  258.078835] DEBUG 668:do_unbind:897: nr=2 endp=2 dcid=0 flags=0 misc=20 lpid=668 vpid=3 nodeid=0 name=loop_copy_serve 
		Sep 11 12:11:15 node0 kernel: [  258.078836] DEBUG 668:do_unbind:953: wakeup with error those processes waiting this process MIGRATION
		Sep 11 12:11:15 node0 kernel: [  258.078837] DEBUG 668:do_unbind:1003: wakeup those processes waiting this process UNBINDING
		Sep 11 12:11:15 node0 kernel: [  258.078839] DEBUG 668:do_unbind:1037: nr=2 endp=2 dcid=0 flags=0 misc=20 lpid=668 vpid=3 nodeid=0 name=loop_copy_serve 
		Sep 11 12:11:15 node0 kernel: [  258.078840] DEBUG 668:init_proc_desc:16: p_name=loop_copy_serve dcid=0
		Sep 11 12:11:15 node0 kernel: [  258.078841] DEBUG 668:init_proc_desc:27: Clearing Privileges
		Sep 11 12:11:15 node0 kernel: [  258.078842] DEBUG 668:init_proc_desc:35: Setting Default DVK calls privileges
		Sep 11 12:11:15 node0 kernel: [  258.078843] DEBUG 668:init_proc_desc:41: Clearing Process fields
		Sep 11 12:11:15 node0 kernel: [  258.078844] DEBUG 668:do_unbind:1039: initialized 
		Sep 11 12:11:15 node0 kernel: [  258.078845] DEBUG 668:do_unbind:1040: DC_DECREF counter=3
		
		Sep 11 12:11:15 node0 kernel: [  258.078846] DEBUG 668:new_unbind:1950: WUNLOCK_TASK pid=668 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078848] DEBUG 668:new_unbind:1978: WUNLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078849] DEBUG 668:new_unbind:1981: WUNLOCK_DC dc=0 count=0
		===============================================================================================
		Sep 11 12:11:15 node0 kernel: [  258.078850] DEBUG 668:SYSC_ipc:169: ipc_unbind: call=2560 rcode=0
		Sep 11 12:11:15 node0 kernel: [  258.078851] ERROR: 668:SYSC_ipc:170: rcode=0
		Sep 11 12:11:15 node0 kernel: [  258.078862] DEBUG 668:SYSC_ipc:166: ipc_unbind: call=2560 first=0 second=3 third=-1 fifth=0 
		Sep 11 12:11:15 node0 kernel: [  258.078863] DEBUG 668:ipc_unbind:215: dcid=0 proc_ep=3 timeout_ms=-1
		Sep 11 12:11:15 node0 kernel: [  258.078865] DEBUG 668:new_unbind:1831: dcid=0 proc_ep=3
		Sep 11 12:11:15 node0 kernel: [  258.078866] DEBUG 668:check_caller:608: caller_pid=668 caller_tgid=668
		
		Sep 11 12:11:15 node0 kernel: [  258.078867] DEBUG 668:check_caller:644: WLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078869] DEBUG 668:check_caller:668: nr=2 endp=2 dcid=0 flags=1 misc=0 lpid=-1 vpid=-1 nodeid=-1 name=$noname 
		Sep 11 12:11:15 node0 kernel: [  258.078871] DEBUG 668:check_caller:669: WUNLOCK_PROC ep=2 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078872] ERROR: 668:check_caller:669: rcode=-106 <<<<<<<<<<<<<<<<<<< EDVSNOTREADY ???
		
		Sep 11 12:11:15 node0 kernel: [  258.078873] DEBUG 668:new_unbind:1847: RLOCK_DC dc=0 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078874] DEBUG 668:new_unbind:1859: RUNLOCK_DC dc=0 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078876] DEBUG 668:new_unbind:1861: RLOCK_PROC ep=3 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078877] DEBUG 668:new_unbind:1883: RUNLOCK_PROC ep=3 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.078878] DEBUG 668:new_unbind:1914: WLOCK_DC dc=0 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078879] DEBUG 668:new_unbind:1918: WLOCK_PROC ep=3 count=0
		Sep 11 12:11:15 node0 kernel: [  258.078881] DEBUG 668:do_unbind:738: nr=3 endp=3 dcid=0 flags=1000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=client3 
		Sep 11 12:11:15 node0 kernel: [  258.078883] DEBUG 668:do_unbind:767: Caller nr=2 endp=2 dcid=0 flags=1 misc=0 lpid=-1 vpid=-1 nodeid=-1 name=$noname 
		Sep 11 12:11:15 node0 kernel: [  258.078885] DEBUG 668:do_unbind:806: wakeup with error those processes trying to send a message to the proc
		Sep 11 12:11:15 node0 kernel: [  258.078885] DEBUG 668:do_unbind:835: delete notify messages bits sent by the proc
		Sep 11 12:11:15 node0 kernel: [  258.078889] DEBUG 668:do_unbind:842: Skip, caller process
		Sep 11 12:11:15 node0 kernel: [  258.078890] DEBUG 668:do_unbind:848: Skip, self process
		Sep 11 12:11:15 node0 kernel: [  258.078902] DEBUG 668:do_unbind:897: nr=3 endp=3 dcid=0 flags=1000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=client3 
		Sep 11 12:11:15 node0 kernel: [  258.078903] DEBUG 668:do_unbind:953: wakeup with error those processes waiting this process MIGRATION
		Sep 11 12:11:15 node0 kernel: [  258.078904] DEBUG 668:do_unbind:1003: wakeup those processes waiting this process UNBINDING
		Sep 11 12:11:15 node0 kernel: [  258.078906] DEBUG 668:do_unbind:1037: nr=3 endp=3 dcid=0 flags=1000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=client3 
		Sep 11 12:11:15 node0 kernel: [  258.078908] DEBUG 668:init_proc_desc:16: p_name=client3 dcid=0
		Sep 11 12:11:15 node0 kernel: [  258.078909] DEBUG 668:init_proc_desc:27: Clearing Privileges
		Sep 11 12:11:15 node0 kernel: [  258.078910] DEBUG 668:init_proc_desc:35: Setting Default DVK calls privileges
		Sep 11 12:11:15 node0 kernel: [  258.078910] DEBUG 668:init_proc_desc:41: Clearing Process fields
		Sep 11 12:11:15 node0 kernel: [  258.078912] DEBUG 668:do_unbind:1039: initialized 
		Sep 11 12:11:15 node0 kernel: [  258.078913] DEBUG 668:do_unbind:1040: DC_DECREF counter=2
		Sep 11 12:11:15 node0 kernel: [  258.078914] DEBUG 668:new_unbind:1978: WUNLOCK_PROC ep=3 count=0
		
		Sep 11 12:11:15 node0 kernel: [  258.079011] BUG: unable to handle kernel NULL pointer dereference at 00000bd8 <<<< 
		Sep 11 12:11:15 node0 kernel: [  258.079822] IP: [<cb23c5fe>] new_unbind+0x13fe/0x17a0
		Sep 11 12:11:15 node0 kernel: [  258.080064] *pdpt = 000000002b5df001 *pde = 0000000000000000 
		Sep 11 12:11:15 node0 kernel: [  258.080177] 
		Sep 11 12:11:15 node0 kernel: [  258.080208] Oops: 0000 [#1] SMP
		Sep 11 12:11:15 node0 kernel: [  258.080253] Modules linked in: iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev serio_raw pcspkr drm vmw_balloon vmw_vmci sg shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sd_mod sr_mod cdrom ata_generic psmouse uhci_hcd ehci_pci xhci_pci ehci_hcd xhci_hcd pcnet32 usbcore mii mptspi scsi_transport_spi mptscsih mptbase ata_piix libata scsi_mod i2c_piix4
		Sep 11 12:11:15 node0 kernel: [  258.101563] CPU: 2 PID: 668 Comm: loop_copy_serve Not tainted 4.9.88-DVK #152
		Sep 11 12:11:15 node0 kernel: [  258.102666] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
		Sep 11 12:11:15 node0 kernel: [  258.113484] task: f5f53500 task.stack: e445e000
		Sep 11 12:11:15 node0 kernel: [  258.114753] EIP: 0060:[<cb23c5fe>] EFLAGS: 00010202 CPU: 2
		Sep 11 12:11:15 node0 kernel: [  258.117260] EIP is at new_unbind+0x13fe/0x17a0
		Sep 11 12:11:15 node0 kernel: [  258.118696] EAX: 00000a00 EBX: ed1e4a00 ECX: f69d6840 EDX: f5f53500
		Sep 11 12:11:15 node0 kernel: [  258.120445] ESI: 00000000 EDI: 00000000 EBP: e445ff0c ESP: e445fe7c
		Sep 11 12:11:15 node0 kernel: [  258.122228]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
		Sep 11 12:11:15 node0 kernel: [  258.123721] CR0: 80050033 CR2: 00000bd8 CR3: 2b524c40 CR4: 000006f0
		Sep 11 12:11:15 node0 kernel: [  258.125343] Stack:
		Sep 11 12:11:15 node0 kernel: [  258.126915]  cb789894 0000029c cb5ea038 000007ba 00000003 00000000 cb0acf01 cb98a602
		Sep 11 12:11:15 node0 kernel: [  258.128649]  00001b7d cb98a680 00000038 00000002 e445fee4 cb0b2b2e 00000000 00000040
		Sep 11 12:11:15 node0 kernel: [  258.130661]  00000001 01000000 00000001 ffffff96 ffffffff cb9c6220 ed1e4bd8 00000003
		Sep 11 12:11:15 node0 kernel: [  258.132890] Call Trace:
		Sep 11 12:11:15 node0 kernel: [  258.135242]  [<cb0acf01>] ? mutex_optimistic_spin+0x161/0x190
		Sep 11 12:11:15 node0 kernel: [  258.137107]  [<cb0b2b2e>] ? vprintk_emit+0x2ee/0x4f0
		Sep 11 12:11:15 node0 kernel: [  258.139162]  [<cb0b2f17>] ? vprintk_default+0x37/0x40
		Sep 11 12:11:15 node0 kernel: [  258.141080]  [<cb268e87>] ? ipc_unbind+0x27/0x80
		Sep 11 12:11:15 node0 kernel: [  258.143108]  [<cb268e60>] ? ipc_bind_X+0x90/0x90
		Sep 11 12:11:15 node0 kernel: [  258.145075]  [<cb22edf9>] ? SyS_ipc+0x69/0x410
		Sep 11 12:11:15 node0 kernel: [  258.147145]  [<cb0036a8>] ? do_fast_syscall_32+0x98/0x160
		Sep 11 12:11:15 node0 kernel: [  258.149162]  [<cb5cba82>] ? sysenter_past_esp+0x47/0x75
		Sep 11 12:11:15 node0 kernel: [  258.151691] Code: 5e cb 89 54 24 10 8b 80 5c 03 00 00 c7 04 24 94 98 78 cb 89 44 24 04 e8 4d a3 f0 ff e9 d9 f2 ff ff 8b 45 d0 64 8b 15 48 42 96 cb <8b> 88 d8 01 00 00 89 4c 24 14 8b 40 04 c7 44 24 0c bc 07 00 00
		Sep 11 12:11:15 node0 kernel: [  258.158860] EIP: [<cb23c5fe>] 
		Sep 11 12:11:15 node0 kernel: [  258.158904] new_unbind+0x13fe/0x17a0
		Sep 11 12:11:15 node0 kernel: [  258.161279]  SS:ESP 0068:e445fe7c
		Sep 11 12:11:15 node0 kernel: [  258.163634] CR2: 0000000000000bd8
		Sep 11 12:11:15 node0 kernel: [  258.166093] ---[ end trace 7fcabb079824b626 ]---
		Sep 11 12:11:15 node0 kernel: [  258.168426] do_exit: local_nodeid:0
		Sep 11 12:11:15 node0 kernel: [  258.170902] DEBUG 668:new_exit_unbind:267: code=-881382670
		Sep 11 12:11:15 node0 kernel: [  258.174621] do_exit: local_nodeid:0
		Sep 11 12:11:15 node0 kernel: [  258.176524] DEBUG 668:new_exit_unbind:271: WLOCK_TASK pid=668 count=0
		Sep 11 12:11:15 node0 kernel: [  258.176526] DEBUG 668:new_exit_unbind:277: RLOCK_PROC ep=2 count=0
		Sep 11 12:11:15 node0 kernel: [  258.176529] DEBUG 668:new_exit_unbind:278: nr=2 endp=2 dcid=0 flags=1 misc=0 lpid=-1 vpid=-1 nodeid=-1 name=$noname 
		Sep 11 12:11:15 node0 kernel: [  258.176531] DEBUG 668:new_exit_unbind:311:  Exiting endpoint=2 lpid=-1
		Sep 11 12:11:15 node0 kernel: [  258.176533] DEBUG 668:new_exit_unbind:313: RUNLOCK_PROC ep=2 count=0	
			
			
	PROBLEMA. El caller puede o no estar bindeado. controlar eso con 
			if( (caller_bound) == TRUE && ( caller_nr != proc_nr ) ) 
			
	
		if( caller_bound == TRUE){
		if( caller_ptr->p_usr.p_dcid != proc_ptr->p_usr.p_dcid){
			ERROR_RUNLOCK_PROC(proc_ptr,EDVSBADDCID);
		}
	}
	
		
	SOLUCIONADO: Se probaron transferencias remotas y el test_unbind y funcionaron OK
	
==============================================================================================================
20200913:
			Tratar de hacer funcionar la interface DVKIOCTL cuando esta cargada y cuando no esta cargada la 
			interface DVKIPC.

1 ) CONFIG_DVKIOCTL CARGADA - CONFIG_DVKIPC NO CARGADA
		# Linux/x86 4.9.88-DVK Kernel Configuration
		# CONFIG_DVKIPC is not set
		CONFIG_DVKIOCTL=y
	
	ATENCION, ALGO NECESARIO PARA REEMPLAZAR ETIQUETAS DEL MODULO QUE HABIA DESCONFIGURADO.
		reljmp: failed to lookup jump_label_lock 
		MODIFIQUE EL .config
			root@node0:/usr/src/linux# grep JUMP .config
			CONFIG_JUMP_LABEL=y
			CONFIG_HAVE_ARCH_JUMP_LABEL=y
			CONFIG_USB_STORAGE_JUMPSHOT=y

			SE SOLUCIONO AHORA FALTA PROBAR APLICIONES LOCALES Y REMOTAS.
			
==============================================================================================================
20200914:
			SE PROBARON APLICIONES LOCALES Y REMOTAS con interface CONFIG_DVKIOCTL.

		Cuando en NODE0 se ejecuta loop_copy_server y se detiene con CTRL-C y luego se hace
		cat /proc/dvs/DC0/procs SE CUELGA.
		
		El SERVER YA HIZO EL SELF BIND, AHORA ESTA TRANTADO DE HACER EL BIND DEL CLIENTE REMOTO 
		Sep 15 01:18:35 node0 kernel: [  844.494185] DEBUG 1337:dvk_ioctl:349: cmd=4004E309 arg=BFCBE628
		Sep 15 01:18:35 node0 kernel: [  844.494186] DEBUG 1337:dvk_ioctl:369: DVK_CALL=9 (io_bind) 
		Sep 15 01:18:35 node0 kernel: [  844.494187] DEBUG 1337:io_bind:97: 
		Sep 15 01:18:35 node0 kernel: [  844.494190] DEBUG 1337:new_bind:1531: oper=2 dcid=0 param_pid=-1077156220 endpoint=1 nodeid=1
		Sep 15 01:18:35 node0 kernel: [  844.494191] DEBUG 1337:new_bind:1554: dc_ptr=f89fdd00
		Sep 15 01:18:35 node0 kernel: [  844.494193] DEBUG 1337:new_bind:1556: RLOCK_DC dc=0 count=0    		<<<<<<<<<<<   
		Sep 15 01:18:35 node0 kernel: [  844.494194] DEBUG 1337:new_bind:1569: proc_ptr=f02e4600
		Sep 15 01:18:35 node0 kernel: [  844.494195] DEBUG 1337:new_bind:1570: WLOCK_PROC ep=1 count=0
		Sep 15 01:18:35 node0 kernel: [  844.494197] DEBUG 1337:lock_sr_proxies:745: px_nr=1
		Sep 15 01:18:35 node0 kernel: [  844.494198] DEBUG 1337:lock_sr_proxies:753: WLOCK_PROXY pxid=1 count=0 <<<<<<<<<<<

		APARENTEMENTE ESTA LOCKEADO EL DC 
		Sep 15 01:20:33 node0 kernel: [  962.829623] DEBUG 1369:dc_procs_read:527: last_proc=0 count=131072 ppos=0
		Sep 15 01:20:33 node0 kernel: [  962.829627] DEBUG 1369:dc_procs_read:542: dcid=0
		
	
	AL ARRANCAR TODO POR PRIMERA VEZ 
		root@node0:/usr/src/dvs/dvk-proxies/test# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     A       1        0        0 -------------------------------X node1
	
	CON NODE1 CAIDO 
		root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     A       1        0        1 -------------------------------X node1 
		root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/proxies/info 
		Proxies Flags Sender Receiver --Proxies_Name- 10987654321098765432109876543210 
		  1     5    646      647           node1 ------------------------------X-
	
	 EN NODE1 AL LEVANTAR CON NODE0 LEVANTADO 
		 root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6       0        0        0 -------------------------------X node0   PORQUE "6" = 4 + 2 = NODE_SCONNECTED + NODE_ATTACHED ????         
		 1     6      -1        0        0 -------------------------------X node1 
	 EN NODE0 AL LEVANTAR NODE1 
		root@node0:/usr/src/dvs/dvk-proxies/test# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     A       1        0        1 -------------------------------X node1  	PORQUE "A" = 8 + 2 = NODE_BIT_RCONNECTED + NODE_ATTACHED???? 
	 
	 EN AMBOS CASOS DEBERIA SER:  NODE_BIT_RCONNECTED + NODE_SCONNECTED + NODE_ATTACHED = 8 + 4 + 2 = 14 = 0x0E 

      ......................................................................................................
	  AL ARRANCAR NODE0 SIN NODE1
		root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     2       1        0        0 -------------------------------X node1 
	  AL CONECTAR NODE1 
		 root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     E       1        0        0 -------------------------------X node1 
	  DESPUES DE REALIZAR CORRECTAMENTE LA TRANSFERENCIA DE MENSAJES, SE REBOOTEA NODE1 
	PERO EN NODE0 TODAVIA EL TCP NO INFORMO LA CAIDA POR ESO QUE SE MANTIENE EL ESTADO DEL node1 EN "E"
		tcp        0      0 192.168.1.100:3001      192.168.1.101:34842     ESTABLISHED
		tcp        0    264 192.168.1.100:41020     192.168.1.101:3000      ESTABLISHED
		SE SIGUE MANTENIENDO UNA SESION ABIERTA A PESAR DE HABER REBOOTEADO !!!!!!!!!!!!!!!!!!!!!!!!!11
	tcp        0      0 192.168.1.100:3001      192.168.1.101:34842     ESTABLISHED
		root@node0:/usr/src/dvs/dvk-proxies/test# cat /proc/dvs/nodes 
		ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		 0     6      -1        0        0 -------------------------------X node0           
		 1     A       1      402      182 -------------------------------X node1 << conectado el RPROXY
		 
	

	LUEGO, AL REBOOTEAR, SE ARRANCO TODO DE VUELTA Y FUNCIONO OK CON 2 THREADS POR SERVER Y CLIENT 
	
==============================================================================================================
20200915; 
			El problema parece suceder cuando se rearranca el NODE1.
			Es decir: se hacen transferencia remota entre NODE0 y NODE1 
					Luego se rearranca NODE1
					Se intenta transferencia NODE0 y NODE1
					Pero NODE0 queda colgado en el rmtbind()
			Luego se hace CTRL-C y al hacer cat /proc/dvs/DC0/procs se CUELGA
			
			EL PROBLEMA ES QUE LOS PROXIES SERVERS QUEDAN CONECTADOS A NIVEL TCP A PESAR DE REARRANCAR EL OTRO NODO
			PARA ELLO INCLUI EN EL SCRIPT tests.sh
				# tcp_keepalive_time: the interval between the last data packet sent (simple ACKs are not considered data) 
				#           and the first keepalive probe; after the connection is marked to need keepalive, this counter is not used any further
				# tcp_keepalive_intvl:  the interval between subsequential keepalive probes, regardless of what the connection has exchanged in the meantime
				# tcp_keepalive_probes: the number of unacknowledged probes to send before considering the connection dead and notifying the application layer
				echo 60 > /proc/sys/net/ipv4/tcp_keepalive_time
				echo 30 > /proc/sys/net/ipv4/tcp_keepalive_intvl
				echo 3  > /proc/sys/net/ipv4/tcp_keepalive_probes
			
			root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
			ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
			 0     6      -1        0        0 -------------------------------X node0           
			 1     E       1        0        0 -------------------------------X node1 		

			root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
			ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
			 0     E       0        0        0 -------------------------------X node0           
			 1     6      -1        0        0 -------------------------------X node1 
		
			tcp        0      0 192.168.1.100:3001      192.168.1.101:48878     ESTABLISHED
			tcp        0    232 192.168.1.100:22        192.168.1.11:52952      ESTABLISHED

			Cuando vuelve a levantar el NODE1 sigue manteniendo la sesion anterior!!!!
			tcp        0      0 192.168.1.100:3001      192.168.1.101:48878     ESTABLISHED <<<<<<<<<<<<< ANTERIOR
			tcp        0      0 192.168.1.100:3001      192.168.1.101:45362     ESTABLISHED <<<<<<<<<<<<< NUEVA 
			tcp        0    232 192.168.1.100:22        192.168.1.11:52952      ESTABLISHED

			root@node0:/usr/src/dvs/dvk-proxies/test# netstat -nat
			Active Internet connections (servers and established)
			Proto Recv-Q Send-Q Local Address           Foreign Address         State      
			tcp        0      0 0.0.0.0:4001            0.0.0.0:*               LISTEN     
			tcp        0      0 0.0.0.0:16385           0.0.0.0:*               LISTEN     
			tcp        0      0 0.0.0.0:4803            0.0.0.0:*               LISTEN     
			tcp        0      0 0.0.0.0:5001            0.0.0.0:*               LISTEN     
			tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     
			tcp        1      0 0.0.0.0:3001            0.0.0.0:*               LISTEN     
			tcp        0      0 192.168.1.100:3001      192.168.1.101:48878     ESTABLISHED
			tcp        0      0 192.168.1.100:22        192.168.1.11:53074      ESTABLISHED
			tcp      660      0 192.168.1.100:3001      192.168.1.101:45362     ESTABLISHED
			tcp        0    232 192.168.1.100:22        192.168.1.11:52952      ESTABLISHED
			tcp6       0      0 :::80                   :::*                    LISTEN     
			tcp6       0      0 :::22                   :::*                    LISTEN     
			root@node0:/usr/src/dvs/dvk-proxies/test# cat /proc/dvs/nodes 
			ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
			 0     6      -1        0        0 -------------------------------X node0           
			 1     A       1       12        9 -------------------------------X node1
 
			EN NODE1
			root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/nodes 
			ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
			 0     6       0        0        0 -------------------------------X node0           
			 1     6      -1        0        0 -------------------------------X node1 
		
	TODO: 	VER LA FORMA DE QUE SI UN CLIENT SE DESCONECTA: POR EJEMPLO EL SPROXY, ENTONCES MATA LA SESION DEL RPROXY.
	
		
	ATENCION:  ESTA DANDO ESTO
ERROR: 931:dvk_getprocinfo:1181: rcode=-302
	
Sep 15 16:42:21 node0 kernel: [  520.327473] DEBUG 654:dvk_ioctl:369: DVK_CALL=15 (io_getprocinfo) 
Sep 15 16:42:21 node0 kernel: [  520.327474] DEBUG 654:io_getprocinfo:164: 
Sep 15 16:42:21 node0 kernel: [  520.327476] DEBUG 654:new_getprocinfo:2107: dcid=0 p_nr=0
Sep 15 16:42:21 node0 kernel: [  520.327479] DEBUG 654:check_caller:608: caller_pid=654 caller_tgid=654
Sep 15 16:42:21 node0 kernel: [  520.327481] DEBUG 654:check_caller:644: WLOCK_PROC ep=27342 count=0
Sep 15 16:42:21 node0 kernel: [  520.327482] DEBUG 654:check_caller:674: WUNLOCK_PROC ep=27342 count=0
Sep 15 16:42:21 node0 kernel: [  520.327483] DEBUG 654:check_caller:691: caller_pid=654 
Sep 15 16:42:21 node0 kernel: [  520.327484] ERROR: 654:new_getprocinfo:2128: rcode=-302

	
	ATENCION, NO HAY FORMA DE QUE SE CORTE LA COMUNICACION TCP AUN REARRANCANDO EL NODE1 
	
		tcp        0      0 192.168.1.100:3001      192.168.1.101:36114     ESTABLISHED
		tcp        0    264 192.168.1.100:49998     192.168.1.101:3000      ESTABLISHED

		root@node0:/usr/src/dvs/dvk-proxies/test# netstat -nat
		Active Internet connections (servers and established)
		Proto Recv-Q Send-Q Local Address           Foreign Address         State      
		tcp        0      0 192.168.1.100:3001      192.168.1.101:36114     ESTABLISHED


==============================================================================================================
20200919/20:
		Se creo dvs-apps/dvs_run para disparar procesos locales, replicas y backups en el nodo local	
			para un determinado dcid y endpoint.
		Para probar la migracion de procesos se creo migr_server.c y migr.client.c 
		
		Esta fallando el wait4bind
		Sep 20 13:08:14 node0 kernel: [  664.489880] DEBUG 711:new_wait4bind:2486: Self process bind waiting
		Sep 20 13:08:14 node0 kernel: [  664.489881] DEBUG 711:new_wait4bind:2506: ret=-512
		Sep 20 13:08:14 node0 kernel: [  664.489882] DEBUG 711:new_wait4bind:2508: pending: sig[0]:0x00000100, sig[1]:0x00000000
		Sep 20 13:08:14 node0 kernel: [  664.489883] DEBUG 711:new_wait4bind:2511: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
		
		

			Sep 20 13:16:11 node0 kernel: [  136.406840] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:11 node0 kernel: [  136.406844] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:11 node0 kernel: [  136.406846] DEBUG 619:new_wait4bind:2486: Self process bind waiting
		UN SEGUNDO DE DIFERENCIA
			Sep 20 13:16:12 node0 kernel: [  137.431057] DEBUG 619:new_wait4bind:2506: ret=0
			Sep 20 13:16:12 node0 kernel: [  137.431063] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:12 node0 kernel: [  137.431066] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:12 node0 kernel: [  137.431069] DEBUG 619:new_wait4bind:2486: Self process bind waiting
		UN SEGUNDO DE DIFERENCIA
			Sep 20 13:16:13 node0 kernel: [  138.455263] DEBUG 619:new_wait4bind:2506: ret=0
			Sep 20 13:16:13 node0 kernel: [  138.455269] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:13 node0 kernel: [  138.455272] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
			Sep 20 13:16:13 node0 kernel: [  138.455275] DEBUG 619:new_wait4bind:2486: Self process bind waiting
		UN SEGUNDO DE DIFERENCIA
			Sep 20 13:16:14 node0 kernel: [  139.407699] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000100, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407700] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407701] DEBUG 619:new_wait4bind:2486: Self process bind waiting
			Sep 20 13:16:14 node0 kernel: [  139.407702] DEBUG 619:new_wait4bind:2506: ret=-512
		ACA SE PUDRE TODO 
			Sep 20 13:16:14 node0 kernel: [  139.407703] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000100, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407704] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407705] DEBUG 619:new_wait4bind:2486: Self process bind waiting
			Sep 20 13:16:14 node0 kernel: [  139.407706] DEBUG 619:new_wait4bind:2506: ret=-512
			Sep 20 13:16:14 node0 kernel: [  139.407708] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000100, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407709] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407710] DEBUG 619:new_wait4bind:2486: Self process bind waiting
			Sep 20 13:16:14 node0 kernel: [  139.407711] DEBUG 619:new_wait4bind:2506: ret=-512
			Sep 20 13:16:14 node0 kernel: [  139.407712] DEBUG 619:new_wait4bind:2508: pending: sig[0]:0x00000100, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407713] DEBUG 619:new_wait4bind:2511: shared_pending sig[0]:0x00000002, sig[1]:0x00000000
			Sep 20 13:16:14 node0 kernel: [  139.407714] DEBUG 619:new_wait4bind:2486: Self process bind waiting
			Sep 20 13:16:14 node0 kernel: [  139.407715] DEBUG 619:new_wait4bind:2506: ret=-512

	SOLUCIONADO: Se arreglo el wait4bind 

	SE HIZO EL COMANDO dvs_migrate que permite start/rollback/commit de procesos 
	Se probo en un unico nodo el start y rollback y funcionaron bien.
	
	TODO:  EL wait4bind ni el getep deben devolver errores >  EDVSERRCODE 
		DEBUG 31:dvk_wait4bindep_X:615: cmd=0 endpoint=35534 timeout=30000
		DEBUG 31:dvk_wait4bindep_X:628: ioctl ret=-1 errno=106
		DEBUG 31:dvk_getep:233: pid=31
		DEBUG 31:dvk_getep:243: ioctl ret=-1 errno=106
		DEBUG 31:dvk_getep:248: ioctl ret=-1
		DEBUG 31:dvk_getep:251: ret=-1
	
	ATENCION: UN PROCESO DE TIPO LOCAL/BACKUP/REPLICA DEBE RETORNAR CORRECTAMENTE DEL WAIT4BIND
		Sep 20 17:14:28 node0 kernel: [  265.529210] DEBUG 2050:dvk_ioctl:349: cmd=4004E31D arg=BFA0A830
		Sep 20 17:14:28 node0 kernel: [  265.529212] DEBUG 2050:dvk_ioctl:369: DVK_CALL=29 (io_wait4bind) 
		Sep 20 17:14:28 node0 kernel: [  265.529213] DEBUG 2050:io_wait4bind:309: 
		Sep 20 17:14:28 node0 kernel: [  265.529216] DEBUG 2050:new_wait4bind:2374: oper=0 other_ep=35534 timeout_ms=30000
		Sep 20 17:14:28 node0 kernel: [  265.529219] DEBUG 2050:check_caller:604: caller_pid=2050 caller_tgid=2050
		Sep 20 17:14:28 node0 kernel: [  265.529221] DEBUG 2050:check_caller:640: WLOCK_PROC ep=11 count=0
		Sep 20 17:14:28 node0 kernel: [  265.529224] DEBUG 2050:check_caller:664: nr=11 endp=11 dcid=0 flags=1000 (BIT_REMOTE) 
																					misc=30 (0011 0000) MIS_BIT_RMTBACKUP | MIS_BIT_GRPLEADER
																					lpid=2050 vpid=7 nodeid=1 name=dvs_run 
		Sep 20 17:14:28 node0 kernel: [  265.529225] DEBUG 2050:check_caller:665: WUNLOCK_PROC ep=11 count=0
		Sep 20 17:14:28 node0 kernel: [  265.529226] ERROR: 2050:check_caller:665: rcode=-320
		Sep 20 17:14:28 node0 kernel: [  265.529228] DEBUG 2050:new_wait4bind:2388: caller_pid=0 ret=-320
		Sep 20 17:14:28 node0 kernel: [  265.529229] ERROR: 2050:new_wait4bind:2399: rcode=-320
		Sep 20 17:14:28 node0 kernel: [  265.529231] ERROR: 2050:dvk_ioctl:373: rcode=-320
	
	
	TODO: 	AHORA TODO FUNCIONA BIEN PERO TENGO EL SIGUIENTE PROBLEMA
			Cuando un proceso BACKUP hace wait4bind y un proceso extern (dvs_run) lo bindea, automaticamente sale
			del wait4bind y se cree que puede procesar.
			Si es BACKUP no deberia seguir procesando.
			
			En el bind se controla si el proceso esta esperando
				/* Wake up the waiting process on dvk_wait4bind() */
			if( oper == LCL_BIND || oper == REPLICA_BIND || oper == BKUP_BIND){
				if( current != task_ptr ){
					/* Wakes up it as if has in wait4bind state  */
					if( waitqueue_active(&task_ptr->task_wqh))
						wake_up_interruptible(&task_ptr->task_wqh); 
				}
			}
			no deberia levantarlo si BKUP_BIND
			
			PERO, QUIEN CAMBIA EL ESTADO DE BKUP_BIND a LOCAL_BIND ??
			SUPONGO QUE LA MIGRACION. 
			
			root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
			DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
			 0  10    10   703/4      0    8   20 31438 27342 27342 27342 dvs_run        
			 0  11    11   756/7      1 1800   30 27342 27342 27342 27342 dvs_run        <<<<< BLOQUEADO EN WAIT4BIND
			 
			root@node0:/usr/src/dvs/dvs-apps/dvs_run#  nsenter -p -t$DC0 ./dvs_migrate -c 0 11 0 7  <<< ORDENA LA MIGRACION 
			DEBUG 13:dvk_open:108: Open dvk device file /dev/dvk
			 dvs_migrate.c:get_dvs_params:27:
			DEBUG 13:dvk_getdvsinfo:258: 
			DEBUG 13:dvk_getdvsinfo:267: ioctl ret=0 errno=0
			DEBUG 13:dvk_getdvsinfo:272: ioctl ret=0
			 dvs_migrate.c:get_dvs_params:29:local_nodeid=0
			 dvs_migrate.c:get_dvs_params:32:d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			 dvs_migrate.c:main:108:new_nodeid=0
			 dvs_migrate.c:main:114:proc_pid=7
			 dvs_migrate.c:main:123:getopt migr_type=1
			 dvs_migrate.c:main:129:dcid=0
			 dvs_migrate.c:get_dc_params:42:dcid=0
			DEBUG 13:dvk_getdcinfo:359: dcid=0
			DEBUG 13:dvk_getdcinfo:371: ioctl ret=0 errno=0
			DEBUG 13:dvk_getdcinfo:376: ioctl ret=0
			 dvs_migrate.c:get_dc_params:50:dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
			 dvs_migrate.c:get_dc_params:51:flags=0 dc_nodes=3 dc_pid=670 dc_name=DC0
			 dvs_migrate.c:main:133:proc_ep=11
			 dvs_migrate.c:main:148:argc=6 
			DEBUG 13:dvk_migrate_X:1283: cmd=1 pid=7 dcid=0 endpoint=11 nodeid=0
			DEBUG 13:dvk_migrate_X:1298: ioctl ret=0 errno=0
			DEBUG 13:dvk_migrate_X:1307: ioctl ret=0
			root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
			DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
			 0  10    10   703/4      0    8   20 31438 27342 27342 27342 dvs_run        
			 0  11    11   756/7      0    0   20 27342 27342 27342 27342 dvs_run  <<<<<< LISTO pero no lo levanta 


==============================================================================================================
20200921:
		AL EJECUTAR EQUIVOCADAMENTE (DEBIO EJECUTARSE A TRAVES DE dvs_run)
				nsenter -p -t$DC0 ./migr_server
 
			Sep 21 15:57:00 node0 kernel: [ 1051.492341] DEBUG 1383:dvk_ioctl:349: cmd=4004E31D arg=BFAF5D80
			Sep 21 15:57:00 node0 kernel: [ 1051.492343] DEBUG 1383:dvk_ioctl:369: DVK_CALL=29 (io_wait4bind) 
			Sep 21 15:57:00 node0 kernel: [ 1051.492344] DEBUG 1383:io_wait4bind:309: 
			Sep 21 15:57:00 node0 kernel: [ 1051.492467] DEBUG 1383:new_wait4bind:2372: oper=0 other_ep=35534 timeout_ms=1000
			Sep 21 15:57:00 node0 kernel: [ 1051.492470] DEBUG 1383:check_caller:604: caller_pid=1383 caller_tgid=1383
			Sep 21 15:57:00 node0 kernel: [ 1051.492472] ERROR: 1383:check_caller:638: rcode=-310
			Sep 21 15:57:00 node0 kernel: [ 1051.492474] DEBUG 1383:new_wait4bind:2386: caller_pid=0 ret=-310
			Sep 21 15:57:00 node0 kernel: [ 1051.492476] DEBUG 1383:new_wait4bind:2462: WLOCK_TASK pid=1383 count=0
			Sep 21 15:57:00 node0 kernel: [ 1051.492477] DEBUG 1383:new_wait4bind:2477: WUNLOCK_TASK pid=1383 count=0
			Sep 21 15:57:00 node0 kernel: [ 1051.492479] DEBUG 1383:new_wait4bind:2492: Self process bind waiting
			Sep 21 15:57:01 node0 kernel: [ 1052.523146] BUG: unable to handle kernel paging request at d41cf565
			Sep 21 15:57:01 node0 kernel: [ 1052.523296] IP: [<d45bc512>] mutex_lock+0x12/0x30
			Sep 21 15:57:01 node0 kernel: [ 1052.523399] *pdpt = 0000000014981001 *pde = 00000000140001e1 
			Sep 21 15:57:01 node0 kernel: [ 1052.523513] 
			Sep 21 15:57:01 node0 kernel: [ 1052.524709] Oops: 0003 [#1] SMP
			Sep 21 15:57:01 node0 kernel: [ 1052.525800] Modules linked in: dvk(O) fuse iptable_filter vmwgfx ttm drm_kms_helper joydev drm sg pcspkr vmw_balloon serio_raw evdev shpchp vmw_vmci ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse pcnet32 xhci_pci xhci_hcd ehci_pci mii ata_piix uhci_hcd mptspi ehci_hcd scsi_transport_spi mptscsih mptbase usbcore libata i2c_piix4 scsi_mod
			Sep 21 15:57:01 node0 kernel: [ 1052.533244] CPU: 3 PID: 1383 Comm: migr_server Tainted: G           O    4.9.88-DVK #157
			Sep 21 15:57:01 node0 kernel: [ 1052.535014] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
			Sep 21 15:57:01 node0 kernel: [ 1052.538952] task: dbf875c0 task.stack: eac08000
			Sep 21 15:57:01 node0 kernel: [ 1052.541183] EIP: 0060:[<d45bc512>] EFLAGS: 00010286 CPU: 3
			Sep 21 15:57:01 node0 kernel: [ 1052.543375] EIP is at mutex_lock+0x12/0x30
			Sep 21 15:57:01 node0 kernel: [ 1052.545566] EAX: d41cf565 EBX: d41cf565 ECX: 00000202 EDX: 80000000
			Sep 21 15:57:01 node0 kernel: [ 1052.547959] ESI: dbf87bcc EDI: ea8121c0 EBP: eac09e04 ESP: eac09e00
			Sep 21 15:57:01 node0 kernel: [ 1052.550353]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
			Sep 21 15:57:01 node0 kernel: [ 1052.552880] CR0: 80050033 CR2: d41cf565 CR3: 2d8e87c0 CR4: 000006f0
			Sep 21 15:57:01 node0 kernel: [ 1052.555730] Stack:
			Sep 21 15:57:01 node0 kernel: [ 1052.558382]  00000000 eac09e8c f8dd6bf0 f8e05e60 00000567 f8e0245c 000009bc 00000567
			Sep 21 15:57:01 node0 kernel: [ 1052.560322]  00000000 000003e8 00000002 eac09e64 d40ba4fe 00000000 00000000 00000000
			Sep 21 15:57:01 node0 kernel: [ 1052.562049]  00000000 dbf87bcc ea8121c0 d41cf38d 00000000 dbf875c0 00000000 dbf875c0
			Sep 21 15:57:01 node0 kernel: [ 1052.563821] Call Trace:
			Sep 21 15:57:01 node0 kernel: [ 1052.565598]  [<f8dd6bf0>] ? new_wait4bind+0x1015/0x1d40 [dvk]
			Sep 21 15:57:01 node0 kernel: [ 1052.567494]  [<d40ba4fe>] ? vprintk_emit+0x2ee/0x4f0
			Sep 21 15:57:01 node0 kernel: [ 1052.569418]  [<d41cf38d>] ? __check_object_size+0x9d/0x11c
			Sep 21 15:57:01 node0 kernel: [ 1052.571518]  [<d40a9a00>] ? prepare_to_wait_event+0xd0/0xd0
			Sep 21 15:57:01 node0 kernel: [ 1052.573522]  [<f8dc8e80>] ? io_wait4bind+0x4e/0xce [dvk]
			Sep 21 15:57:01 node0 kernel: [ 1052.575612]  [<f8dc71fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
			Sep 21 15:57:01 node0 kernel: [ 1052.577655]  [<f8dc716d>] ? dvk_write+0x49/0x49 [dvk]
			Sep 21 15:57:01 node0 kernel: [ 1052.579708]  [<d41e6624>] ? do_vfs_ioctl+0x94/0x730
			Sep 21 15:57:01 node0 kernel: [ 1052.581811]  [<d4193dbc>] ? handle_mm_fault+0x9fc/0xf40
			Sep 21 15:57:01 node0 kernel: [ 1052.583932]  [<d41d3159>] ? vfs_write+0x149/0x1c0
			Sep 21 15:57:01 node0 kernel: [ 1052.586081]  [<d41e6d20>] ? SyS_ioctl+0x60/0x70
			Sep 21 15:57:01 node0 kernel: [ 1052.588434]  [<d4003708>] ? do_fast_syscall_32+0x98/0x160
			Sep 21 15:57:01 node0 kernel: [ 1052.590703]  [<d45bf682>] ? sysenter_past_esp+0x47/0x75
			Sep 21 15:57:01 node0 kernel: [ 1052.592952] Code: c3 90 8d b4 26 00 00 00 00 31 c0 5d c3 8d b6 00 00 00 00 8d bf 00 00 00 00 55 89 e5 53 0f 1f 44 00 00 89 c3 e8 a0 ef ff ff 89 d8 <f0> ff 08 79 05 e8 74 07 00 00 64 a1 48 12 97 d4 89 43 10 5b 5d
			Sep 21 15:57:01 node0 kernel: [ 1052.599901] EIP: [<d45bc512>] 
			Sep 21 15:57:01 node0 kernel: [ 1052.599946] mutex_lock+0x12/0x30
			Sep 21 15:57:01 node0 kernel: [ 1052.602158]  SS:ESP 0068:eac09e00
			Sep 21 15:57:01 node0 kernel: [ 1052.604412] CR2: 00000000d41cf565
			Sep 21 15:57:01 node0 kernel: [ 1052.610153] ---[ end trace ea8647ffd123b9cf ]---
		
SOLUCIONADO: Se modifico wait4bind
		 
	TODO: Pensar como seria el proceso de migracion. Que cosas tiene que bindearse.
	
		NODE0														NODE1 
	1) ./tests.sh 0 0												./tests.sh 1 0
	2) . /dev/shm/DC0.sh											. /dev/shm/DC0.sh
	3) nsenter -p -t$DC0 ./dvs_run -l 0 10 ./migr_server 
	4) nsenter -p -t$DC0 ./dvs_run -b 0 11 1 ./migr_client 10 4096 100 1
		HASTA AHORA 
		root@node0:~# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		 0  10    10  2137/4      0    8   20 31438 27342 27342 27342 dvs_run        
		 0  11    11  2227/25     1 1000   30 27342 27342 27342 27342 dvs_run     
	
	5)																nsenter -p -t$DC0 ./test_rmtbind 0 10 0 migr_server	
																		root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs 
																		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
																		 0  10    10    -1/-1     0 1000    0 27342 27342 27342 27342 migr_server  
	6)																nsenter -p -t$DC0 ./dvs_run -l 0 11 ./migr_client 10 4096 100 1	
	7a)						 										./dvs_migrate -s 0 11 	
																	root@node1:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
																	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
																	 0  10    10    -1/-1     0 1000    0 27342 27342 27342 27342 migr_server    
																	 0  11    11   669/6      1  800   20 27342 27342 27342 27342 dvs_run 
	7b)	./dvs_migrate -s 0 11 	
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		 0  10    10   680/4      0    8   20 31438 27342 27342 27342 dvs_run        
		 0  11    11   685/7      1 1800   30 27342 27342 27342 27342 dvs_run 	
	8a)								 								./dvs_migrate -c 0 11 0 
																	root@node1:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
																	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
																	 0  10    10    -1/-1     0 1000    0 27342 27342 27342 27342 migr_server    
																	 0  11    11    -1/6      0 1000    0 27342 27342 27342 27342 dvs_run  	
	8b)	./dvs_migrate -c 0 11 0 0	 								 	
	root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  10    10   680/4      0    8   20 31438 27342 27342 27342 dvs_run        
	 0  11    11   685/7      0    0   20 27342 27342 27342 27342 dvs_run
		
root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10   762/4      0    8   20 31438 27342 27342 27342 dvs_run        
 0  11    11   766/7      0    0   20 27342 27342 27342 27342 dvs_run  

==============================================================================================================
20200923:
	Se probo Migración de REMOTO a LOCAL 
	FUNCIONO OK!!

==============================================================================================================
20200925:
	
	TODO: Probar el commit remoto: Migración de LOCAL A REMOTO 

		NODE0														NODE1 
	1) ./tests.sh 0 0												./tests.sh 1 0
	2) . /dev/shm/DC0.sh											. /dev/shm/DC0.sh
	3) nsenter -p -t$DC0 ./dvs_run -l 0 10 ./migr_server 
	4) 																nsenter -p -t$DC0 ./test_rmtbind 0 11 0 migr_client	
	5)																nsenter -p -t$DC0 ./dvs_run -b 0 10 0 ./migr_server 
																		root@node1:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
																		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
																		 0  10    10   619/5      0 9000   30 27342 27342    10 27342 dvs_run  
																									9000  <<<  BIT_WAITMIGR+BIT_REMOTE  
																										   30 <<< MIS_BIT_RMTBACKUP+MIS_BIT_GRPLEADER					
																		 0  11    11    -1/-1     0 1000    0 27342 27342 27342 27342 migr_client
																									1000 <<< BIT_REMOTE
	6) nsenter -p -t$DC0 ./dvs_run -l 0 11 ./migr_client 10 4096 100 1	
	7a)	./dvs_migrate -s 0 10 	
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		0  10    10   672/4      0  800   20 27342 27342 27342 27342 dvs_run        
		0  11    11   677/7      0 8000   20 27342 27342    10 27342 dvs_run        

	7b)																./dvs_migrate -s 0 10 
	8b)								 								./dvs_migrate -c 0 10 1 0
																	root@node1:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
																	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
																	 0  10    10   619/5      1    0   20 27342 27342    10 27342 dvs_run        
																	 0  11    11    -1/-1     0 1000    0 27342 27342 27342 27342 migr_client  

	8a)	./dvs_migrate -c 0 10 1	 				

==============================================================================================================
20200928:
			FUNCIONO!! pero con el dvs_migrate con varios sleep()
	
			SE agrego codigo en el check_caller() para que los procesos que son BACKUP REMOTOS se detengan
			esperando la migracion del proceso y la conversion a PRIMARY.
			
	TODO:	Si un endpoint esta dado de alta como REMOTO podria convertirse a BACKUP asociandole el PID. 
			OJO porque el wait4bind chechea esto y no permite que el proceso siga adelanta con el binding.
		LISTO 
	
	TODO: 	Habria que analizar cuales son las situaciones en las cuales NO PUEDE HABER MIGRACION 
			se podría poner un flag BIT_MISC_NOMIGRATE, el problema se presenta en los procesos  
			REMOTOS que nada impide que se MIGREN.	
		LISTO 

==============================================================================================================
20200929:	SE MODIFICARON LOS migr_client y migr_server para utilizar o no WAIT4BIND- (Flag USE_WAIT4BIND)
	
	TODO:	PROBAR DMTCP 
	En sesion 1:
	/usr/src/lkl/dmtcp/bin/dmtcp_coordinator -p 1234
			
	En sesion 2: despues de arrancar el DVS 
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# /usr/src/lkl/dmtcp/bin/dmtcp_launch -h node0 -p 1234 ./migr_server 0 10
		DEBUG 43000:dvk_open:108: Open dvk device file /dev/dvk
		 migr_server.c:main:63:[./migr_server] dcid=0 svr_ep=10
		DEBUG 43000:dvk_bind_X:1214: cmd=0 dcid=0 pid=-1 endpoint=10 nodeid=-1
		DEBUG 43000:dvk_bind_X:1229: ioctl ret=10 errno=0
		DEBUG 43000:dvk_bind_X:1241: ioctl ret=10
		DEBUG 43000:dvk_bind_X:1244: ret=10
		 migr_server.c:main:65:[./migr_server] rcode=10
		 migr_server.c:main:69:SERVER svr_pid=43000 svr_ep=10
		DEBUG 43000:dvk_getprocinfo:1170: dcid=-1 p_nr=10 
		DEBUG 43000:dvk_getprocinfo:1183: ioctl ret=0 errno=0
		DEBUG 43000:dvk_getprocinfo:1188: ioctl ret=0
		 migr_server.c:main:80:SERVER: nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		 migr_server.c:main:108:SERVER buffer abcdefghijklmnopqrstuvwxyabcde
		 migr_server.c:main:111:SERVER: Starting loops
		 migr_server.c:main:114:SERVER: Receiving loops=0
		 migr_server.c:main:115:SERVER: BEFORE dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		DEBUG 43000:dvk_receive_T:890: endpoint=31438 timeout=30000
		
	RECIBE EDVSINTR cuando en el coordinador se ordena "bc" Block/Checkpoint, pero como se modifico el fuente, hace un retry del dvk_receive.	
		DEBUG 43000:dvk_receive_T:903: ioctl ret=-1 errno=4
		ERROR: 43000:dvk_receive_T:905: rcode=-4
		DEBUG 43000:dvk_receive_T:913: ioctl ret=-4
		ERROR: 43000:dvk_receive_T:915: rcode=-4
		 migr_server.c:main:117:SERVER: AFTER  dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		 migr_server.c:main:114:SERVER: Receiving loops=0
		 migr_server.c:main:115:SERVER: BEFORE dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		DEBUG 43000:dvk_receive_T:890: endpoint=31438 timeout=30000
	
	DESDE EL COORDINADOR SE ENVÍA UN KILL "k"
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# ls -lt *.dmtcp
		-rw------- 1 root root 2511134 sep 29 16:04 ckpt_migr_server_5e678476-43000-71207cdb.dmtcp
	
	LUEGO SE HACE EL RESTART
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# dmtcp_restart ckpt_migr_server_5e678476-43000-71207cdb.dmtcp 
		WARNING:  Running dmtcp_restart as root can be dangerous.
		  An unknown checkpoint image or bugs in DMTCP may lead to unforeseen
		  consequences.  Continuing as root ....
		DEBUG 43000:dvk_receive_T:903: ioctl ret=-1 errno=4 <<<<<<<<<<<<< ULTIMO ESTADO GRABADO 
		ERROR: 43000:dvk_receive_T:905: rcode=-4
		DEBUG 43000:dvk_receive_T:913: ioctl ret=-4
		ERROR: 43000:dvk_receive_T:915: rcode=-4
	CUANDO VUELVE A HACER EL dvk_receive() obviamente no esta bindeado.	
		 migr_server.c:main:117:SERVER: AFTER  dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		 migr_server.c:main:114:SERVER: Receiving loops=0
		 migr_server.c:main:115:SERVER: BEFORE dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		DEBUG 43000:dvk_receive_T:890: endpoint=31438 timeout=30000
		DEBUG 43000:dvk_receive_T:903: ioctl ret=-1 errno=310
		ERROR: 43000:dvk_receive_T:905: rcode=-310
		DEBUG 43000:dvk_receive_T:913: ioctl ret=-310
		ERROR: 43000:dvk_receive_T:915: rcode=-310
		 migr_server.c:main:117:SERVER: AFTER  dvk_receive_T nr=10 endp=10 dcid=0 flags=0 misc=20 lpid=1429 vpid=1429 nodeid=0 name=migr_server 
		ERROR: migr_server.c:main:122: rcode=-310


	LISTO: 
			Cuando se recibe un (-310) EDVSNOTBIND se tiene que hacer la inicializacion del DVK nuevamente.
	
			1) si get_dvsinfo == EDVSNOTTY es porque no esta el open del dvk
				dvk_open()
				nodeid = get_dvsinfo();
			2) si nodeid != local_nodeid 
				local_nodeid = nodeid 
				ESTOY EN OTRO NODO   
				Si estoy en otro NODO, 
					si se quiere hacer el bind y ya existe el endpoint bindeado retorna rcode=-337 EDVSSLOTUSED  
					hay que hacer dvk_getprocinfo() para ver cual es el tipo de proceso bindedo
					si el actual es REMOTO, hay que hacer migr_start y luego migr_commit con el PID propio (PROC_NO_PID)
					si el actual es LOCAL o REPLICA, ==> EXIT 
					
	FUNCIONA PERFECTO LOCAL!! 
		DESPUES DE CHECKPOINT->KILL se hace RESTART 
			root@node0:/usr/src/dvs/dvs-apps/dvs_run# dmtcp_restart ckpt_migr_server_5e678476-45000-ffffffffc1ed4e19.dmtcp 
			WARNING:  Running dmtcp_restart as root can be dangerous.
			  An unknown checkpoint image or bugs in DMTCP may lead to unforeseen
			  consequences.  Continuing as root ....
			  
			DEBUG 45000:dvk_receive_T:903: ioctl ret=-1 errno=4 <<<<<<<<< RECIBE EDVSINTR
			ERROR: 45000:dvk_receive_T:905: rcode=-4
			DEBUG 45000:dvk_receive_T:913: ioctl ret=-4
			ERROR: 45000:dvk_receive_T:915: rcode=-4
			
			 migr_server.c:main:187:SERVER: Receiving loops=0
			DEBUG 45000:dvk_receive_T:890: endpoint=31438 timeout=30000 <<<< EDVSNOTBIND
			DEBUG 45000:dvk_receive_T:903: ioctl ret=-1 errno=310
			ERROR: 45000:dvk_receive_T:905: rcode=-310
			DEBUG 45000:dvk_receive_T:913: ioctl ret=-310
			ERROR: 45000:dvk_receive_T:915: rcode=-310
			migr_server.c:get_dvs_params:90:
			DEBUG 45000:dvk_getdvsinfo:258: 
			DEBUG 45000:dvk_getdvsinfo:267: ioctl ret=0 errno=0
			DEBUG 45000:dvk_getdvsinfo:272: ioctl ret=0
			 migr_server.c:get_dvs_params:92:nodeid=0
			 migr_server.c:get_dvs_params:96:d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			 migr_server.c:get_dvs_params:90:
			DEBUG 45000:dvk_getdvsinfo:258: 
			DEBUG 45000:dvk_getdvsinfo:267: ioctl ret=0 errno=0
			DEBUG 45000:dvk_getdvsinfo:272: ioctl ret=0
			 migr_server.c:get_dvs_params:92:nodeid=0
			 migr_server.c:get_dvs_params:96:d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
			 migr_server.c:migr_restart:47:nodeid=0 local_nodeid=0
			 migr_server.c:migr_restart:75:dcid=0 svr_ep=10
			DEBUG 45000:dvk_bind_X:1214: cmd=0 dcid=0 pid=-1 endpoint=10 nodeid=-1
			DEBUG 45000:dvk_bind_X:1229: ioctl ret=10 errno=0
			DEBUG 45000:dvk_bind_X:1241: ioctl ret=10
			DEBUG 45000:dvk_bind_X:1244: ret=10
			 migr_server.c:migr_restart:77:rcode=10
			 
			DEBUG 45000:dvk_receive_T:890: endpoint=31438 timeout=30000
			DEBUG 45000:dvk_receive_T:903: ioctl ret=-1 errno=61
			ERROR: 45000:dvk_receive_T:905: rcode=-61
			DEBUG 45000:dvk_receive_T:913: ioctl ret=-61
			ERROR: 45000:dvk_receive_T:915: rcode=-61
			 migr_server.c:main:187:SERVER: Receiving loops=0
			DEBUG 45000:dvk_receive_T:890: endpoint=31438 timeout=30000

==============================================================================================================
20200930:
		
	TODO: PROBAR REMOTO 
		Escenario PREVIO
			migr_server: NODE0
			migr_client: NODE1
		Escenario FINAL:
			migr_server: NODE1
			migr_client: NODE1
			
		NODE0																	NODE1 
	1) cd /usr/src/dvs/dvk-tests/ 												cd /usr/src/dvs/dvk-tests/ 
	   ./tests.sh 0 0															./tests.sh 1 0
	2) . /dev/shm/DC0.sh														. /dev/shm/DC0.sh
	3) nsenter -p -t$DC0 ./test_rmtbind 0 11 1 migr_client						nsenter -p -t$DC0 ./test_rmtbind 0 10 0 migr_server
		cd /usr/src/dvs/dvs-apps/dvs_run										cd /usr/src/dvs/dvs-apps/dvs_run
		rm *.dmtcp 																rm *.dmtcp 
	4) EN OTRA TERMINAL /usr/src/lkl/dmtcp/bin/dmtcp_coordinator -p 1234  
	EN LA ANTERIOR /usr/src/lkl/dmtcp/bin/dmtcp_launch -h node0 -p 1234 ./migr_server 0 10 
	5) 																			nsenter -p -t$DC0 ./migr_client 0 11 10 4096 100 1 &
						---------------------------- EJECUTANDO ------------------------------
																				cd /usr/src/dvs/dvs-apps/dvs_run
	6a)																			./dvs_migrate -s 0 10
	6b) EN OTRA TERMINAL cd /usr/src/dvs/dvs-apps/dvs_run
	    ./dvs_migrate -s 0 10	
	7) /usr/src/lkl/dmtcp/bin/dmtcp_command   -h node0 -p 1234 -bc				nc -l -p 7555 > checkpoint_migr_server.dmtcp
       /usr/src/lkl/dmtcp/bin/dmtcp_command   -h node0 -p 1234 -k 
		mv ckpt_*.dmtcp checkpoint_migr_server.dmtcp
	8)  nc node1 7555 < checkpoint_migr_server.dmtcp
	9)																			dmtcp_restart checkpoint_migr_server.dmtcp
						---------------------------- EJECUTANDO ------------------------------
	10)																			./dvs_migrate -s 0 10	
	11) 																		./dvs_migrate -r 0 10
						---------------------------- EJECUTANDO ------------------------------

==============================================================================================================



20201002:
		Escenario PREVIO
			migr_server: NODE1
			migr_client: NODE1
		Escenario FINAL:
			migr_server: NODE0
			migr_client: NODE1


		NODE0																	NODE1 
	1) cd /usr/src/dvs/dvk-tests/ 												cd /usr/src/dvs/dvk-tests/ 
	   ./tests.sh 0 0															./tests.sh 1 0
	2) . /dev/shm/DC0.sh														. /dev/shm/DC0.sh
	3) 	nsenter -p -t$DC0 ./test_rmtbind 0 11 1 migr_client
		nsenter -p -t$DC0 ./test_rmtbind 0 10 1 migr_server	
		cd /usr/src/dvs/dvs-apps/dvs_run										cd /usr/src/dvs/dvs-apps/dvs_run
		rm *.dmtcp 																rm *.dmtcp 
	4) 																			EN OTRA TERMINAL /usr/src/lkl/dmtcp/bin/dmtcp_coordinator -p 1234  
																				EN LA ANTERIOR /usr/src/lkl/dmtcp/bin/dmtcp_launch -h node1 -p 1234 ./migr_server 0 10 
	5) 																			EN OTRA TERMINAL 
																				. /dev/shm/DC0.sh
																				cd /usr/src/dvs/dvs-apps/dvs_run
																				nsenter -p -t$DC0 ./migr_client 0 11 10 4096 100 1 &
						---------------------------- EJECUTANDO ------------------------------
	6a)																			./dvs_migrate -s 0 10
	6b)./dvs_migrate -s 0 10	
		nc -l -p 7555 > checkpoint_migr_server.dmtcp
	7) 																			/usr/src/lkl/dmtcp/bin/dmtcp_command   -h node1 -p 1234 -bc				
																				mv ckpt_*.dmtcp checkpoint_migr_server.dmtcp
	8)  																		nc node0 7555 < checkpoint_migr_server.dmtcp
	9)	dmtcp_restart checkpoint_migr_server.dmtcp
    10) 																		./dvs_migrate -c 0 10 0  
							---------------------------- EJECUTANDO ------------------------------
																				/usr/src/lkl/dmtcp/bin/dmtcp_command   -h node1 -p 1234 -k 

 	

==============================================================================================================


	TODO: PROBAR REMOTO 
		Escenario PREVIO
			migr_server: NODE0
			migr_client: NODE1
		Escenario FINAL:
			migr_server: NODE1
			migr_client: NODE1
			
		NODE0																	NODE1 
	1) cd /usr/src/dvs/dvk-tests/ 												cd /usr/src/dvs/dvk-tests/ 
	   ./tests.sh 0 0															./tests.sh 1 0
	2) . /dev/shm/DC0.sh														. /dev/shm/DC0.sh
	3) nsenter -p -t$DC0 ./test_rmtbind 0 11 1 migr_client						nsenter -p -t$DC0 ./test_rmtbind 0 10 0 migr_server
		cd /usr/src/dvs/dvs-apps/dvs_run										cd /usr/src/dvs/dvs-apps/dvs_run
		rm *.dmtcp 																rm *.dmtcp 
		
		
	EN LA ANTERIOR /usr/src/lkl/dmtcp/bin/dmtcp_launch -h node0 -p 1234 ./migr_server 0 10 
	5) 																			nsenter -p -t$DC0 ./migr_client 0 11 10 4096 100 1 &
						---------------------------- EJECUTANDO ------------------------------
																				cd /usr/src/dvs/dvs-apps/dvs_run
	6a)																			./dvs_migrate -s 0 10
	6b) EN OTRA TERMINAL cd /usr/src/dvs/dvs-apps/dvs_run
	    ./dvs_migrate -s 0 10	
	7) /usr/src/lkl/dmtcp/bin/dmtcp_command   -h node0 -p 1234 -bc				
       /usr/src/lkl/dmtcp/bin/dmtcp_command   -h node0 -p 1234 -k 
		mv ckpt_*.dmtcp checkpoint_migr_server.dmtcp
	8) sshpass -p "root" scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -r checkpoint_migr_server.dmtcp root@node1:/usr/src/dvs/dvs-apps/dvs_run
	9)																			dmtcp_restart checkpoint_migr_server.dmtcp
						---------------------------- EJECUTANDO ------------------------------
	10)																			./dvs_migrate -s 0 10	
	11) 																		./dvs_migrate -r 0 10
						---------------------------- EJECUTANDO ------------------------------



    Para hacer las transferencias de imagenes usar 
		 sshpass -p "root" scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -r checkpoint_migr_server.dmtcp root@node1:/usr/src/dvs/dvs-apps/dvs_run

		Ver que pasa en el siguiete escenario 
		Escenario PREVIO
			migr_server: NODE1
			migr_client: NODE1
		Escenario FINAL:
			migr_server: NODE0
			migr_client: NODE1
			
			Que pasa cuando mato al proceso ya migrado en el NODO1 ???
			Si se hace el unbind() no deberia pasar nada porque el descriptor ahora no esta usado 
			Verificar que pasa en el migrate commit y en el unbind 
			
			
	

			Aplicar los cambios que se hicieron en new_mini_send y new_mini_sendrec a
			new_rcvrqst y new_reply
			
			
			Supongamos que un server en NODO0 quiere migrar.
			El programa de gestion de migración deberia enviar un mensaje a todos los nodos de esa intención.
			En todo nodo, debería setearse el descriptor del proceso con BIT_MIGRATE.
			Si el NODOx donde recide el proceso no puede setearlo porque tiene el descriptor tiene bit BIT_MISC_NOMIGRATE
			se lo informa al NODO0 para que haga lo que le parezca 
				1) Decirle al NODOx que reintente 
				2) Hacer un rollback de la migración
				
		Cuando un server recibe un mensaje de un proceso via dvk_sendrec(), evita su MIGRACION hasta dar la respuesta			
			sea un sendrec() local o remoto.
			De igual forma habria que hacer con las COPIAS.

Como estamos en migracion en modo usuario, lo mejor seria volver a MODO USUARIO con un error tipo EDVSTIMEOUT en caso de que sea con
tiempo sino EDVSMIGRATE si no es con tiempo.

ATENCION: Segun como trabaja DMTCP puede que el groupleader no se al main!!!!  
	En dvk_migrate NO SE HACE la migración si no es LEADER.
	if(!test_bit(MIS_BIT_GRPLEADER, &proc_ptr->p_usr.p_misc_flags))	
		ERROR_RETURN(EDVSGRPLEADER);


	 	
		Escenario PREVIO
			migr_server: NODE0
			migr_client: NODE1
		Escenario FINAL:
			migr_server: NODE2
			migr_client: NODE1
==============================================================================================================
20201011:
		Como al probar dvs_uml_switch y presionar CTRL-C dio error el unbind se hicieron algunos cambios
		PERO NO SE APLICARON EN EL CODIGO PORQUE EL KERNEL ESTA COMPILADO PARA UML !!!!!!
		
		Se cambio SIGPIPE por SIGTERM
		Se cambiaron seteos de BITs =| por setbit()
		Se hace DVKDEBUG de cuando en new_dvkbind_X se detecta que el proceso es el GRPLEADER dado
		que en las pruebas del dvs_uml_switch aparecía un thread hijo como leader. 
		Quizás el problema se da porque el main no hace bind, solo los threads hijos.
		
		
		
ERRORES DE LISTAS !!! 

Oct 21 12:30:54 node1 kernel: [ 4643.542943] DEBUG 872:dvk_ioctl:369: DVK_CALL=2 (io_mini_send) 
Oct 21 12:30:54 node1 kernel: [ 4643.542945] DEBUG 872:io_mini_send:24: 
Oct 21 12:30:54 node1 kernel: [ 4643.542948] DEBUG 872:new_mini_send:32: dst_ep=30
Oct 21 12:30:54 node1 kernel: [ 4643.542951] DEBUG 872:check_caller:605: caller_pid=872 caller_tgid=869
Oct 21 12:30:54 node1 kernel: [ 4643.542954] DEBUG 872:check_caller:643: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543363] DEBUG 872:check_caller:648: nr=1 endp=1 dcid=0 flags=0 misc=1000 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.543369] DEBUG 872:check_caller:708: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543372] DEBUG 872:check_caller:711: dcid=0
Oct 21 12:30:54 node1 kernel: [ 4643.543375] DEBUG 872:check_caller:715: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543378] DEBUG 872:check_caller:719: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543380] DEBUG 872:check_caller:725: caller_pid=872 
Oct 21 12:30:54 node1 kernel: [ 4643.543382] DEBUG 872:check_caller:728: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543384] DEBUG 872:check_caller:732: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543388] DEBUG 872:new_mini_send:44: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543390] DEBUG 872:new_mini_send:48: caller_nr=1 caller_ep=1 dst_ep=30 
Oct 21 12:30:54 node1 kernel: [ 4643.543393] DEBUG 872:new_mini_send:53: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543394] DEBUG 872:new_mini_send:55: dcid=0
Oct 21 12:30:54 node1 kernel: [ 4643.543397] DEBUG 872:new_mini_send:59: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543399] DEBUG 872:new_mini_send:64: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543401] DEBUG 872:new_mini_send:81: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543403] DEBUG 872:new_mini_send:88: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543406] DEBUG 872:new_mini_send:91: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543408] DEBUG 872:new_mini_send:91: WLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543410] DEBUG 872:new_mini_send:98: dst_nr=30 dst_ep=30
Oct 21 12:30:54 node1 kernel: [ 4643.543412] DEBUG 872:new_mini_send:117: dst_ptr->p_usr.p_nodeid=0
Oct 21 12:30:54 node1 kernel: [ 4643.543415] DEBUG 872:new_mini_send:121: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543417] DEBUG 872:new_mini_send:125: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.543658] DEBUG 872:new_mini_send:142: dcid=0 caller_pid=872 caller_nr=1 dst_ep=30 
Oct 21 12:30:54 node1 kernel: [ 4643.543663] DEBUG 872:new_mini_send:150: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.543665] DEBUG 872:new_mini_send:162: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.545482] DEBUG 872:new_mini_send:186: WUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.545493] DEBUG 872:sproxy_enqueue:28: nr=1 endp=1 dcid=0 flags=2004 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.545497] DEBUG 872:sproxy_enqueue:30: cmd=0x1 dcid=0 src=1 dst=30 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.545513] DEBUG 872:sproxy_enqueue:33: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.545515] DEBUG 872:sproxy_enqueue:40: LIST_ADD_TAIL
Oct 21 12:30:54 node1 kernel: [ 4643.545521] ------------[ cut here ]------------
Oct 21 12:30:54 node1 kernel: [ 4643.545876] WARNING: CPU: 2 PID: 872 at lib/list_debug.c:33 __list_add+0xb4/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.545885] list_add corruption. prev->next should be next (f8ad9ccc), but was f61e46b4. (prev=f61e46b4).
Oct 21 12:30:54 node1 kernel: [ 4643.545891] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4
Oct 21 12:30:54 node1 kernel: [ 4643.547268] CPU: 2 PID: 872 Comm: dvs_uml_switch Tainted: G           O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.547272] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.547275]  f69b5d80 da2b1fa2 f69b5d94 00000000 da06778a da797bd8 f69b5db4 00000368
Oct 21 12:30:54 node1 kernel: [ 4643.547285]  da79753c 00000021 da2d0bf4 00000021 00000009 da2d0bf4 f61e46b4 f61e4600
Oct 21 12:30:54 node1 kernel: [ 4643.547296]  00000000 f69b5da0 da0677f6 00000009 00000000 f69b5d94 da797bd8 f69b5db4
Oct 21 12:30:54 node1 kernel: [ 4643.547303] Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.547459]  [<da2b1fa2>] ? dump_stack+0x55/0x73
Oct 21 12:30:54 node1 kernel: [ 4643.547652]  [<da06778a>] ? __warn+0xea/0x110
Oct 21 12:30:54 node1 kernel: [ 4643.547659]  [<da2d0bf4>] ? __list_add+0xb4/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.547662]  [<da2d0bf4>] ? __list_add+0xb4/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.547667]  [<da0677f6>] ? warn_slowpath_fmt+0x46/0x60
Oct 21 12:30:54 node1 kernel: [ 4643.547670]  [<da2d0bf4>] ? __list_add+0xb4/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.547825]  [<f8ab4e78>] ? sproxy_enqueue+0xa4/0x4a3 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.547840]  [<f8ac0cd2>] ? new_mini_send+0xfa5/0x194b [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.547850]  [<f8a95b4c>] ? io_mini_send+0x4e/0xce [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.547858]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.547866]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.547871]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4643.547876]  [<da1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4643.547879]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4643.547884]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4643.547938]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4643.547943] ---[ end trace 7b4305b5b9585091 ]---
Oct 21 12:30:54 node1 kernel: [ 4643.547945] ------------[ cut here ]------------
Oct 21 12:30:54 node1 kernel: [ 4643.547950] WARNING: CPU: 2 PID: 872 at lib/list_debug.c:36 __list_add+0xfc/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.547952] list_add double add: new=f61e46b4, prev=f61e46b4, next=f8ad9ccc.
Oct 21 12:30:54 node1 kernel: [ 4643.547953] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4
Oct 21 12:30:54 node1 kernel: [ 4643.548006] CPU: 2 PID: 872 Comm: dvs_uml_switch Tainted: G        W  O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.548008] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.548009]  f69b5d80 da2b1fa2 f69b5d94 00000000 da06778a da797c28 f69b5db4 00000368
Oct 21 12:30:54 node1 kernel: [ 4643.548018]  da79753c 00000024 da2d0c3c 00000024 00000009 da2d0c3c f61e46b4 f61e4600
Oct 21 12:30:54 node1 kernel: [ 4643.548025]  00000000 f69b5da0 da0677f6 00000009 00000000 f69b5d94 da797c28 f69b5db4
Oct 21 12:30:54 node1 kernel: [ 4643.548032] Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.548037]  [<da2b1fa2>] ? dump_stack+0x55/0x73
Oct 21 12:30:54 node1 kernel: [ 4643.548042]  [<da06778a>] ? __warn+0xea/0x110
Oct 21 12:30:54 node1 kernel: [ 4643.548045]  [<da2d0c3c>] ? __list_add+0xfc/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.548048]  [<da2d0c3c>] ? __list_add+0xfc/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.548053]  [<da0677f6>] ? warn_slowpath_fmt+0x46/0x60
Oct 21 12:30:54 node1 kernel: [ 4643.548056]  [<da2d0c3c>] ? __list_add+0xfc/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.548068]  [<f8ab4e78>] ? sproxy_enqueue+0xa4/0x4a3 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.548078]  [<f8ac0cd2>] ? new_mini_send+0xfa5/0x194b [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.548088]  [<f8a95b4c>] ? io_mini_send+0x4e/0xce [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.548095]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.548104]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.548107]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4643.548111]  [<da1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4643.548115]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4643.548118]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4643.548122]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4643.548124] ---[ end trace 7b4305b5b9585092 ]---
Oct 21 12:30:54 node1 kernel: [ 4643.548130] DEBUG 872:sproxy_enqueue:42: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=724 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.548132] DEBUG 872:sproxy_enqueue:49: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.548135] DEBUG 872:sleep_proc:355: timeout=30000
Oct 21 12:30:54 node1 kernel: [ 4643.548138] DEBUG 872:sleep_proc:366: BEFORE DOWN lpid=872 p_sem=0 timeout=30000
Oct 21 12:30:54 node1 kernel: [ 4643.548140] DEBUG 872:sleep_proc:368: endpoint=1 flags=2004
Oct 21 12:30:54 node1 kernel: [ 4643.548143] DEBUG 872:sleep_proc:381: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.551490] DEBUG 724:sleep_proc:390: pending: sig[0]:0x00000000, sig[1]:0x00000000
Oct 21 12:30:54 node1 kernel: [ 4643.551496] DEBUG 724:sleep_proc:393: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
Oct 21 12:30:54 node1 kernel: [ 4643.551500] DEBUG 724:sleep_proc:399: endpoint=27342 ret=0 p_rcode=0
Oct 21 12:30:54 node1 kernel: [ 4643.551503] DEBUG 724:sleep_proc:400: endpoint=27342 flags=0 cpuid=2
Oct 21 12:30:54 node1 kernel: [ 4643.551505] DEBUG 724:sleep_proc:401: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.551508] DEBUG 724:sleep_proc:429: pid=724 ret=-61
Oct 21 12:30:54 node1 kernel: [ 4643.551513] DEBUG 724:sleep_proc:454: nr=0 endp=27342 dcid=-1 lpid=724 p_cpumask=FFFFFFFF nodemap=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.551515] DEBUG 724:sleep_proc:456: someone wakeups me: sem=0 p_rcode=-61
Oct 21 12:30:54 node1 kernel: [ 4643.551518] DEBUG 724:new_get2rmt:577: Someone wakes up the sender proxy
Oct 21 12:30:54 node1 kernel: [ 4643.551521] DEBUG 724:new_get2rmt:580: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.551523] ERROR: 724:new_get2rmt:581: rcode=-61
Oct 21 12:30:54 node1 kernel: [ 4643.551526] ERROR: 724:dvk_ioctl:373: rcode=-61
Oct 21 12:30:54 node1 kernel: [ 4643.552517] DEBUG 724:dvk_ioctl:349: cmd=8004E315 arg=BFF69960
Oct 21 12:30:54 node1 kernel: [ 4643.552522] DEBUG 724:dvk_ioctl:369: DVK_CALL=21 (io_get2rmt) 
Oct 21 12:30:54 node1 kernel: [ 4643.552524] DEBUG 724:io_get2rmt:228: 
Oct 21 12:30:54 node1 kernel: [ 4643.552528] DEBUG 724:new_get2rmt:87: 
Oct 21 12:30:54 node1 kernel: [ 4643.552533] DEBUG 724:check_caller:605: caller_pid=724 caller_tgid=724
Oct 21 12:30:54 node1 kernel: [ 4643.552536] DEBUG 724:check_caller:643: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552541] DEBUG 724:check_caller:648: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=724 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.552544] DEBUG 724:check_caller:708: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552546] DEBUG 724:check_caller:725: caller_pid=724 
Oct 21 12:30:54 node1 kernel: [ 4643.552548] DEBUG 724:check_caller:728: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552550] DEBUG 724:check_caller:732: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552671] DEBUG 724:new_get2rmt:104: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552679] DEBUG 724:new_get2rmt:123: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552791] DEBUG 724:new_get2rmt:124: RLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552796] DEBUG 724:new_get2rmt:129: RUNLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552799] DEBUG 724:new_get2rmt:134: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552801] DEBUG 724:new_get2rmt:136: LIST_FOR_EACH_ENTRY_SAFE
Oct 21 12:30:54 node1 kernel: [ 4643.552804] DEBUG 724:new_get2rmt:140: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552806] DEBUG 724:new_get2rmt:141: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552808] DEBUG 724:new_get2rmt:142: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552810] DEBUG 724:new_get2rmt:146: Found a message. p_endpoint=1 c_cmd=1
Oct 21 12:30:54 node1 kernel: [ 4643.552812] DEBUG 724:new_get2rmt:148: LIST_DEL
Oct 21 12:30:54 node1 kernel: [ 4643.552817] DEBUG 724:new_get2rmt:159: nr=1 endp=1 dcid=0 flags=2004 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.552821] DEBUG 724:new_get2rmt:165: cmd=0x1 dcid=0 src=1 dst=30 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.552825] DEBUG 724:new_get2rmt:168: source=0 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=  (null) m1p2=  (null) m1p3=  (null) 
Oct 21 12:30:54 node1 kernel: [ 4643.552828] DEBUG 724:new_get2rmt:172: n_nodeid=0 n_proxies=0 n_flags=E n_dcs=1 n_name=node0
Oct 21 12:30:54 node1 kernel: [ 4643.552831] DEBUG 724:new_get2rmt:181: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552833] DEBUG 724:new_get2rmt:183: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552835] DEBUG 724:new_get2rmt:185: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552837] DEBUG 724:new_get2rmt:194: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552839] DEBUG 724:new_get2rmt:197: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552842] DEBUG 724:new_get2rmt:204: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552844] DEBUG 724:new_get2rmt:206: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552846] DEBUG 724:new_get2rmt:243: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552848] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552851] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552853] DEBUG 724:new_get2rmt:258: WUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552857] DEBUG 724:new_get2rmt:264: TIMESTAMP sec=1603294254 nsec=347051195
Oct 21 12:30:54 node1 kernel: [ 4643.552862] DEBUG 724:new_get2rmt:425: LOCAL nr=1 endp=1 dcid=0 flags=2004 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.552864] DEBUG 724:new_get2rmt:486: CMD_SEND_MSG
Oct 21 12:30:54 node1 kernel: [ 4643.552868] DEBUG 724:new_get2rmt:489: source=1 type=8194 m1i1=-61 m1i2=98 m1i3=4396128 m1p1=31706174 m1p2=  (null) m1p3=  (null) 
Oct 21 12:30:54 node1 kernel: [ 4643.552872] DEBUG 724:new_get2rmt:541: cmd=0x1 dcid=0 src=1 dst=30 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.552875] DEBUG 724:new_get2rmt:542: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
Oct 21 12:30:54 node1 kernel: [ 4643.552877] DEBUG 724:new_get2rmt:544: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552882] DEBUG 724:new_get2rmt:552: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552884] DEBUG 724:new_get2rmt:560: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.552921] DEBUG 724:dvk_ioctl:349: cmd=8004E30F arg=BFF69960
Oct 21 12:30:54 node1 kernel: [ 4643.552924] DEBUG 724:dvk_ioctl:369: DVK_CALL=15 (io_getprocinfo) 
Oct 21 12:30:54 node1 kernel: [ 4643.552926] DEBUG 724:io_getprocinfo:164: 
Oct 21 12:30:54 node1 kernel: [ 4643.552929] DEBUG 724:new_getprocinfo:2132: dcid=0 p_nr=1
Oct 21 12:30:54 node1 kernel: [ 4643.552933] DEBUG 724:new_getprocinfo:2161: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552936] DEBUG 724:new_getprocinfo:2168: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552938] DEBUG 724:new_getprocinfo:2245: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552941] DEBUG 724:new_getprocinfo:2246: lpid=872 name=dvs_uml_switch
Oct 21 12:30:54 node1 kernel: [ 4643.552944] DEBUG 724:new_getprocinfo:2248: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552952] DEBUG 724:dvk_ioctl:349: cmd=8004E30F arg=BFF69960
Oct 21 12:30:54 node1 kernel: [ 4643.552954] DEBUG 724:dvk_ioctl:369: DVK_CALL=15 (io_getprocinfo) 
Oct 21 12:30:54 node1 kernel: [ 4643.552956] DEBUG 724:io_getprocinfo:164: 
Oct 21 12:30:54 node1 kernel: [ 4643.552958] DEBUG 724:new_getprocinfo:2132: dcid=0 p_nr=30
Oct 21 12:30:54 node1 kernel: [ 4643.552960] DEBUG 724:new_getprocinfo:2161: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552962] DEBUG 724:new_getprocinfo:2168: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552964] DEBUG 724:new_getprocinfo:2245: RLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.552967] DEBUG 724:new_getprocinfo:2246: lpid=-1 name=TAPclient00
Oct 21 12:30:54 node1 kernel: [ 4643.552976] DEBUG 724:new_getprocinfo:2248: RUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.556183] DEBUG 724:dvk_ioctl:349: cmd=8004E315 arg=BFF69960
Oct 21 12:30:54 node1 kernel: [ 4643.556278] DEBUG 724:dvk_ioctl:369: DVK_CALL=21 (io_get2rmt) 
Oct 21 12:30:54 node1 kernel: [ 4643.556280] DEBUG 724:io_get2rmt:228: 
Oct 21 12:30:54 node1 kernel: [ 4643.556284] DEBUG 724:new_get2rmt:87: 
Oct 21 12:30:54 node1 kernel: [ 4643.556289] DEBUG 724:check_caller:605: caller_pid=724 caller_tgid=724
Oct 21 12:30:54 node1 kernel: [ 4643.570061] DEBUG 724:check_caller:643: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570070] DEBUG 724:check_caller:648: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=724 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.570074] DEBUG 724:check_caller:708: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570077] DEBUG 724:check_caller:725: caller_pid=724 
Oct 21 12:30:54 node1 kernel: [ 4643.570079] DEBUG 724:check_caller:728: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570084] DEBUG 724:check_caller:732: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570087] DEBUG 724:new_get2rmt:104: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570090] DEBUG 724:new_get2rmt:123: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570092] DEBUG 724:new_get2rmt:124: RLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570094] DEBUG 724:new_get2rmt:129: RUNLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570096] DEBUG 724:new_get2rmt:134: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570098] DEBUG 724:new_get2rmt:136: LIST_FOR_EACH_ENTRY_SAFE
Oct 21 12:30:54 node1 kernel: [ 4643.570100] DEBUG 724:new_get2rmt:140: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570103] DEBUG 724:new_get2rmt:141: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570105] DEBUG 724:new_get2rmt:142: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570107] DEBUG 724:new_get2rmt:146: Found a message. p_endpoint=1 c_cmd=0
Oct 21 12:30:54 node1 kernel: [ 4643.570109] DEBUG 724:new_get2rmt:148: LIST_DEL
Oct 21 12:30:54 node1 kernel: [ 4643.570112] ------------[ cut here ]------------
Oct 21 12:30:54 node1 kernel: [ 4643.570123] WARNING: CPU: 2 PID: 724 at lib/list_debug.c:53 __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.570125] list_del corruption, f61e46b4->next is LIST_POISON1 (00000100)
Oct 21 12:30:54 node1 kernel: [ 4643.570127] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4
Oct 21 12:30:54 node1 kernel: [ 4643.570198] CPU: 2 PID: 724 Comm: lz4tcp_proxy_ba Tainted: G        W  O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.570200] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.570203]  e0b7fda4 da2b1fa2 e0b7fdb8 00000000 da06778a da797c58 e0b7fdd8 000002d4
Oct 21 12:30:54 node1 kernel: [ 4643.570212]  da79753c 00000035 da2d0cc0 00000035 00000009 da2d0cc0 f61e46b4 e55a0b40
Oct 21 12:30:54 node1 kernel: [ 4643.570220]  f61e4600 e0b7fdc4 da0677f6 00000009 00000000 e0b7fdb8 da797c58 e0b7fdd8
Oct 21 12:30:54 node1 kernel: [ 4643.570227] Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.570237]  [<da2b1fa2>] ? dump_stack+0x55/0x73
Oct 21 12:30:54 node1 kernel: [ 4643.570243]  [<da06778a>] ? __warn+0xea/0x110
Oct 21 12:30:54 node1 kernel: [ 4643.570247]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.570250]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.570254]  [<da0677f6>] ? warn_slowpath_fmt+0x46/0x60
Oct 21 12:30:54 node1 kernel: [ 4643.570258]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.570261]  [<da2d0d68>] ? list_del+0x8/0x20
Oct 21 12:30:54 node1 kernel: [ 4643.570275]  [<f8ab5c24>] ? new_get2rmt+0x9ad/0x4338 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.570378]  [<da0b4901>] ? mutex_optimistic_spin+0x161/0x190
Oct 21 12:30:54 node1 kernel: [ 4643.570386]  [<da1cf38d>] ? __check_object_size+0x9d/0x11c
Oct 21 12:30:54 node1 kernel: [ 4643.570398]  [<f8a969a7>] ? io_get2rmt+0x4e/0xce [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.570411]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.570419]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.570422]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4643.570427]  [<da1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4643.570430]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4643.570435]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4643.570440]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4643.570443] ---[ end trace 7b4305b5b9585093 ]---
Oct 21 12:30:54 node1 kernel: [ 4643.570449] DEBUG 724:new_get2rmt:159: nr=1 endp=1 dcid=0 flags=4 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.570453] DEBUG 724:new_get2rmt:165: cmd=0x0 dcid=0 src=1 dst=30 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.570457] DEBUG 724:new_get2rmt:168: source=0 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=  (null) m1p2=  (null) m1p3=  (null) 
Oct 21 12:30:54 node1 kernel: [ 4643.570461] DEBUG 724:new_get2rmt:172: n_nodeid=0 n_proxies=0 n_flags=E n_dcs=1 n_name=node0
Oct 21 12:30:54 node1 kernel: [ 4643.570463] DEBUG 724:new_get2rmt:181: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570465] DEBUG 724:new_get2rmt:183: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570468] DEBUG 724:new_get2rmt:185: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570470] DEBUG 724:new_get2rmt:194: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570472] DEBUG 724:new_get2rmt:197: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.570475] DEBUG 724:new_get2rmt:204: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570477] DEBUG 724:new_get2rmt:206: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570479] DEBUG 724:new_get2rmt:243: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570481] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570484] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570486] DEBUG 724:new_get2rmt:258: WUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570490] DEBUG 724:new_get2rmt:264: TIMESTAMP sec=1603294254 nsec=363050474
Oct 21 12:30:54 node1 kernel: [ 4643.570494] DEBUG 724:new_get2rmt:425: LOCAL nr=1 endp=1 dcid=0 flags=4 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.570498] DEBUG 724:inherit_cpu:298: cpuid=2 vpid=13
Oct 21 12:30:54 node1 kernel: [ 4643.570502] ERROR: 724:inherit_cpu:302: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.570506] DEBUG 724:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=872 p_cpumask=FFFFFFFF nodemap=2 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.570508] DEBUG 724:new_get2rmt:524: BEFORE UP lpid=872 p_sem=-1
Oct 21 12:30:54 node1 kernel: [ 4643.570524] DEBUG 724:new_get2rmt:525: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570536] DEBUG 872:sleep_proc:390: pending: sig[0]:0x00000000, sig[1]:0x00000000
Oct 21 12:30:54 node1 kernel: [ 4643.570539] DEBUG 872:sleep_proc:393: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
Oct 21 12:30:54 node1 kernel: [ 4643.570541] DEBUG 872:sleep_proc:399: endpoint=1 ret=7497 p_rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.570544] DEBUG 872:sleep_proc:400: endpoint=1 flags=0 cpuid=2
Oct 21 12:30:54 node1 kernel: [ 4643.570546] DEBUG 872:sleep_proc:401: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570548] DEBUG 872:sleep_proc:429: pid=872 ret=-22
Oct 21 12:30:54 node1 kernel: [ 4643.570552] DEBUG 872:sleep_proc:454: nr=1 endp=1 dcid=0 lpid=872 p_cpumask=FFFFFFFF nodemap=2 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.570554] DEBUG 872:sleep_proc:456: someone wakeups me: sem=0 p_rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.570558] DEBUG 872:new_mini_send:284: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.570560] ERROR: 872:new_mini_send:285: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.570562] ERROR: 872:dvk_ioctl:373: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.579565] DEBUG 724:new_get2rmt:528: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.579589] DEBUG 724:new_get2rmt:530: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.579590] ERROR: 724:new_get2rmt:532: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.579593] ERROR: 724:dvk_ioctl:373: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.588588] DEBUG 726:dvk_ioctl:349: cmd=4004E314 arg=BFF69914
Oct 21 12:30:54 node1 kernel: [ 4643.588592] DEBUG 726:dvk_ioctl:369: DVK_CALL=20 (io_put2lcl) 
Oct 21 12:30:54 node1 kernel: [ 4643.588594] DEBUG 726:io_put2lcl:217: 
Oct 21 12:30:54 node1 kernel: [ 4643.588599] DEBUG 726:check_caller:605: caller_pid=726 caller_tgid=726
Oct 21 12:30:54 node1 kernel: [ 4643.588601] DEBUG 726:check_caller:643: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588604] DEBUG 726:check_caller:648: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=726 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.588606] DEBUG 726:check_caller:708: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588608] DEBUG 726:check_caller:725: caller_pid=726 
Oct 21 12:30:54 node1 kernel: [ 4643.588609] DEBUG 726:check_caller:728: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588610] DEBUG 726:check_caller:732: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588612] DEBUG 726:new_put2lcl:574: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588614] DEBUG 726:new_put2lcl:600: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588616] DEBUG 726:new_put2lcl:601: cmd=0x2001 dcid=0 src=30 dst=1 snode=0 dnode=1 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.588617] DEBUG 726:new_put2lcl:607: dcid=0
Oct 21 12:30:54 node1 kernel: [ 4643.588618] DEBUG 726:new_put2lcl:623: WLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588621] DEBUG 726:new_put2lcl:655: TIMESTAMP sec=1603294254 nsec=383049572
Oct 21 12:30:54 node1 kernel: [ 4643.588622] DEBUG 726:new_put2lcl:674: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588624] DEBUG 726:new_put2lcl:674: WLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588626] DEBUG 726:new_put2lcl:738: REMOTE source OK endpoint=30
Oct 21 12:30:54 node1 kernel: [ 4643.588627] DEBUG 726:new_put2lcl:746: LOCAL destination OK endpoint=1
Oct 21 12:30:54 node1 kernel: [ 4643.588628] DEBUG 726:new_put2lcl:820: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588629] DEBUG 726:new_put2lcl:822: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588631] DEBUG 726:new_put2lcl:823: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588633] DEBUG 726:new_put2lcl:920: CMD_SEND_ACK dcid=0 rmt_ep=30 rmt_nr=30 lcl_ep=1 lcl_nr=1
Oct 21 12:30:54 node1 kernel: [ 4643.588635] DEBUG 726:send_ack_rmt2lcl:141: dcid=0 src_ep=30 dst_ep=1 rcode=0
Oct 21 12:30:54 node1 kernel: [ 4643.588636] ERROR: 726:send_ack_rmt2lcl:147: rcode=-312
Oct 21 12:30:54 node1 kernel: [ 4643.588637] DEBUG 726:new_put2lcl:957: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588639] DEBUG 726:new_put2lcl:957: WUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588640] DEBUG 726:new_put2lcl:973: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588641] DEBUG 726:new_put2lcl:975: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.588643] ERROR: 726:new_put2lcl:978: rcode=-312
Oct 21 12:30:54 node1 kernel: [ 4643.588644] ERROR: 726:dvk_ioctl:373: rcode=-312
Oct 21 12:30:54 node1 kernel: [ 4643.588784] DEBUG 872:dvk_ioctl:349: cmd=4004E30A arg=B63FF270
Oct 21 12:30:54 node1 kernel: [ 4643.588785] DEBUG 872:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 21 12:30:54 node1 kernel: [ 4643.588787] DEBUG 872:io_unbind:109: 
Oct 21 12:30:54 node1 kernel: [ 4643.588789] DEBUG 872:new_unbind:1857: dcid=0 proc_ep=1
Oct 21 12:30:54 node1 kernel: [ 4643.588791] DEBUG 872:check_caller:605: caller_pid=872 caller_tgid=869
Oct 21 12:30:54 node1 kernel: [ 4643.588792] DEBUG 872:check_caller:643: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588795] DEBUG 872:check_caller:648: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.588796] DEBUG 872:check_caller:708: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588797] DEBUG 872:check_caller:711: dcid=0
Oct 21 12:30:54 node1 kernel: [ 4643.588798] DEBUG 872:check_caller:715: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588799] DEBUG 872:check_caller:719: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588801] DEBUG 872:check_caller:725: caller_pid=872 
Oct 21 12:30:54 node1 kernel: [ 4643.588802] DEBUG 872:check_caller:728: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588803] DEBUG 872:check_caller:732: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588804] DEBUG 872:new_unbind:1873: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588806] DEBUG 872:new_unbind:1885: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588808] DEBUG 872:new_unbind:1887: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588809] DEBUG 872:new_unbind:1904: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588810] DEBUG 872:new_unbind:1907: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.588812] DEBUG 872:new_unbind:1912: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.589302] BUG: unable to handle kernel NULL pointer dereference at 00000618
Oct 21 12:30:54 node1 kernel: [ 4643.592995] IP: [<da5bc512>] mutex_lock+0x12/0x30
Oct 21 12:30:54 node1 kernel: [ 4643.596781] *pdpt = 00000000255aa001 *pde = 0000000000000000 
Oct 21 12:30:54 node1 kernel: [ 4643.596914] 
Oct 21 12:30:54 node1 kernel: [ 4643.597550] Oops: 0002 [#1] SMP
Oct 21 12:30:54 node1 kernel: [ 4643.598354] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4
Oct 21 12:30:54 node1 kernel: [ 4643.600693] DEBUG 724:dvk_ioctl:349: cmd=8004E315 arg=BFF69960
Oct 21 12:30:54 node1 kernel: [ 4643.600696] DEBUG 724:dvk_ioctl:369: DVK_CALL=21 (io_get2rmt) 
Oct 21 12:30:54 node1 kernel: [ 4643.600697] DEBUG 724:io_get2rmt:228: 
Oct 21 12:30:54 node1 kernel: [ 4643.600701] DEBUG 724:new_get2rmt:87: 
Oct 21 12:30:54 node1 kernel: [ 4643.601001] DEBUG 724:check_caller:605: caller_pid=724 caller_tgid=724
Oct 21 12:30:54 node1 kernel: [ 4643.601012] DEBUG 724:check_caller:643: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601016] DEBUG 724:check_caller:648: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=724 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.601023] DEBUG 724:check_caller:708: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601025] DEBUG 724:check_caller:725: caller_pid=724 
Oct 21 12:30:54 node1 kernel: [ 4643.601026] DEBUG 724:check_caller:728: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601027] DEBUG 724:check_caller:732: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601029] DEBUG 724:new_get2rmt:104: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601030] DEBUG 724:new_get2rmt:123: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601032] DEBUG 724:new_get2rmt:124: RLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.601033] DEBUG 724:new_get2rmt:129: RUNLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.601034] DEBUG 724:new_get2rmt:134: RLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601035] DEBUG 724:new_get2rmt:136: LIST_FOR_EACH_ENTRY_SAFE
Oct 21 12:30:54 node1 kernel: [ 4643.601036] DEBUG 724:new_get2rmt:140: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601037] DEBUG 724:new_get2rmt:141: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.601038] DEBUG 724:new_get2rmt:142: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.601040] DEBUG 724:new_get2rmt:146: Found a message. p_endpoint=1 c_cmd=0
Oct 21 12:30:54 node1 kernel: [ 4643.601041] DEBUG 724:new_get2rmt:148: LIST_DEL
Oct 21 12:30:54 node1 kernel: [ 4643.601043] ------------[ cut here ]------------
Oct 21 12:30:54 node1 kernel: [ 4643.601051] WARNING: CPU: 3 PID: 724 at lib/list_debug.c:53 __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.601053] list_del corruption, f61e46b4->next is LIST_POISON1 (00000100)
Oct 21 12:30:54 node1 kernel: [ 4643.601053] Modules linked in:
Oct 21 12:30:54 node1 kernel: [ 4643.601055]  dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4<4>[ 4643.602645] CPU: 3 PID: 724 Comm: lz4tcp_proxy_ba Tainted: G        W  O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.602647] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.602649]  e0b7fda4
Oct 21 12:30:54 node1 kernel: [ 4643.602650]  da2b1fa2 e0b7fdb8 00000000 da06778a da797c58 e0b7fdd8 000002d4 da79753c
Oct 21 12:30:54 node1 kernel: [ 4643.602655]  00000035 da2d0cc0 00000035 00000009 da2d0cc0 f61e46b4 e55a0b40 f61e4600
Oct 21 12:30:54 node1 kernel: [ 4643.602659]  e0b7fdc4 da0677f6 00000009 00000000 e0b7fdb8 da797c58 e0b7fdd8Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.602673]  [<da2b1fa2>] ? dump_stack+0x55/0x73
Oct 21 12:30:54 node1 kernel: [ 4643.602677]  [<da06778a>] ? __warn+0xea/0x110
Oct 21 12:30:54 node1 kernel: [ 4643.602680]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.602681]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.602684]  [<da0677f6>] ? warn_slowpath_fmt+0x46/0x60
Oct 21 12:30:54 node1 kernel: [ 4643.602686]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.602687]  [<da2d0d68>] ? list_del+0x8/0x20
Oct 21 12:30:54 node1 kernel: [ 4643.602699]  [<f8ab5c24>] ? new_get2rmt+0x9ad/0x4338 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.602702]  [<da0b4955>] ? down_trylock+0x25/0x30
Oct 21 12:30:54 node1 kernel: [ 4643.602706]  [<da1cf38d>] ? __check_object_size+0x9d/0x11c
Oct 21 12:30:54 node1 kernel: [ 4643.602711]  [<f8a969a7>] ? io_get2rmt+0x4e/0xce [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.602716]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.602720]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.602723]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4643.602725]  [<da1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4643.602727]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4643.602730]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4643.602733]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4643.602735] ---[ end trace 7b4305b5b9585094 ]---
Oct 21 12:30:54 node1 kernel: [ 4643.602739] DEBUG 724:new_get2rmt:159: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.602742] DEBUG 724:new_get2rmt:165: cmd=0x0 dcid=0 src=1 dst=30 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:30:54 node1 kernel: [ 4643.602744] DEBUG 724:new_get2rmt:168: source=0 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=  (null) m1p2=  (null) m1p3=  (null) 
Oct 21 12:30:54 node1 kernel: [ 4643.602746] DEBUG 724:new_get2rmt:172: n_nodeid=0 n_proxies=0 n_flags=E n_dcs=1 n_name=node0
Oct 21 12:30:54 node1 kernel: [ 4643.602748] DEBUG 724:new_get2rmt:181: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602749] DEBUG 724:new_get2rmt:183: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602751] DEBUG 724:new_get2rmt:185: RLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602752] DEBUG 724:new_get2rmt:194: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602753] DEBUG 724:new_get2rmt:197: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602755] DEBUG 724:new_get2rmt:204: RUNLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602756] DEBUG 724:new_get2rmt:206: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602757] DEBUG 724:new_get2rmt:243: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602758] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602760] DEBUG 724:new_get2rmt:244: WLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602761] DEBUG 724:new_get2rmt:258: WUNLOCK_PROC ep=30 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602763] DEBUG 724:new_get2rmt:264: TIMESTAMP sec=1603294254 nsec=383049572
Oct 21 12:30:54 node1 kernel: [ 4643.602766] DEBUG 724:new_get2rmt:425: LOCAL nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.602768] DEBUG 724:inherit_cpu:298: cpuid=3 vpid=13
Oct 21 12:30:54 node1 kernel: [ 4643.602772] ERROR: 724:inherit_cpu:302: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.602774] DEBUG 724:inherit_cpu:306: nr=1 endp=1 dcid=0 lpid=872 p_cpumask=FFFFFFFF nodemap=2 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4643.602775] DEBUG 724:new_get2rmt:524: BEFORE UP lpid=872 p_sem=0
Oct 21 12:30:54 node1 kernel: [ 4643.602777] DEBUG 724:new_get2rmt:525: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602778] DEBUG 724:new_get2rmt:528: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602779] DEBUG 724:new_get2rmt:530: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602780] ERROR: 724:new_get2rmt:532: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.602782] ERROR: 724:dvk_ioctl:373: rcode=-22
Oct 21 12:30:54 node1 kernel: [ 4643.602890] DEBUG 724:dvk_ioctl:349: cmd=4004E31C arg=BFF699A4
Oct 21 12:30:54 node1 kernel: [ 4643.602892] DEBUG 724:dvk_ioctl:369: DVK_CALL=28 (io_proxy_conn) 
Oct 21 12:30:54 node1 kernel: [ 4643.602893] DEBUG 724:io_proxy_conn:298: 
Oct 21 12:30:54 node1 kernel: [ 4643.602928] DEBUG 724:new_proxy_conn:3168: px_nr=0, status=3
Oct 21 12:30:54 node1 kernel: [ 4643.602930] DEBUG 724:lock_sr_proxies:787: px_nr=0
Oct 21 12:30:54 node1 kernel: [ 4643.602932] DEBUG 724:lock_sr_proxies:795: WLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602933] DEBUG 724:lock_sr_proxies:803: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602934] DEBUG 724:lock_sr_proxies:804: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602935] DEBUG 724:lock_sr_proxies:806: sproxy_pid=724, rproxy_pid=726
Oct 21 12:30:54 node1 kernel: [ 4643.602937] DEBUG 724:flush_sending_procs:470: SPROXY wakeup with error all process trying to send a CMD to node=1
Oct 21 12:30:54 node1 kernel: [ 4643.602938] DEBUG 724:flush_sending_procs:474: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602939] DEBUG 724:flush_sending_procs:475: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.602941] DEBUG 724:flush_sending_procs:481: WLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.602941] DEBUG 724:flush_sending_procs:483: LIST_DEL
Oct 21 12:30:54 node1 kernel: [ 4643.602943] ------------[ cut here ]------------
Oct 21 12:30:54 node1 kernel: [ 4643.602946] WARNING: CPU: 3 PID: 724 at lib/list_debug.c:53 __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.602948] list_del corruption, f61e46b4->next is LIST_POISON1 (00000100)
Oct 21 12:30:54 node1 kernel: [ 4643.602948] Modules linked in:
Oct 21 12:30:54 node1 kernel: [ 4643.602949]  dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx ttm drm_kms_helper joydev evdev drm vmw_balloon serio_raw pcspkr sg vmw_vmci shpchp ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4<4>[ 4643.602992] CPU: 3 PID: 724 Comm: lz4tcp_proxy_ba Tainted: G        W  O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.602993] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.602994]  e0b7fda0
Oct 21 12:30:54 node1 kernel: [ 4643.602995]  da2b1fa2 e0b7fdb4 00000000 da06778a da797c58 e0b7fdd4 000002d4 da79753c
Oct 21 12:30:54 node1 kernel: [ 4643.603001]  00000035 da2d0cc0 00000035 00000009 da2d0cc0 f61e46b4 f8ad9c20 0000004c
Oct 21 12:30:54 node1 kernel: [ 4643.603006]  e0b7fdc0 da0677f6 00000009 00000000 e0b7fdb4 da797c58 e0b7fdd4Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.603018]  [<da2b1fa2>] ? dump_stack+0x55/0x73
Oct 21 12:30:54 node1 kernel: [ 4643.603021]  [<da06778a>] ? __warn+0xea/0x110
Oct 21 12:30:54 node1 kernel: [ 4643.603023]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.603024]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.603027]  [<da0677f6>] ? warn_slowpath_fmt+0x46/0x60
Oct 21 12:30:54 node1 kernel: [ 4643.603029]  [<da2d0cc0>] ? __list_del_entry+0x60/0x100
Oct 21 12:30:54 node1 kernel: [ 4643.603031]  [<da2d0d68>] ? list_del+0x8/0x20
Oct 21 12:30:54 node1 kernel: [ 4643.603037]  [<f8aac1cc>] ? flush_sending_procs+0x210/0x575 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.603043]  [<f8aa930a>] ? new_proxy_conn+0x289/0x719 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.603045]  [<da1cf38d>] ? __check_object_size+0x9d/0x11c
Oct 21 12:30:54 node1 kernel: [ 4643.603050]  [<f8a96db2>] ? io_proxy_conn+0x47/0xc7 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.603055]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.603060]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.603061]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4643.603064]  [<da1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4643.603066]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4643.603068]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4643.603070]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4643.603071] ---[ end trace 7b4305b5b9585095 ]---
Oct 21 12:30:54 node1 kernel: [ 4643.603072] DEBUG 724:flush_sending_procs:487: Find process 1 trying to send a CMD
Oct 21 12:30:54 node1 kernel: [ 4643.603073] DEBUG 724:flush_sending_procs:493: Wakeup SENDER with error ep=1  pid=872
Oct 21 12:30:54 node1 kernel: [ 4643.603075] DEBUG 724:flush_sending_procs:515: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.603076] ERROR: 724:flush_sending_procs:516: rcode=-107
Oct 21 12:30:54 node1 kernel: [ 4643.603077] DEBUG 724:new_proxy_conn:3193: WLOCK_NODE node=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.603078] DEBUG 724:new_proxy_conn:3195: WUNLOCK_NODE node=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.603080] DEBUG 724:unlock_sr_proxies:750: px_nr=0
Oct 21 12:30:54 node1 kernel: [ 4643.603081] DEBUG 724:unlock_sr_proxies:766: sproxy_pid=724, rproxy_pid=726
Oct 21 12:30:54 node1 kernel: [ 4643.603082] DEBUG 724:unlock_sr_proxies:768: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.603083] DEBUG 724:unlock_sr_proxies:769: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:30:54 node1 kernel: [ 4643.603084] DEBUG 724:unlock_sr_proxies:770: WUNLOCK_PROXY pxid=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.603087] DEBUG 724:new_proxy_conn:3248: nr=0 endp=27342 dcid=-1 flags=0 misc=1 lpid=724 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.603089] DEBUG 724:new_proxy_conn:3249: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=726 vpid=-1 nodeid=1 name=lz4tcp_proxy_ba 
Oct 21 12:30:54 node1 kernel: [ 4643.649023] do_exit: local_nodeid:1
Oct 21 12:30:54 node1 kernel: [ 4643.649093] DEBUG 892:new_exit_unbind:267: code=-629729834
Oct 21 12:30:54 node1 kernel: [ 4643.649096] DEBUG 892:new_exit_unbind:271: WLOCK_TASK pid=216 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.649098] DEBUG 892:new_exit_unbind:400: Process not bound pid=892 systemd-journal
Oct 21 12:30:54 node1 kernel: [ 4643.649099] DEBUG 892:new_exit_unbind:402: WUNLOCK_TASK pid=216 count=0
Oct 21 12:30:54 node1 kernel: [ 4643.956084]  overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse ata_piix xhci_pci mptspi ehci_pci uhci_hcd xhci_hcd ehci_hcd scsi_transport_spi usbcore libata pcnet32 mptscsih mii mptbase scsi_mod i2c_piix4
Oct 21 12:30:54 node1 kernel: [ 4643.962663] CPU: 2 PID: 872 Comm: dvs_uml_switch Tainted: G        W  O    4.9.88-DVK #158
Oct 21 12:30:54 node1 kernel: [ 4643.964536] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:30:54 node1 kernel: [ 4643.968349] task: eaf38100 task.stack: f69b4000
Oct 21 12:30:54 node1 kernel: [ 4643.970379] EIP: 0060:[<da5bc512>] EFLAGS: 00010286 CPU: 2
Oct 21 12:30:54 node1 kernel: [ 4643.972844] EIP is at mutex_lock+0x12/0x30
Oct 21 12:30:54 node1 kernel: [ 4643.975168] EAX: 00000618 EBX: 00000618 ECX: f70d68c0 EDX: 80000000
Oct 21 12:30:54 node1 kernel: [ 4643.977220] ESI: 00000000 EDI: 00000000 EBP: f69b5df8 ESP: f69b5df4
Oct 21 12:30:54 node1 kernel: [ 4643.980351]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
Oct 21 12:30:54 node1 kernel: [ 4643.982407] CR0: 80050033 CR2: 00000618 CR3: 1830f2c0 CR4: 000006f0
Oct 21 12:30:54 node1 kernel: [ 4643.984528] Stack:
Oct 21 12:30:54 node1 kernel: [ 4643.987339]  f61e4600 f69b5e8c f8aa1ded f8ad3364 00000368 f8ad04cc 00000778 00000001
Oct 21 12:30:54 node1 kernel: [ 4643.989518]  00000000 da99ad02 00029b64 da99ad00 00000019 00000002 f69b5e64 da0ba4fe
Oct 21 12:30:54 node1 kernel: [ 4643.991960]  eaf38100 00000040 00000368 01010000 f8ae2d00 00000001 00000000 f8ae2d40
Oct 21 12:30:54 node1 kernel: [ 4643.994929] Call Trace:
Oct 21 12:30:54 node1 kernel: [ 4643.997110]  [<f8aa1ded>] ? new_unbind+0xaca/0x1577 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4643.999278]  [<da0ba4fe>] ? vprintk_emit+0x2ee/0x4f0
Oct 21 12:30:54 node1 kernel: [ 4644.001431]  [<f8a96148>] ? io_unbind+0x4e/0xce [dvk]
Oct 21 12:30:54 node1 kernel: [ 4644.004308]  [<f8a951fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4644.006461]  [<f8a9516d>] ? dvk_write+0x49/0x49 [dvk]
Oct 21 12:30:54 node1 kernel: [ 4644.008617]  [<da1e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 21 12:30:54 node1 kernel: [ 4644.010817]  [<da1d3159>] ? vfs_write+0x149/0x1c0
Oct 21 12:30:54 node1 kernel: [ 4644.012936]  [<da1e6d20>] ? SyS_ioctl+0x60/0x70
Oct 21 12:30:54 node1 kernel: [ 4644.015070]  [<da003708>] ? do_fast_syscall_32+0x98/0x160
Oct 21 12:30:54 node1 kernel: [ 4644.017224]  [<da5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:30:54 node1 kernel: [ 4644.019618] Code: c3 90 8d b4 26 00 00 00 00 31 c0 5d c3 8d b6 00 00 00 00 8d bf 00 00 00 00 55 89 e5 53 0f 1f 44 00 00 89 c3 e8 a0 ef ff ff 89 d8 <f0> ff 08 79 05 e8 74 07 00 00 64 a1 48 12 97 da 89 43 10 5b 5d
Oct 21 12:30:54 node1 kernel: [ 4644.027504] EIP: [<da5bc512>] 
Oct 21 12:30:54 node1 kernel: [ 4644.027549] mutex_lock+0x12/0x30
Oct 21 12:30:54 node1 kernel: [ 4644.029689]  SS:ESP 0068:f69b5df4
Oct 21 12:30:54 node1 kernel: [ 4644.031836] CR2: 0000000000000618
Oct 21 12:30:54 node1 kernel: [ 4644.054696] ---[ end trace 7b4305b5b9585096 ]---
Oct 21 12:30:54 node1 kernel: [ 4644.056914] do_exit: local_nodeid:1
Oct 21 12:30:54 node1 kernel: [ 4644.059562] DEBUG 872:new_exit_unbind:267: code=-629729834
Oct 21 12:30:54 node1 kernel: [ 4644.064856] DEBUG 872:new_exit_unbind:271: WLOCK_TASK pid=869 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.067415] DEBUG 872:new_exit_unbind:277: RLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.069486] DEBUG 872:new_exit_unbind:278: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4644.073622] DEBUG 872:new_exit_unbind:311:  Exiting endpoint=1 lpid=872
Oct 21 12:30:54 node1 kernel: [ 4644.076357] DEBUG 872:new_exit_unbind:313: RUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.080145] DEBUG 872:new_exit_unbind:314: WLOCK_DC dc=0 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.083007] DEBUG 872:new_exit_unbind:315: WLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.084942] DEBUG 872:new_exit_unbind:322:  endpoint=1 lpid=872
Oct 21 12:30:54 node1 kernel: [ 4644.087740] DEBUG 872:do_unbind:736: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4644.093145] DEBUG 872:do_unbind:765: Caller nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4644.100604] DEBUG 872:do_unbind:829: wakeup with error those processes trying to send a message to the proc
Oct 21 12:30:54 node1 kernel: [ 4644.104901] DEBUG 872:do_unbind:858: delete notify messages bits sent by the proc
Oct 21 12:30:54 node1 kernel: [ 4644.107482] DEBUG 872:do_unbind:865: Skip, caller process
Oct 21 12:30:54 node1 kernel: [ 4644.111602] DEBUG 872:do_unbind:920: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4644.118828] DEBUG 872:do_unbind:976: wakeup with error those processes waiting this process MIGRATION
Oct 21 12:30:54 node1 kernel: [ 4644.123475] DEBUG 872:do_unbind:1026: wakeup those processes waiting this process UNBINDING
Oct 21 12:30:54 node1 kernel: [ 4644.140358] DEBUG 872:do_unbind:1060: nr=1 endp=1 dcid=0 flags=0 misc=0 lpid=872 vpid=13 nodeid=1 name=dvs_uml_switch 
Oct 21 12:30:54 node1 kernel: [ 4644.148612] DEBUG 872:init_proc_desc:16: p_name=dvs_uml_switch dcid=0
Oct 21 12:30:54 node1 kernel: [ 4644.151212] DEBUG 872:init_proc_desc:27: Clearing Privileges
Oct 21 12:30:54 node1 kernel: [ 4644.154257] DEBUG 872:init_proc_desc:35: Setting Default DVK calls privileges
Oct 21 12:30:54 node1 kernel: [ 4644.160038] DEBUG 872:init_proc_desc:41: Clearing Process fields
Oct 21 12:30:54 node1 kernel: [ 4644.164858] DEBUG 872:do_unbind:1062: initialized 
Oct 21 12:30:54 node1 kernel: [ 4644.167362] DEBUG 872:do_unbind:1063: DC_DECREF counter=6
Oct 21 12:30:54 node1 kernel: [ 4644.169728] DEBUG 872:new_exit_unbind:384: WUNLOCK_PROC ep=1 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.172125] DEBUG 872:new_exit_unbind:385: WUNLOCK_TASK pid=869 count=0
Oct 21 12:30:54 node1 kernel: [ 4644.174392] DEBUG 872:new_exit_unbind:386: WUNLOCK_DC dc=0 count=0
Oct 21 12:32:30 node1 kernel: [ 4739.895532] do_exit: local_nodeid:1
Oct 21 12:32:30 node1 kernel: [ 4739.898498] DEBUG 793:new_exit_unbind:267: code=-629729834
Oct 21 12:32:30 node1 kernel: [ 4739.901183] DEBUG 793:new_exit_unbind:271: WLOCK_TASK pid=793 count=0
Oct 21 12:32:30 node1 kernel: [ 4739.903704] DEBUG 793:new_exit_unbind:400: Process not bound pid=793 kworker/dying
Oct 21 12:32:30 node1 kernel: [ 4739.907911] DEBUG 793:new_exit_unbind:402: WUNLOCK_TASK pid=793 count=0
Oct 21 12:33:15 node1 kernel: [ 4784.254688] do_exit: local_nodeid:1
Oct 21 12:33:15 node1 kernel: [ 4784.257683] DEBUG 894:new_exit_unbind:267: code=-629729834
Oct 21 12:33:15 node1 kernel: [ 4784.260133] DEBUG 894:new_exit_unbind:271: WLOCK_TASK pid=894 count=0
Oct 21 12:33:15 node1 kernel: [ 4784.262576] DEBUG 894:new_exit_unbind:400: Process not bound pid=894 ls
Oct 21 12:33:15 node1 kernel: [ 4784.265319] DEBUG 894:new_exit_unbind:402: WUNLOCK_TASK pid=894 count=0


ESTE ES OTRO ERROR
Oct 21 12:57:32 node0 kernel: [  607.084515] DEBUG 804:do_unbind:736: nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 21 12:57:32 node0 kernel: [  607.084517] DEBUG 804:do_unbind:765: Caller nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 21 12:57:32 node0 kernel: [  607.084519] DEBUG 804:do_unbind:829: wakeup with error those processes trying to send a message to the proc
Oct 21 12:57:32 node0 kernel: [  607.084520] DEBUG 804:do_unbind:858: delete notify messages bits sent by the proc
Oct 21 12:57:32 node0 kernel: [  607.084524] DEBUG 804:do_unbind:865: Skip, caller process
Oct 21 12:57:32 node0 kernel: [  607.084528] DEBUG 804:do_unbind:905: Process 31 is no more waiting a message from the unbinded process 0
Oct 21 12:57:32 node0 kernel: [  607.084529] DEBUG 804:generic_ack_lcl2rmt:20: ack=8193 rcode=-108
Oct 21 12:57:32 node0 kernel: [  607.084532] DEBUG 804:sproxy_enqueue:28: nr=31 endp=31 dcid=0 flags=3000 misc=2000 lpid=-1 vpid=-1 nodeid=1 name=TAPclient01 
Oct 21 12:57:32 node0 kernel: [  607.084535] DEBUG 804:sproxy_enqueue:30: cmd=0x2001 dcid=0 src=0 dst=31 snode=0 dnode=1 rcode=-108 len=0
Oct 21 12:57:32 node0 kernel: [  607.084536] DEBUG 804:sproxy_enqueue:33: WLOCK_PROC ep=27342 count=1
Oct 21 12:57:32 node0 kernel: [  607.084537] DEBUG 804:sproxy_enqueue:40: LIST_ADD_TAIL
Oct 21 12:57:32 node0 kernel: [  607.084540] DEBUG 804:sproxy_enqueue:42: nr=1 endp=27342 dcid=-1 flags=0 misc=1 lpid=718 vpid=-1 nodeid=0 name=lz4tcp_proxy_ba 
Oct 21 12:57:32 node0 kernel: [  607.084541] DEBUG 804:sproxy_enqueue:49: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:57:32 node0 kernel: [  607.084645] DEBUG 804:do_unbind:920: nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 21 12:57:32 node0 kernel: [  607.084647] DEBUG 804:do_unbind:976: wakeup with error those processes waiting this process MIGRATION
Oct 21 12:57:32 node0 kernel: [  607.084650] DEBUG 804:do_unbind:1010: waitunbind nr=0 endp=0 dcid=0 flags=0 misc=0 lpid=0 vpid=0 nodeid=0 name= 
Oct 21 12:57:32 node0 kernel: [  607.084651] DEBUG 804:do_unbind:1015: WUNLOCK_PROC ep=0 count=0
Oct 21 12:57:32 node0 kernel: [  607.092210] BUG: unable to handle kernel NULL pointer dereference at   (null)
Oct 21 12:57:32 node0 kernel: [  607.092663] IP: [<de2d0b4b>] __list_add+0xb/0x120
Oct 21 12:57:32 node0 kernel: [  607.093082] *pdpt = 000000002a791001 *pde = 0000000000000000 
Oct 21 12:57:32 node0 kernel: [  607.093149] 
Oct 21 12:57:32 node0 kernel: [  607.093568] Oops: 0000 [#1] SMP
Oct 21 12:57:32 node0 kernel: [  607.094096] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse joydev serio_raw vmw_balloon pcspkr vmwgfx evdev ttm drm_kms_helper vmw_vmci sg shpchp drm ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache sr_mod cdrom sd_mod ata_generic hid_generic usbhid hid psmouse ata_piix uhci_hcd xhci_pci ehci_pci ehci_hcd xhci_hcd libata pcnet32 mii usbcore mptspi scsi_transport_spi mptscsih mptbase i2c_piix4 scsi_mod
Oct 21 12:57:32 node0 kernel: [  607.100790] CPU: 0 PID: 804 Comm: dvs_uml_switch Tainted: G           O    4.9.88-DVK #157
Oct 21 12:57:32 node0 kernel: [  607.101914] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 21 12:57:32 node0 kernel: [  607.104188] task: df3f7340 task.stack: d4942000
Oct 21 12:57:32 node0 kernel: [  607.105372] EIP: 0060:[<de2d0b4b>] EFLAGS: 00210246 CPU: 0
Oct 21 12:57:32 node0 kernel: [  607.106712] EIP is at __list_add+0xb/0x120
Oct 21 12:57:32 node0 kernel: [  607.108061] EAX: d4943d08 EBX: 00000000 ECX: ccf3f9e0 EDX: 00000000
Oct 21 12:57:32 node0 kernel: [  607.109517] ESI: ccf3f9dc EDI: df3f7340 EBP: d4943d24 ESP: d4943cd8
Oct 21 12:57:32 node0 kernel: [  607.110948]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
Oct 21 12:57:32 node0 kernel: [  607.112377] CR0: 80050033 CR2: 00000000 CR3: 1f328940 CR4: 000006f0
Oct 21 12:57:32 node0 kernel: [  607.113918] Stack:
Oct 21 12:57:32 node0 kernel: [  607.115671]  df3f7340 ccf3f9ec df3f7340 00000000 ccf3f9e8 00000000 ccf3f9d8 ccf445d8
Oct 21 12:57:32 node0 kernel: [  607.117773]  ccf3f9d8 ccf3f9d8 de5bcd08 ccf3f9e0 d4943d28 de0ba8e7 00000000 66dd1316
Oct 21 12:57:32 node0 kernel: [  607.119512]  ccf3f9d8 ccf445d8 ccf44400 d4943d30 de5bc51c ccf3f800 d4943d8c f8850b65
Oct 21 12:57:32 node0 kernel: [  607.121350] Call Trace:
Oct 21 12:57:32 node0 kernel: [  607.123491]  [<de5bcd08>] ? __mutex_lock_slowpath+0x78/0x100
Oct 21 12:57:32 node0 kernel: [  607.125416]  [<de0ba8e7>] ? vprintk_default+0x37/0x40
Oct 21 12:57:32 node0 kernel: [  607.127316]  [<de5bc51c>] ? mutex_lock+0x1c/0x30
Oct 21 12:57:32 node0 kernel: [  607.129283]  [<f8850b65>] ? do_unbind+0x1a13/0x2234 [dvk]
Oct 21 12:57:32 node0 kernel: [  607.131274]  [<f8853743>] ? new_exit_unbind+0x290/0x11c6 [dvk]
Oct 21 12:57:32 node0 kernel: [  607.133974]  [<de06c342>] ? do_exit+0xa42/0xa50
Oct 21 12:57:32 node0 kernel: [  607.136047]  [<de0b9f5d>] ? console_unlock+0x3fd/0x6b0
Oct 21 12:57:32 node0 kernel: [  607.138104]  [<de074197>] ? recalc_sigpending+0x17/0x50
Oct 21 12:57:32 node0 kernel: [  607.140248]  [<de074777>] ? dequeue_signal+0x87/0x1a0
Oct 21 12:57:32 node0 kernel: [  607.142633]  [<de06c3c7>] ? do_group_exit+0x37/0x90
Oct 21 12:57:32 node0 kernel: [  607.144782]  [<de0770df>] ? get_signal+0x24f/0x600
Oct 21 12:57:32 node0 kernel: [  607.147018]  [<de0ba4fe>] ? vprintk_emit+0x2ee/0x4f0
Oct 21 12:57:32 node0 kernel: [  607.149216]  [<de0224d9>] ? do_signal+0x29/0x520
Oct 21 12:57:32 node0 kernel: [  607.151597]  [<de160097>] ? printk+0x1a/0x1c
Oct 21 12:57:32 node0 kernel: [  607.153786]  [<f884a2f0>] ? dvk_ioctl+0x183/0x1a8 [dvk]
Oct 21 12:57:32 node0 kernel: [  607.156504]  [<de1d316d>] ? vfs_write+0x15d/0x1c0
Oct 21 12:57:32 node0 kernel: [  607.158648]  [<de0032b8>] ? exit_to_usermode_loop+0x78/0xb0
Oct 21 12:57:32 node0 kernel: [  607.160707]  [<de0037af>] ? do_fast_syscall_32+0x13f/0x160
Oct 21 12:57:32 node0 kernel: [  607.162696]  [<de5bf682>] ? sysenter_past_esp+0x47/0x75
Oct 21 12:57:32 node0 kernel: [  607.164681] Code: 00 89 ea c7 41 04 00 00 00 00 e9 ce fe ff ff 89 ea 83 cf 01 e9 c4 fe ff ff e8 82 69 d9 ff 66 90 53 83 ec 24 8b 59 04 39 d3 75 25 <8b> 1a 39 d9 75 6f 39 c2 0f 84 af 00 00 00 39 c1 0f 84 a7 00 00
Oct 21 12:57:32 node0 kernel: [  607.170900] EIP: [<de2d0b4b>] 
Oct 21 12:57:32 node0 kernel: [  607.170941] __list_add+0xb/0x120
Oct 21 12:57:32 node0 kernel: [  607.172832]  SS:ESP 0068:d4943cd8
Oct 21 12:57:32 node0 kernel: [  607.174703] CR2: 0000000000000000
Oct 21 12:57:32 node0 kernel: [  607.176985] ---[ end trace e21fa2b88254f622 ]---
Oct 21 12:57:32 node0 kernel: [  607.178933] do_exit: local_nodeid:0
Oct 21 12:57:32 node0 kernel: [  607.180746] DEBUG 804:new_exit_unbind:267: code=-562620970
Oct 21 12:57:32 node0 kernel: [  607.194390] do_exit: local_nodeid:0
Oct 21 12:57:32 node0 kernel: [  607.197147] DEBUG 806:new_exit_unbind:267: code=-562620970
Oct 21 12:57:32 node0 kernel: [  607.204305] DEBUG 806:new_exit_unbind:271: WLOCK_TASK pid=804 count=0
Oct 21 12:57:32 node0 kernel: [  607.207671] DEBUG 806:new_exit_unbind:277: RLOCK_PROC ep=30 count=0
Oct 21 12:57:32 node0 kernel: [  607.210631] DEBUG 806:new_exit_unbind:278: nr=30 endp=30 dcid=0 flags=0 misc=0 lpid=806 vpid=4 nodeid=0 name=dvs_uml_switch 
Oct 21 12:57:32 node0 kernel: [  607.217556] DEBUG 806:new_exit_unbind:311:  Exiting endpoint=30 lpid=806
Oct 21 12:57:32 node0 kernel: [  607.220994] DEBUG 806:new_exit_unbind:313: RUNLOCK_PROC ep=30 count=0
Oct 21 12:57:32 node0 kernel: [  607.225132] DEBUG 804:new_exit_unbind:271: WLOCK_TASK pid=804 count=0
Oct 21 12:57:32 node0 kernel: [  607.227361] DEBUG 804:new_exit_unbind:277: RLOCK_PROC ep=0 count=0
Oct 21 12:57:32 node0 kernel: [  607.229193] DEBUG 804:new_exit_unbind:278: nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 21 12:57:32 node0 kernel: [  607.234761] DEBUG 804:new_exit_unbind:311:  Exiting endpoint=0 lpid=807
Oct 21 12:57:32 node0 kernel: [  607.236974] DEBUG 804:new_exit_unbind:313: RUNLOCK_PROC ep=0 count=0
Oct 21 12:57:40 node0 kernel: [  615.685444] DEBUG 719:dvk_ioctl:349: cmd=4004E314 arg=BFC86C34
Oct 21 12:57:40 node0 kernel: [  615.688779] DEBUG 719:dvk_ioctl:369: DVK_CALL=20 (io_put2lcl) 
Oct 21 12:57:40 node0 kernel: [  615.692074] DEBUG 719:io_put2lcl:217: 
Oct 21 12:57:40 node0 kernel: [  615.695228] DEBUG 719:check_caller:605: caller_pid=719 caller_tgid=719
Oct 21 12:57:40 node0 kernel: [  615.699688] DEBUG 719:check_caller:643: WLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.703242] DEBUG 719:check_caller:648: nr=1 endp=27342 dcid=-1 flags=0 misc=3 lpid=719 vpid=-1 nodeid=0 name=lz4tcp_proxy_ba 
Oct 21 12:57:40 node0 kernel: [  615.709730] DEBUG 719:check_caller:708: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.713433] DEBUG 719:check_caller:725: caller_pid=719 
Oct 21 12:57:40 node0 kernel: [  615.717011] DEBUG 719:check_caller:728: RLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.719207] DEBUG 719:check_caller:732: RUNLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.723639] DEBUG 719:new_put2lcl:574: WLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.725701] DEBUG 719:new_put2lcl:600: WUNLOCK_PROC ep=27342 count=1
Oct 21 12:57:40 node0 kernel: [  615.727803] DEBUG 719:new_put2lcl:601: cmd=0x3 dcid=0 src=31 dst=0 snode=1 dnode=0 rcode=0 len=0
Oct 21 12:57:40 node0 kernel: [  615.729900] DEBUG 719:new_put2lcl:607: dcid=0
Oct 2
	
		
TODO:	Recompilar el modulo 
		verificar que se registran los threads como NO GRPLEADER
		Verificar que termina bien con CTRL-C

TODO:   Supongamos un proceso PROC0 en NODE0 y PROC1  en NODE1 	
		PROC1 hace un dvk_receive(proc0_endpoint, msg);
		y PROC0 muere en otro nodo.-
		NO Hay forma de que en NODE1 se entere de que PROC0 murio 
		y entonces PROC1 quedaria esperando eternamente!!!
		
		Porque, cuando se hace el do_unbind()  
		si el proceso que muere es LOCAL 

		/* it another LOCAL process is waiting to receive 			*/
		/* a message from the unbound process, inform that it exits.*/ 
		if( (!test_bit(BIT_SENDING, &rp->p_usr.p_rts_flags) && test_bit(BIT_RECEIVING, &rp->p_usr.p_rts_flags) ) 
			&& (rp->p_usr.p_getfrom == proc_ptr->p_usr.p_endpoint)) {
			DVKDEBUG(INTERNAL,"Process %d is no more waiting a message from the unbound process %d\n",
				rp->p_usr.p_endpoint, proc_ptr->p_usr.p_endpoint);
			if( IT_IS_LOCAL(rp)) {
				DVKDEBUG(INTERNAL,"Wakeup RECEIVER with error ep=%d  pid=%d\n",
					rp->p_usr.p_endpoint, rp->p_usr.p_lpid);
				LOCAL_PROC_UP(rp, EDVSSRCDIED);
			}else{		<<<<<<<<<<<<<<<<<<<<<<<< ESTA PARTE SOLO EJECUTARIA SI UN PROCESO REMOTO HIZO UN SENDREC Y QUEDO EN RECEIVING
				clear_bit(BIT_RECEIVING, &rp->p_usr.p_rts_flags);
//	reemplazado generic_ack_lcl2rmt(CMD_SNDREC_ACK, rp, proc_ptr, EDVSSRCDIED);
				send_ack_lcl2rmt(rp, proc_ptr, EDVSSRCDIED);
			}
		}

		EL UNBIND LO HACE EL PROCESO CON  endp=0  (LOCAL) 
		
DEBUG 804:do_unbind:736: nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
DEBUG 804:do_unbind:765: Caller nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
DEBUG 804:do_unbind:829: wakeup with error those processes trying to send a message to the proc
DEBUG 804:do_unbind:858: delete notify messages bits sent by the proc
DEBUG 804:do_unbind:865: Skip, caller process
		EL PROCESO 31 QUE ES REMOTO ESTA EN ESTADO RECEIVING PORQUE HIZO UN SENDREC y el PROCESO 0 MUERE SIN RESPONDER  
DEBUG 804:do_unbind:905: Process 31 is no more waiting a message from the unbound process 0
DEBUG 804:generic_ack_lcl2rmt:20: ack=8193 rcode=-108
DEBUG 804:sproxy_enqueue:28: nr=31 endp=31 dcid=0 flags=3000 misc=2000 lpid=-1 vpid=-1 nodeid=1 name=TAPclient01 
DEBUG 804:sproxy_enqueue:30: cmd=0x2001 dcid=0 src=0 dst=31 snode=0 dnode=1 rcode=-108 len=0
DEBUG 804:sproxy_enqueue:33: WLOCK_PROC ep=27342 count=1
DEBUG 804:sproxy_enqueue:40: LIST_ADD_TAIL
DEBUG 804:sproxy_enqueue:42: nr=1 endp=27342 dcid=-1 flags=0 misc=1 lpid=718 vpid=-1 nodeid=0 name=lz4tcp_proxy_ba 
DEBUG 804:sproxy_enqueue:49: WUNLOCK_PROC ep=27342 count=1
DEBUG 804:do_unbind:920: nr=0 endp=0 dcid=0 flags=20000 misc=1020 lpid=807 vpid=5 nodeid=0 name=dvs_uml_switch 
DEBUG 804:do_unbind:976: wakeup with error those processes waiting this process MIGRATION
DEBUG 804:do_unbind:1010: waitunbind nr=0 endp=0 dcid=0 flags=0 misc=0 lpid=0 vpid=0 nodeid=0 name= 
DEBUG 804:do_unbind:1015: WUNLOCK_PROC ep=0 count=0		

El proceso con ep=0 en NODE0 hace el unbind() y ve que el proceso ep=31 de NODE1 tiene el bit RECEIVING 
indicando que hizo un dvk_sendrec() y tiene que encolar su descriptor en el SPROXY para notificar el UNBIND.


????????????????????????????????????????????????????????????????????????????????????
ERROR GROSSO ????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????????????

Cuando un proceso LOCAL en NODE0 quiere enviarle un ACK a un proceso REMOTO en NODE1 
encola en el SENDER PROXY el descriptor del proceso REMOTO.

Puede darse el caso que se encole otra vez el mismo descriptor por otra razon ???
Oct 21 12:30:54 node1 kernel: [ 4643.547950] WARNING: CPU: 2 PID: 872 at lib/list_debug.c:36 __list_add+0xfc/0x120
Oct 21 12:30:54 node1 kernel: [ 4643.547952] list_add double add: new=f61e46b4, prev=f61e46b4, next=f8ad9ccc.

Cual podria ser la razon de un doble encolamiento?
Para que se usan los remote process descriptors?
1) los usa el proxy receiver para enconlarlo en la cola de mensajes del receptor.
2) los usa el proxy receiver para enconlarlo en la cola de mensajes del proxy sender por un ACK de error 
3) Cuando un proceso local hace un do_unbind() y tiene encolado un proceso remoto, lo desencola y 
lo encola con el proxy sender para que le avise del error
4) Cuando el proceso remoto hizo un sendrec sobre el proceso local quien ya hizo el receive y lo desencolo, el proceso remoto quedo todavia con
	el bit RECEIVING, y si el proceso local hace un do_unbind() encola el descriptor remoto en el PROXY SENDER para enviar un ACK de ERROR.
	
EL errore esta en estos 2 ultimos casos 

		/////////////////// CONDICION EN EL QUE EL PROCESO REMOTO HIZO UN SENDREC PERO NO FUE RECIBIDO POR EL DESTINATARIO QUE HACE UNBIND 
		DVKDEBUG(INTERNAL,"Find process %d trying to send a message to %d\n",
			src_ptr->p_usr.p_endpoint, proc_ptr->p_usr.p_endpoint);
//		assert(test_bit(BIT_SENDING, &src_ptr->p_usr.p_rts_flags));
		clear_bit(BIT_SENDING, &src_ptr->p_usr.p_rts_flags);
		
		if( IT_IS_LOCAL(src_ptr)) {
			DVKDEBUG(INTERNAL,"Wakeup SENDER with error ep=%d  pid=%d\n",
				src_ptr->p_usr.p_endpoint, src_ptr->p_usr.p_lpid);	
			LOCAL_PROC_UP(src_ptr, EDVSDSTDIED);
		} else {
			// check if it is a SENDREC 
			if( test_bit(BIT_RECEIVING, &src_ptr->p_usr.p_rts_flags) ) {
				clear_bit(BIT_RECEIVING, &src_ptr->p_usr.p_rts_flags);
				clear_bit(MIS_BIT_ATOMIC, &src_ptr->p_usr.p_misc_flags);
			}
			/* REMOTE process descriptor are used for ACKNOWLEDGES 							 */
			send_ack_lcl2rmt(src_ptr,proc_ptr,EDVSDSTDIED);
		}


		/////////////////// CONDICION EN EL QUE EL PROCESO REMOTO HIZO UN SENDREC PERO YA RUE RECIBIDO PERO NO 
		/////////////////// RESPONDIDO AUN POR EL DESTINATARIO QUE HACE UNBIND 
		/* IF another LOCAL process is waiting to receive  		*/
		/* a message from the unbound process,  because it has 	*/
		/* done a dvk_sendrec(), inform that it exits.			*/ 
		if( (!test_bit(BIT_SENDING, &rp->p_usr.p_rts_flags) && test_bit(BIT_RECEIVING, &rp->p_usr.p_rts_flags) ) 
			&& (rp->p_usr.p_getfrom == proc_ptr->p_usr.p_endpoint)) {
			DVKDEBUG(INTERNAL,"Process %d is no more waiting a message from the unbound process %d\n",
				rp->p_usr.p_endpoint, proc_ptr->p_usr.p_endpoint);
			// There must be a received but unreplied sendrec()  by the unbouning process 
			clear_bit(BIT_RECEIVING, &rp->p_usr.p_rts_flags);  <<<<<<<<<<<<<<< SE AGREGO  
			clear_bit(MIS_BIT_ATOMIC, &rp->p_usr.p_misc_flags); <<<<<<<<<<<<<<< SE AGREGO  
			if( IT_IS_LOCAL(rp)) {
				DVKDEBUG(INTERNAL,"Wakeup RECEIVER with error ep=%d  pid=%d\n",
					rp->p_usr.p_endpoint, rp->p_usr.p_lpid);
				LOCAL_PROC_UP(rp, EDVSSRCDIED);
			}else{	
				send_ack_lcl2rmt(rp, proc_ptr, EDVSSRCDIED);
			}
		}

=============================================================================

		CLIENT	RPROXY	<------send------------ SPROXY		SERVER			
				
	SI el server hace un SEND al CLIENT, y el CLIENT no esta en RECEIVING entonces 
	encola el mensaje en la cola del CLIENT, pero, por otro lado hay que enviar 
	un ACK al SERVER y alli habria un doble encolamiento!!

	SOLUCION: Ver cuales son los flags que indican encolamiento de los descriptores REMOTE.
	Esto se debe analizar en el RPROXY antes del ACK 
		BIT_SENDING
		BIT_WAITMIGR
		BIT_WAITUNBIND
		BIT_WAITBIND
	DONDE 
		p_list  lista de procesos en estado SENDING 
		p_mlist lista de procesos esperando una MIGRACION 
		p_ulist lista de procesos esperando un BIND o UNBIND 

??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????????????			
	
	
==============================================================================================================

	
ATENCION, EN WAIT4BIND SE USA TAMBIEN ERRORES QUE PUEDEN SER ENDPOINTS!!	
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!111
REVISAR LOS wait_event_interruptible  y wait_event_interruptible_timeout porque retornan valores diferentes!!!

		// The function will return -ERESTARTSYS if it was interrupted by a signal and 0 if condition evaluated to true.
		ret = wait_event_interruptible(current->task_wqh, (current->task_proc != NULL));
	} else {
		// 0 if the condition evaluated to false after the timeout elapsed, 
		// 1 if the condition evaluated to true after the timeout elapsed, 
		// the remaining jiffies (at least 1) if the condition evaluated to true 
		// before the timeout elapsed, or -ERESTARTSYS if it was interrupted by a signal.
			ret = wait_event_interruptible_timeout(current->task_wqh, 
					(current->task_proc != NULL), 
					msecs_to_jiffies(timeout_ms));
		}
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!




==============================================================================================================
20201023:

EL QUE EJECUTA EN NODE1 ES EL MAIN DEL UML SWITCH 836 PERO EL QUE ESTA BINDEADO ES UN THREAD
RECORDAR QUE EL MAIN NO ESTA BINDEADO PERO SUS THREADS SI, Y POR LO TANTO ESTAN CORRIENDO
COMO SE PODRIA SOLUCIONAR?

Oct 23 18:01:54 node1 kernel: [19233.232466] DEBUG 836:dvk_ioctl:349: cmd=4004E30A arg=BFEE2FB0
Oct 23 18:01:54 node1 kernel: [19233.232470] DEBUG 836:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 23 18:01:54 node1 kernel: [19233.232472] DEBUG 836:io_unbind:109: 
Oct 23 18:01:54 node1 kernel: [19233.232474] DEBUG 836:new_unbind:1873: dcid=0 proc_ep=0
Oct 23 18:01:54 node1 kernel: [19233.232478] DEBUG 836:check_caller:605: caller_pid=836 caller_tgid=836    
Oct 23 18:01:54 node1 kernel: [19233.232480] DEBUG 836:check_caller:646: WLOCK_PROC ep=31 count=0

ESTE ES EL DESCRIPTOR DEL HIJO NO DEL MAIN !!!! POR LO TANTO EL ERROR DEBE ESTAR EN EL BIND QUE ASOCIA LA TASK AL PROC 
misc=2020 = MIS_BIT_ATOMIC | MIS_BIT_GRPLEADER !!!!!! 
Oct 23 18:01:54 node1 kernel: [19233.232482] DEBUG 836:check_caller:651: nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch BIT_RMTOPER
Oct 23 18:01:54 node1 kernel: [19233.232485] DEBUG 836:check_caller:679: nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch 
Oct 23 18:01:54 node1 kernel: [19233.232486] DEBUG 836:check_caller:680: WUNLOCK_PROC ep=31 count=0
Oct 23 18:01:54 node1 kernel: [19233.232488] ERROR: 836:check_caller:680: rcode=-320 <<<<<<<<<<<<<<<<<<<<<<<< EDVSPROCRUN EL PROCESO ESTA CORRIENDO
Oct 23 18:01:54 node1 kernel: [19233.232490] DEBUG 836:new_unbind:1889: RLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232491] DEBUG 836:new_unbind:1901: RUNLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232493] DEBUG 836:new_unbind:1903: RLOCK_PROC ep=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232494] DEBUG 836:new_unbind:1920: RUNLOCK_PROC ep=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232496] DEBUG 836:new_unbind:1947: WLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232498] DEBUG 836:new_unbind:1951: WLOCK_PROC ep=0 count=0

ESTO SALE DESPUES DEL TBIND 
 rmttap.c:send_thread:292:nr=31 endp=31 dcid=0 flags=0 misc=20 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch  << MIS_BIT_GRPLEADER ESTA MAL !!!

MATANDO AL SERVER REMOTO 
Oct 23 18:01:54 node1 kernel: [19233.232501] DEBUG 836:do_unbind:736: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 23 18:01:54 node1 kernel: [19233.232503] DEBUG 836:do_unbind:765: Caller nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch 
Oct 23 18:01:54 node1 kernel: [19233.232505] DEBUG 836:do_unbind:829: wakeup with error those processes trying to send a message to the proc
Oct 23 18:01:54 node1 kernel: [19233.232506] DEBUG 836:do_unbind:865: delete notify messages bits sent by the proc
Oct 23 18:01:54 node1 kernel: [19233.232515] DEBUG 836:do_unbind:878: Skip, self process
Oct 23 18:01:54 node1 kernel: [19233.232519] DEBUG 836:do_unbind:872: Skip, caller process
Oct 23 18:01:54 node1 kernel: [19233.232581] DEBUG 836:do_unbind:935: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 23 18:01:54 node1 kernel: [19233.232583] DEBUG 836:do_unbind:991: wakeup with error those processes waiting this process MIGRATION
Oct 23 18:01:54 node1 kernel: [19233.232585] DEBUG 836:do_unbind:1041: wakeup those processes waiting this process UNBINDING
Oct 23 18:01:54 node1 kernel: [19233.232618] DEBUG 836:do_unbind:1075: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 23 18:01:54 node1 kernel: [19233.232623] DEBUG 836:init_proc_desc:16: p_name=TAPserver00 dcid=0
Oct 23 18:01:54 node1 kernel: [19233.232624] DEBUG 836:init_proc_desc:27: Clearing Privileges
Oct 23 18:01:54 node1 kernel: [19233.232625] DEBUG 836:init_proc_desc:35: Setting Default DVK calls privileges
Oct 23 18:01:54 node1 kernel: [19233.232626] DEBUG 836:init_proc_desc:41: Clearing Process fields
Oct 23 18:01:54 node1 kernel: [19233.232628] DEBUG 836:do_unbind:1077: initialized 
Oct 23 18:01:54 node1 kernel: [19233.232630] DEBUG 836:do_unbind:1078: DC_DECREF counter=6
Oct 23 18:01:54 node1 kernel: [19233.232632] DEBUG 836:new_unbind:2012: WUNLOCK_PROC ep=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232634] DEBUG 836:new_unbind:2015: WUNLOCK_DC dc=0 count=0

MATANDO AL CLIENT REMOTO 
Oct 23 18:01:54 node1 kernel: [19233.232703] DEBUG 836:dvk_ioctl:349: cmd=4004E30A arg=BFEE2FB0
Oct 23 18:01:54 node1 kernel: [19233.232704] DEBUG 836:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 23 18:01:54 node1 kernel: [19233.232705] DEBUG 836:io_unbind:109: 
Oct 23 18:01:54 node1 kernel: [19233.232708] DEBUG 836:new_unbind:1873: dcid=0 proc_ep=30
Oct 23 18:01:54 node1 kernel: [19233.232711] DEBUG 836:check_caller:605: caller_pid=836 caller_tgid=836
Oct 23 18:01:54 node1 kernel: [19233.232713] DEBUG 836:check_caller:646: WLOCK_PROC ep=31 count=0
Oct 23 18:01:54 node1 kernel: [19233.232715] DEBUG 836:check_caller:651: nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch 
Oct 23 18:01:54 node1 kernel: [19233.232718] DEBUG 836:check_caller:679: nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch 
Oct 23 18:01:54 node1 kernel: [19233.232719] DEBUG 836:check_caller:680: WUNLOCK_PROC ep=31 count=0
Oct 23 18:01:54 node1 kernel: [19233.232720] ERROR: 836:check_caller:680: rcode=-320
Oct 23 18:01:54 node1 kernel: [19233.232722] DEBUG 836:new_unbind:1889: RLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232723] DEBUG 836:new_unbind:1901: RUNLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232725] DEBUG 836:new_unbind:1903: RLOCK_PROC ep=30 count=0
Oct 23 18:01:54 node1 kernel: [19233.232727] DEBUG 836:new_unbind:1920: RUNLOCK_PROC ep=30 count=0
Oct 23 18:01:54 node1 kernel: [19233.232728] DEBUG 836:new_unbind:1947: WLOCK_DC dc=0 count=0
Oct 23 18:01:54 node1 kernel: [19233.232730] DEBUG 836:new_unbind:1951: WLOCK_PROC ep=30 count=0
Oct 23 18:01:54 node1 kernel: [19233.232732] DEBUG 836:do_unbind:736: nr=30 endp=30 dcid=0 flags=1408 misc=2000 lpid=-1 vpid=-1 nodeid=0 name=TAPclient00 
Oct 23 18:01:54 node1 kernel: [19233.232735] DEBUG 836:do_unbind:765: Caller nr=31 endp=31 dcid=0 flags=8 misc=2020 lpid=838 vpid=4 nodeid=1 name=dvs_uml_switch 
Oct 23 18:01:54 node1 kernel: [19233.232736] DEBUG 836:do_unbind:815: WUNLOCK_PROC ep=30 count=0
Oct 23 18:01:54 node1 kernel: [19233.232738] DEBUG 836:do_unbind:816: WLOCK_PROC ep=1 count=0
Oct 23 18:01:54 node1 kernel: [19233.232739] DEBUG 836:do_unbind:817: WLOCK_PROC ep=30 count=0
Oct 23 18:01:54 node1 kernel: [19233.232740] DEBUG 836:do_unbind:829: wakeup with error those processes trying to send a message to the proc

QUEDA BLOQUEADO AQUI !!!!!! 
Oct 23 18:01:54 node1 kernel: [19233.232741] DEBUG 836:do_unbind:865: delete notify messages bits sent by the proc


SOLUCIONES:
1- Se modifico el do_unbind() que verificaba si el proceso REMOTO estaba en estado SENDING y enviaba un ACK 
	no se verificaba si tambien estaba en RECEIVING. De esa forma, mas adelante cuando verificaba si estaba 
	en RECEIVING volvia a encolar el mismo descriptor.
2- Habia errores en el bind() respecto a que ponie como GRPLEADER al primer thread bindeado, lo que esta mal
	

==============================================================================================================
20201024:

Sigue poniendose el thread como GRPLEADER!! misc=20
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0   0     0  1425/5      0    8   20 31438 27342 27342 27342 dvs_uml_switch 
 
root@node0:/var/log# grep "misc=20 lpid=" kern.log | more
Oct 24 11:19:02 node0 kernel: [  553.353851] DEBUG 1425:new_bind:1826: nr=0 endp=0 dcid=0 fl
ags=0 misc=20 lpid=1425 vpid=5 nodeid=0 name=dvs_uml_switch 
 
 
Oct 24 11:19:02 node0 kernel: [  553.353763] DEBUG 1425:dvk_ioctl:349: cmd=4004E309 arg=B63FF258
Oct 24 11:19:02 node0 kernel: [  553.353766] DEBUG 1425:dvk_ioctl:369: DVK_CALL=9 (io_bind) 
Oct 24 11:19:02 node0 kernel: [  553.353768] DEBUG 1425:io_bind:97: 
Oct 24 11:19:02 node0 kernel: [  553.353772] DEBUG 1425:new_bind:1568: oper=0 dcid=0 param_pid=5 endpoint=0 nodeid=-1
Oct 24 11:19:02 node0 kernel: [  553.353774] DEBUG 1425:new_bind:1591: dc_ptr=f89d6d00
Oct 24 11:19:02 node0 kernel: [  553.353777] DEBUG 1425:new_bind:1593: RLOCK_DC dc=0 count=0
Oct 24 11:19:02 node0 kernel: [  553.353779] DEBUG 1425:new_bind:1606: proc_ptr=ea0a4400
Oct 24 11:19:02 node0 kernel: [  553.353781] DEBUG 1425:new_bind:1607: WLOCK_PROC ep=0 count=0
Oct 24 11:19:02 node0 kernel: [  553.353785] DEBUG 1425:new_bind:1646: param_pid=5 lpid=1425 vpid=5 tid=1422 <<<<<<<<<<<<<<<<<<<<
Oct 24 11:19:02 node0 kernel: [  553.353788] DEBUG 1425:new_bind:1652: SELF_BIND param_pid=5 lpid=1425 vpid=5 tid=1422 <<<<<<<<<<
Oct 24 11:19:02 node0 kernel: [  553.353791] DEBUG 1425:init_proc_desc:16: p_name=$noname dcid=0
Oct 24 11:19:02 node0 kernel: [  553.353792] DEBUG 1425:init_proc_desc:27: Clearing Privileges
Oct 24 11:19:02 node0 kernel: [  553.353794] DEBUG 1425:init_proc_desc:35: Setting Default DVK calls privileges
Oct 24 11:19:02 node0 kernel: [  553.353796] DEBUG 1425:init_proc_desc:41: Clearing Process fields
Oct 24 11:19:02 node0 kernel: [  553.353798] DEBUG 1425:new_bind:1682: WUNLOCK_PROC ep=0 count=0
Oct 24 11:19:02 node0 kernel: [  553.353800] DEBUG 1425:new_bind:1686: WLOCK_TASK pid=1422 count=0
Oct 24 11:19:02 node0 kernel: [  553.353803] DEBUG 1425:new_bind:1687: WLOCK_PROC ep=0 count=0
Oct 24 11:19:02 node0 kernel: [  553.353804] DEBUG 1425:new_bind:1702: NOT thread_group_leader <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Oct 24 11:19:02 node0 kernel: [  553.353806] DEBUG 1425:new_bind:1704: WLOCK_TASK pid=1422 count=0
Oct 24 11:19:02 node0 kernel: [  553.353808] DEBUG 1425:new_bind:1708: WUNLOCK_TASK pid=1422 count=0
Oct 24 11:19:02 node0 kernel: [  553.353811] DEBUG 1425:new_bind:1726: increment the reference count of the task struct=142
2 count=2
Oct 24 11:19:02 node0 kernel: [  553.353813] DEBUG 1425:new_bind:1734: GRPLEADER lpid=1425 vpid=5 tid=1422
Oct 24 11:19:02 node0 kernel: [  553.353815] DEBUG 1425:new_bind:1739: process p_name=dvs_uml_switch *p_name_ptr=dvs_uml_switch
Oct 24 11:19:02 node0 kernel: [  553.353817] DEBUG 1425:new_bind:1819: sizeof(priv_usr_t)=52
Oct 24 11:19:02 node0 kernel: [  553.353820] DEBUG 1425:new_bind:1820: priv_id=34 priv_warn=27342 priv_level=0
Oct 24 11:19:02 node0 kernel: [  553.353822] DEBUG 1425:new_bind:1821: PRINT_SYS_MAP: 1425:new_bind:1821:proc_ptr->p_priv.priv_usr.priv_ipc_to:
Oct 24 11:19:02 node0 kernel: [  553.353826] DEBUG 1425:new_bind:1821: PRINT_SYS_MAP: 1425:new_bind:1821:4:
Oct 24 11:19:02 node0 kernel: [  553.353828] DEBUG 1425:new_bind:1821: 
Oct 24 11:19:02 node0 kernel: [  553.353828] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353830] DEBUG 1425:new_bind:1821: 
Oct 24 11:19:02 node0 kernel: [  553.353830] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353831] DEBUG 1425:new_bind:1821: 
Oct 24 11:19:02 node0 kernel: [  553.353831] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353833] DEBUG 1425:new_bind:1821: 
Oct 24 11:19:02 node0 kernel: [  553.353833] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353835] DEBUG 1425:new_bind:1821: 
Oct 24 11:19:02 node0 kernel: [  553.353837] DEBUG 1425:new_bind:1822: PRINT_DVKALLOWED_MAP: 1425:new_bind:1822:proc_ptr->p_priv.priv_usr.priv_
dvk_allowed:
Oct 24 11:19:02 node0 kernel: [  553.353840] DEBUG 1425:new_bind:1822: PRINT_DVKALLOWED_MAP: 1425:new_bind:1822:3:
Oct 24 11:19:02 node0 kernel: [  553.353842] DEBUG 1425:new_bind:1822: 
Oct 24 11:19:02 node0 kernel: [  553.353842] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353843] DEBUG 1425:new_bind:1822: 
Oct 24 11:19:02 node0 kernel: [  553.353843] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353845] DEBUG 1425:new_bind:1822: 
Oct 24 11:19:02 node0 kernel: [  553.353845] FFFF:
Oct 24 11:19:02 node0 kernel: [  553.353846] DEBUG 1425:new_bind:1822: 
Oct 24 11:19:02 node0 kernel: [  553.353851] DEBUG 1425:new_bind:1826: nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=1425 vpid=5 nodeid=0 name=dvs_um
l_switch 
Oct 24 11:19:02 node0 kernel: [  553.353854] DEBUG 1425:new_bind:1827: nr=0 endp=0 dcid=0 lpid=1425 p_cpumask=FFFFFFFF nodemap=1 name=dvs_uml_s
witch 
Oct 24 11:19:02 node0 kernel: [  553.353856] DEBUG 1425:new_bind:1841: WUNLOCK_TASK pid=1422 count=1
Oct 24 11:19:02 node0 kernel: [  553.353858] DEBUG 1425:new_bind:1847: WUNLOCK_PROC ep=0 count=0
Oct 24 11:19:02 node0 kernel: [  553.353860] DEBUG 1425:new_bind:1849: DC_INCREF counter=5
Oct 24 11:19:02 node0 kernel: [  553.353862] DEBUG 1425:new_bind:1850: RUNLOCK_DC dc=0 count=0


SOLUCIONADO EL TEMA GRPLEADER!!!!!!!!!

EN NODE0 DVS_UML_SWITCH DA ESTE ERROR 
 rmttap.c:recv_thread:424:RECEIVED REQUEST: source=31 type=2 m3i1=10 m3i2=70 m3p1=0x50b460 m3ca1=[tap1]
 rmttap.c:recv_thread:460:RECEIVED REQ_RT_WRITE REQUEST
 rmttap.c:recv_thread:472:REQ_RT_WRITE: rt_index=1 rt_nodeid=0 rt_tap=tap1 rt_ctrl_fd=-1 rt_data_fd=-1 rt_poll_idx=-1 rt_rmt_ep=31 rt_rmttap_fd=10 rt_name=LTAP1
DEBUG 2:dvk_vcopy:126: src_ep=31 dst_ep=0 bytes=70
DEBUG 2:dvk_sendrec_T:944: ioctl ret=0 errno=0
DEBUG 2:dvk_sendrec_T:953: ioctl ret=0
 rmttap.c:rt_write_packet:158:WRITE REPLY: source=1 type=8194 m3i1=-61 m3i2=98 m3p1=0x42e460 m3ca1=[tap1]
ERROR: rmttap.c:rt_write_packet:160: rcode=-61
 rmttap.c:try_rmttap_open:245:nr_rmttap=2
 rmttap.c:try_rmttap_open:251:rt_index=0 rt_nodeid=1 rt_tap=tap1 rt_ctrl_fd=7 rt_data_fd=9 rt_poll_idx=-1 rt_rmt_ep=1 rt_rmttap_fd=10 rt_name=RTAP1
ERROR: rmttap.c:send_thread:368: rcode=-61
DEBUG 2:dvk_vcopy:142: ioctl ret=-1 errno=324
DEBUG 2:dvk_vcopy:154: ioctl ret=-324 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<EDVSPROCSTS
ERROR: rmttap.c:recv_thread:487: rcode=-324
 rmttap.c:recv_thread:565:SEND REPLY: source=31 type=8194 m3i1=-324 m3i2=70 m3p1=0x50b460 m3ca1=[tap1]
DEBUG 2:dvk_send_T:864: endpoint=31 timeout=30000

EN EL KERNEL REPORTA
Oct 24 13:26:44 node1 kernel: [  830.567733] DEBUG 694:new_put2lcl:943: CMD_COPYOUT_DATA dcid=0 rmt_ep=30 rmt_nr=30 lcl_ep=1 lcl_nr=1
Oct 24 13:26:44 node1 kernel: [  830.567736] DEBUG 694:copyout_data_rmt2lcl:498: cmd=0x2006 dcid=0 src=30 dst=1 snode=0 dnode=1 rcode=0 len=98
Oct 24 13:26:44 node1 kernel: [  830.567739] DEBUG 694:copyout_data_rmt2lcl:499: src=30 dst=1 rqtr=1 saddr=0042e460 daddr=0050ba60 bytes=98 
Oct 24 13:26:44 node1 kernel: [  830.567741] ERROR: 694:copyout_data_rmt2lcl:546: rcode=-324 <<<<<<<<<<<<<<<< EDVSPROCSTS

	do {
		ret = OK;

		uproc_ptr = &lcl_ptr->p_usr;
		DVKDEBUG(DBGPARAMS,PROC_USR_FORMAT, PROC_USR_FIELDS(uproc_ptr));	<<<<<<<<<<<<<<<< SE AGREGARON ESTOS DEBUG PARA VER EL ESTADO 
		uproc_ptr = &dst_ptr->p_usr;
		DVKDEBUG(DBGPARAMS,PROC_USR_FORMAT, PROC_USR_FIELDS(uproc_ptr));
		uproc_ptr = &rmt_ptr->p_usr;
		DVKDEBUG(DBGPARAMS,PROC_USR_FORMAT, PROC_USR_FIELDS(uproc_ptr));

		if( lcl_ptr->p_usr.p_rts_flags == PROC_RUNNING) {ret= EDVSPROCRUN; break;}
		if( dst_ptr->p_usr.p_rts_flags == PROC_RUNNING) {ret= EDVSPROCRUN; break;}

		/*check that all envolved processes are on ONCOPY state */
		if(!test_bit(BIT_ONCOPY, &lcl_ptr->p_usr.p_rts_flags))	{ret= EDVSPROCSTS; break;} <<<<<<<<<<<<<<<< AQUI DA LOS ERRORES
	    if(!test_bit(BIT_ONCOPY, &dst_ptr->p_usr.p_rts_flags))	{ret= EDVSPROCSTS; break;} <<<<<<<<<<<<<<<< AQUI DA LOS ERRORES
	    if(!test_bit(BIT_ONCOPY, &rmt_ptr->p_usr.p_rts_flags))	{ret= EDVSPROCSTS; break;} <<<<<<<<<<<<<<<< AQUI DA LOS ERRORES
	}while(0);

Esta claro que el destintario no debe estar en RUNNING, pero cuales serian las 

==============================================================================================================
20201025:

Oct 25 08:03:22 node0 kernel: [ 2690.270257] DEBUG 1315:new_put2lcl:862: RUNLOCK_DC dc=0 count=0
Oct 25 08:03:22 node0 kernel: [ 2690.270259] DEBUG 1315:new_put2lcl:974: CMD_COPYOUT_DATA dcid=0 rmt_ep=31 rmt_nr=31 lcl_ep=0 lcl_nr=0
Oct 25 08:03:22 node0 kernel: [ 2690.270262] DEBUG 1315:copyout_data_rmt2lcl:502: cmd=0x2006 dcid=0 src=31 dst=0 snode=1 dnode=0 rcode=0 len=90
Oct 25 08:03:22 node0 kernel: [ 2690.270264] DEBUG 1315:copyout_data_rmt2lcl:503: src=31 dst=0 rqtr=0 saddr=0050b460 daddr=004c3a60 bytes=90 
Oct 25 08:03:22 node0 kernel: [ 2690.270267] DEBUG 1315:copyout_data_rmt2lcl:516: nr=0 endp=0 dcid=0 flags=400 misc=1000 lpid=1357 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 25 08:03:22 node0 kernel: [ 2690.270270] DEBUG 1315:copyout_data_rmt2lcl:518: nr=31 endp=31 dcid=0 flags=1408 misc=2000 lpid=-1 vpid=-1 nodeid=1 name=TAPclient01 
Oct 25 08:03:22 node0 kernel: [ 2690.270271] ERROR: 1315:copyout_data_rmt2lcl:527: rcode=-324  <<<<<<<<<<<<<<<<<<< EDVSPROCSTS
Oct 25 08:03:22 node0 kernel: [ 2690.270272] DEBUG 1315:new_put2lcl:996: WUNLOCK_PROC ep=0 count=0
Oct 25 08:03:22 node0 kernel: [ 2690.270273] DEBUG 1315:new_put2lcl:996: WUNLOCK_PROC ep=31 count=0
Oct 25 08:03:22 node0 kernel: [ 2690.270275] DEBUG 1315:new_put2lcl:1012: WLOCK_PROC ep=27342 count=1
Oct 25 08:03:22 node0 kernel: [ 2690.270276] DEBUG 1315:new_put2lcl:1014: WUNLOCK_PROC ep=27342 count=1
Oct 25 08:03:22 node0 kernel: [ 2690.270277] ERROR: 1315:new_put2lcl:1017: rcode=-324
Oct 25 08:03:22 node0 kernel: [ 2690.270278] ERROR: 1315:dvk_ioctl:373: rcode=-324

SOLUCIONADO 
Oct 25 08:41:05 node0 kernel: [  420.182863] DEBUG 658:new_put2lcl:955: CMD_COPYOUT_DATA dcid=0 rmt_ep=31 rmt_nr=31 lcl_ep=0 lcl_nr=0
Oct 25 08:41:05 node0 kernel: [  420.182866] DEBUG 658:copyout_data_rmt2lcl:501: cmd=0x2006 dcid=0 src=31 dst=0 snode=1 dnode=0 rcode=0 len=94
Oct 25 08:41:05 node0 kernel: [  420.182868] DEBUG 658:copyout_data_rmt2lcl:502: src=31 dst=0 rqtr=0 saddr=00498460 daddr=0050ca60 bytes=94 
Oct 25 08:41:05 node0 kernel: [  420.182870] DEBUG 658:copyout_data_rmt2lcl:513: nr=0 endp=0 dcid=0 flags=400 misc=1000 lpid=683 vpid=5 nodeid=0 name=dvs_uml_switch 
Oct 25 08:41:05 node0 kernel: [  420.183305] DEBUG 658:copyout_data_rmt2lcl:515: nr=31 endp=31 dcid=0 flags=1408 misc=2000 lpid=-1 vpid=-1 nodeid=1 name=TAPclient01 
Oct 25 08:41:05 node0 kernel: [  420.183660] DEBUG 658:copy_usr2usr:1073: rqtr_ep=27342 src_ep=27342 src_lpid=658 src_vpid=-1 src_addr=02308000
Oct 25 08:41:05 node0 kernel: [  420.183661] DEBUG 658:copy_usr2usr:1076: dst_ep=0 dst_lpid=683 dst_vpid=5 dst_addr=0050ca60 bytes=94


ERROR:
PROBLEMA, ALGO ESTA PASANDO CON EL LOCK DEL DCID PORQUE DESPUES DE TERMINAR PROCESOS 
SE QUEDA BLOQUEADO EN ALGUNA OPERACION DEL DC 

Oct 25 08:41:05 node0 kernel: [  420.650028] DEBUG 680:dvk_ioctl:349: cmd=4004E30A arg=BF86F430
Oct 25 08:41:05 node0 kernel: [  420.650031] DEBUG 680:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 25 08:41:05 node0 kernel: [  420.650033] DEBUG 680:io_unbind:109: 
Oct 25 08:41:05 node0 kernel: [  420.650035] DEBUG 680:new_unbind:1873: dcid=0 proc_ep=1
Oct 25 08:41:05 node0 kernel: [  420.650038] DEBUG 680:check_caller:605: caller_pid=680 caller_tgid=680
Oct 25 08:41:05 node0 kernel: [  420.650040] ERROR: 680:check_caller:642: rcode=-310
Oct 25 08:41:05 node0 kernel: [  420.650042] DEBUG 680:new_unbind:1889: RLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650044] DEBUG 680:new_unbind:1901: RUNLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650045] DEBUG 680:new_unbind:1903: RLOCK_PROC ep=1 count=0
Oct 25 08:41:05 node0 kernel: [  420.650046] DEBUG 680:new_unbind:1920: RUNLOCK_PROC ep=1 count=0
Oct 25 08:41:05 node0 kernel: [  420.650048] DEBUG 680:new_unbind:1947: WLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650049] DEBUG 680:new_unbind:1951: WLOCK_PROC ep=1 count=0
Oct 25 08:41:05 node0 kernel: [  420.650053] DEBUG 680:do_unbind:736: nr=1 endp=1 dcid=0 flags=3000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=TAPserver01 
Oct 25 08:41:05 node0 kernel: [  420.650054] DEBUG 680:do_unbind:829: wakeup with error those processes trying to send a message to the proc
Oct 25 08:41:05 node0 kernel: [  420.650055] DEBUG 680:do_unbind:865: delete notify messages bits sent by the proc
Oct 25 08:41:05 node0 kernel: [  420.650061] DEBUG 680:do_unbind:878: Skip, self process
Oct 25 08:41:05 node0 kernel: [  420.650078] DEBUG 680:do_unbind:935: nr=1 endp=1 dcid=0 flags=3000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=TAPserver01 
Oct 25 08:41:05 node0 kernel: [  420.650080] DEBUG 680:do_unbind:941: WLOCK_PROC ep=27342 count=1
Oct 25 08:41:05 node0 kernel: [  420.650081] DEBUG 680:do_unbind:942: LIST_DEL_INIT
Oct 25 08:41:05 node0 kernel: [  420.650082] DEBUG 680:do_unbind:943: WUNLOCK_PROC ep=27342 count=1
Oct 25 08:41:05 node0 kernel: [  420.650083] DEBUG 680:do_unbind:991: wakeup with error those processes waiting this process MIGRATION
Oct 25 08:41:05 node0 kernel: [  420.650085] DEBUG 680:do_unbind:1041: wakeup those processes waiting this process UNBINDING
Oct 25 08:41:05 node0 kernel: [  420.650087] DEBUG 680:do_unbind:1075: nr=1 endp=1 dcid=0 flags=1000 misc=0 lpid=-1 vpid=-1 nodeid=1 name=TAPserver01 
Oct 25 08:41:05 node0 kernel: [  420.650089] DEBUG 680:init_proc_desc:16: p_name=TAPserver01 dcid=0
Oct 25 08:41:05 node0 kernel: [  420.650090] DEBUG 680:init_proc_desc:27: Clearing Privileges
Oct 25 08:41:05 node0 kernel: [  420.650091] DEBUG 680:init_proc_desc:35: Setting Default DVK calls privileges
Oct 25 08:41:05 node0 kernel: [  420.650092] DEBUG 680:init_proc_desc:41: Clearing Process fields
Oct 25 08:41:05 node0 kernel: [  420.650094] DEBUG 680:do_unbind:1077: initialized 
Oct 25 08:41:05 node0 kernel: [  420.650095] DEBUG 680:do_unbind:1078: DC_DECREF counter=6
Oct 25 08:41:05 node0 kernel: [  420.650096] DEBUG 680:new_unbind:2012: WUNLOCK_PROC ep=1 count=0
Oct 25 08:41:05 node0 kernel: [  420.650098] DEBUG 680:new_unbind:2015: WUNLOCK_DC dc=0 count=0

Oct 25 08:41:05 node0 kernel: [  420.650141] DEBUG 680:dvk_ioctl:349: cmd=4004E30A arg=BF86F430
Oct 25 08:41:05 node0 kernel: [  420.650142] DEBUG 680:dvk_ioctl:369: DVK_CALL=10 (io_unbind)  
Oct 25 08:41:05 node0 kernel: [  420.650143] DEBUG 680:io_unbind:109: 
Oct 25 08:41:05 node0 kernel: [  420.650145] DEBUG 680:new_unbind:1873: dcid=0 proc_ep=31                   <<<<<<<<<<<<< REMOTO 31 
Oct 25 08:41:05 node0 kernel: [  420.650146] DEBUG 680:check_caller:605: caller_pid=680 caller_tgid=680
Oct 25 08:41:05 node0 kernel: [  420.650147] ERROR: 680:check_caller:642: rcode=-310
Oct 25 08:41:05 node0 kernel: [  420.650149] DEBUG 680:new_unbind:1889: RLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650150] DEBUG 680:new_unbind:1901: RUNLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650151] DEBUG 680:new_unbind:1903: RLOCK_PROC ep=31 count=0
Oct 25 08:41:05 node0 kernel: [  420.650152] DEBUG 680:new_unbind:1920: RUNLOCK_PROC ep=31 count=0
Oct 25 08:41:05 node0 kernel: [  420.650153] DEBUG 680:new_unbind:1947: WLOCK_DC dc=0 count=0
Oct 25 08:41:05 node0 kernel: [  420.650155] DEBUG 680:new_unbind:1951: WLOCK_PROC ep=31 count=0
Oct 25 08:41:05 node0 kernel: [  420.650157] DEBUG 680:do_unbind:736: nr=31 endp=31 dcid=0 flags=1408 misc=2000 lpid=-1 vpid=-1 nodeid=1 name=TAPclient01 
Oct 25 08:41:05 node0 kernel: [  420.650158] DEBUG 680:do_unbind:815: WUNLOCK_PROC ep=31 count=0
Oct 25 08:41:05 node0 kernel: [  420.650160] DEBUG 680:do_unbind:816: WLOCK_PROC ep=0 count=0            <<<<<<<<<<<< proceso local    src_ptr
Oct 25 08:41:05 node0 kernel: [  420.650161] DEBUG 680:do_unbind:817: WLOCK_PROC ep=31 count=0           <<<<<<<<<<<<< proceso remoto  proc_ptr
Oct 25 08:41:05 node0 kernel: [  420.650162] DEBUG 680:do_unbind:829: wakeup with error those processes trying to send a message to the proc
Oct 25 08:41:05 node0 kernel: [  420.650163] DEBUG 680:do_unbind:865: delete notify messages bits sent by the proc <<<<<<< BLOQUEADO 


SIGUE EL ERROR PERO EN OTRO LADO AL HABER MODIFICADO 
Oct 25 11:36:54 node1 kernel: [ 5562.921906] DEBUG 1420:dvk_ioctl:349: cmd=4004E30A arg=BFA94FF0
Oct 25 11:36:54 node1 kernel: [ 5562.921910] DEBUG 1420:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 25 11:36:54 node1 kernel: [ 5562.921912] DEBUG 1420:io_unbind:109: 
Oct 25 11:36:54 node1 kernel: [ 5562.921916] DEBUG 1420:new_unbind:1906: dcid=0 proc_ep=0
Oct 25 11:36:54 node1 kernel: [ 5562.921920] DEBUG 1420:check_caller:605: caller_pid=1420 caller_tgid=1420
Oct 25 11:36:54 node1 kernel: [ 5562.921922] ERROR: 1420:check_caller:642: rcode=-310
Oct 25 11:36:54 node1 kernel: [ 5562.921924] DEBUG 1420:new_unbind:1922: RLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921925] DEBUG 1420:new_unbind:1934: RUNLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921927] DEBUG 1420:new_unbind:1936: RLOCK_PROC ep=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921929] DEBUG 1420:new_unbind:1953: RUNLOCK_PROC ep=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921930] DEBUG 1420:new_unbind:1980: WLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921932] DEBUG 1420:new_unbind:1984: WLOCK_PROC ep=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.921958] DEBUG 1420:do_unbind:733: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 25 11:36:54 node1 kernel: [ 5562.921960] DEBUG 1420:do_unbind:826: wakeup with error those processes trying to send a message to the proc
Oct 25 11:36:54 node1 kernel: [ 5562.921962] DEBUG 1420:do_unbind:864: delete notify messages bits sent by the proc
Oct 25 11:36:54 node1 kernel: [ 5562.922340] DEBUG 1420:do_unbind:876: Skip, self process
Oct 25 11:36:54 node1 kernel: [ 5562.922345] DEBUG 1420:do_unbind:908: Check if another process is waiting to receive a message from the unbound process
Oct 25 11:36:54 node1 kernel: [ 5562.922349] DEBUG 1420:do_unbind:920: Skip, self process
Oct 25 11:36:54 node1 kernel: [ 5562.922352] DEBUG 1420:do_unbind:947: Process 31 is no more waiting a message from the unbound process 0
Oct 25 11:36:54 node1 kernel: [ 5562.922354] DEBUG 1420:do_unbind:948: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 25 11:36:54 node1 kernel: [ 5562.922356] DEBUG 1420:do_unbind:955: Wakeup RECEIVER with error ep=31  pid=1422
Oct 25 11:36:54 node1 kernel: [ 5562.922358] DEBUG 1420:inherit_cpu:298: cpuid=3 vpid=9
Oct 25 11:36:54 node1 kernel: [ 5562.922365] DEBUG 1420:inherit_cpu:306: nr=31 endp=31 dcid=0 lpid=1422 p_cpumask=FFFFFFFF nodemap=2 name=dvs_uml_switch 
Oct 25 11:36:54 node1 kernel: [ 5562.922366] DEBUG 1420:do_unbind:956: BEFORE UP lpid=1422 p_sem=-1 rcode=-108
Oct 25 11:36:54 node1 kernel: [ 5562.922484] DEBUG 1420:do_unbind:968: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 25 11:36:54 node1 kernel: [ 5562.922486] DEBUG 1420:do_unbind:1024: wakeup with error those processes waiting this process MIGRATION
Oct 25 11:36:54 node1 kernel: [ 5562.922487] DEBUG 1420:do_unbind:1074: wakeup those processes waiting this process UNBINDING
Oct 25 11:36:54 node1 kernel: [ 5562.922490] DEBUG 1420:do_unbind:1108: nr=0 endp=0 dcid=0 flags=1000 misc=1000 lpid=-1 vpid=-1 nodeid=0 name=TAPserver00 
Oct 25 11:36:54 node1 kernel: [ 5562.922491] DEBUG 1420:init_proc_desc:16: p_name=TAPserver00 dcid=0
Oct 25 11:36:54 node1 kernel: [ 5562.922492] DEBUG 1420:init_proc_desc:27: Clearing Privileges
Oct 25 11:36:54 node1 kernel: [ 5562.922493] DEBUG 1420:init_proc_desc:35: Setting Default DVK calls privileges
Oct 25 11:36:54 node1 kernel: [ 5562.922494] DEBUG 1420:init_proc_desc:41: Clearing Process fields
Oct 25 11:36:54 node1 kernel: [ 5562.922496] DEBUG 1420:do_unbind:1110: initialized 
Oct 25 11:36:54 node1 kernel: [ 5562.922497] DEBUG 1420:do_unbind:1111: DC_DECREF counter=6
Oct 25 11:36:54 node1 kernel: [ 5562.922499] DEBUG 1420:new_unbind:2043: WUNLOCK_PROC ep=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922501] DEBUG 1420:new_unbind:2046: WUNLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922525] DEBUG 1420:dvk_ioctl:349: cmd=4004E30A arg=BFA94FF0
Oct 25 11:36:54 node1 kernel: [ 5562.922527] DEBUG 1420:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 25 11:36:54 node1 kernel: [ 5562.922528] DEBUG 1420:io_unbind:109: 
Oct 25 11:36:54 node1 kernel: [ 5562.922529] DEBUG 1420:new_unbind:1906: dcid=0 proc_ep=30
Oct 25 11:36:54 node1 kernel: [ 5562.922531] DEBUG 1420:check_caller:605: caller_pid=1420 caller_tgid=1420
Oct 25 11:36:54 node1 kernel: [ 5562.922532] ERROR: 1420:check_caller:642: rcode=-310
Oct 25 11:36:54 node1 kernel: [ 5562.922533] DEBUG 1420:new_unbind:1922: RLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922535] DEBUG 1420:new_unbind:1934: RUNLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922536] DEBUG 1420:new_unbind:1936: RLOCK_PROC ep=30 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922537] DEBUG 1420:new_unbind:1953: RUNLOCK_PROC ep=30 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922539] DEBUG 1420:new_unbind:1980: WLOCK_DC dc=0 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922540] DEBUG 1420:new_unbind:1984: WLOCK_PROC ep=30 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922542] DEBUG 1420:do_unbind:733: nr=30 endp=30 dcid=0 flags=1408 misc=2000 lpid=-1 vpid=-1 nodeid=0 name=TAPclient00 
Oct 25 11:36:54 node1 kernel: [ 5562.922543] DEBUG 1420:do_unbind:812: WUNLOCK_PROC ep=30 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922545] DEBUG 1420:do_unbind:813: WLOCK_PROC ep=1 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922546] DEBUG 1420:do_unbind:814: WLOCK_PROC ep=30 count=0
Oct 25 11:36:54 node1 kernel: [ 5562.922547] DEBUG 1420:do_unbind:826: wakeup with error those processes trying to send a message to the proc
Oct 25 11:36:54 node1 kernel: [ 5562.922548] DEBUG 1420:do_unbind:908: Check if another process is waiting to receive a message from the unbound process


==============================================================================================================
20201027: 

PROBLEMAS CON UNBIND !!!!
ESTOS SON LOS LOCK Y UNLOCK NO LOGGEADOS 
root@node0:/usr/src/linux/ipc/dvk-mod# grep wlock_proc *.c
dvk_hyper.c:                            wlock_proc(rp); /* proc_ptr LOCK is just locked */
dvk_hyper.c:                            wlock_proc(rp);
dvk_hyper.c:                            wlock_proc(proc_ptr);
root@node0:/usr/src/linux/ipc/dvk-mod# grep wunlock_proc *.c
dvk_hyper.c:            wunlock_proc(src_ptr);
dvk_hyper.c:                            wunlock_proc(proc_ptr);
dvk_hyper.c:                            wunlock_proc(rp); 
dvk_hyper.c:                    wunlock_proc(rp); 
dvk_hyper.c:            wunlock_proc(rp); 

EL PROBLEMA ES QUE HAY SALTOS DE LOS CONTADORES 


Oct 27 08:43:21 node1 kernel: [ 2728.211158] DEBUG 6118:new_getprocinfo:2298: RLOCK_PROC ep=0 count=0
Oct 27 09:37:52 node1 kernel: [ 5999.820177] DEBUG 9409:new_getprocinfo:2298: RLOCK_PROC ep=0 count=5
Oct 27 09:40:23 node1 kernel: [ 6151.031460] DEBUG 9408:new_unbind:1942: RLOCK_PROC ep=0 count=5
Oct 27 09:49:58 node1 kernel: [ 6725.550398] DEBUG 9764:new_getprocinfo:2298: RLOCK_PROC ep=0 count=10



Oct 27 17:15:24 node1 kernel: [33451.742656] DEBUG 10463:dvk_ioctl:349: cmd=4004E30A arg=BFE39660
Oct 27 17:15:24 node1 kernel: [33451.742661] DEBUG 10463:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 27 17:15:24 node1 kernel: [33451.742664] DEBUG 10463:io_unbind:109: 
Oct 27 17:15:24 node1 kernel: [33451.742667] DEBUG 10463:new_unbind:1912: dcid=0 proc_ep=30
Oct 27 17:15:24 node1 kernel: [33451.742672] DEBUG 10463:check_caller:605: caller_pid=10463 caller_tgid=10463
Oct 27 17:15:24 node1 kernel: [33451.742722] ERROR: 10463:check_caller:642: rcode=-310
Oct 27 17:15:24 node1 kernel: [33451.742726] DEBUG 10463:new_unbind:1928: RLOCK_DC dc=0 count=0
Oct 27 17:15:24 node1 kernel: [33451.742729] DEBUG 10463:new_unbind:1940: RUNLOCK_DC dc=0 count=0
Oct 27 17:15:24 node1 kernel: [33451.742732] DEBUG 10463:new_unbind:1942: RLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742734] DEBUG 10463:new_unbind:1959: RUNLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742737] DEBUG 10463:new_unbind:1986: WLOCK_DC dc=0 count=0
Oct 27 17:15:24 node1 kernel: [33451.742739] DEBUG 10463:new_unbind:1990: WLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742744] DEBUG 10463:do_unbind:733: nr=30 endp=30 dcid=0 flags=100C misc=2000 lpid=-1 vpid=-1 nodeid=0 name=TAPclient00 
Oct 27 17:15:24 node1 kernel: [33451.742747] DEBUG 10463:do_unbind:812: WUNLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742750] DEBUG 10463:do_unbind:813: WLOCK_PROC ep=1 count=3
Oct 27 17:15:24 node1 kernel: [33451.742752] DEBUG 10463:do_unbind:814: WLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742754] DEBUG 10463:do_unbind:826: wakeup with error those processes trying to send a message to the proc
Oct 27 17:15:24 node1 kernel: [33451.742756] DEBUG 10463:do_unbind:908: Check if another process is waiting to receive a message from the unbound process
Oct 27 17:15:24 node1 kernel: [33451.742763] DEBUG 10463:do_unbind:936: WUNLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742765] DEBUG 10463:do_unbind:937: WLOCK_PROC ep=1 count=2
Oct 27 17:15:24 node1 kernel: [33451.742767] DEBUG 10463:do_unbind:938: WLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742769] DEBUG 10463:do_unbind:970: WUNLOCK_PROC ep=1 count=2
Oct 27 17:15:24 node1 kernel: [33451.742772] DEBUG 10463:do_unbind:936: WUNLOCK_PROC ep=30 count=28 <<<<<<<<<<<<<<<<<<<< COUNT A LA MIERDA!!!
Oct 27 17:15:24 node1 kernel: [33451.742774] DEBUG 10463:do_unbind:937: WLOCK_PROC ep=2 count=21
Oct 27 17:15:24 node1 kernel: [33451.742776] DEBUG 10463:do_unbind:938: WLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.742778] DEBUG 10463:do_unbind:970: WUNLOCK_PROC ep=2 count=21
Oct 27 17:15:24 node1 kernel: [33451.742782] DEBUG 10463:do_unbind:926: Skip, self process
Oct 27 17:15:24 node1 kernel: [33451.742784] DEBUG 10463:do_unbind:933: WLOCK_PROC ep=31 count=6
Oct 27 17:15:24 node1 kernel: [33451.742786] DEBUG 10463:do_unbind:970: WUNLOCK_PROC ep=31 count=6
Oct 27 17:15:24 node1 kernel: [33451.742788] DEBUG 10463:do_unbind:933: WLOCK_PROC ep=32 count=14
Oct 27 17:15:24 node1 kernel: [33451.742790] DEBUG 10463:do_unbind:970: WUNLOCK_PROC ep=32 count=14
Oct 27 17:15:24 node1 kernel: [33451.742810] DEBUG 10463:do_unbind:974: nr=30 endp=30 dcid=0 flags=1004 misc=0 lpid=-1 vpid=-1 nodeid=0 name=TAPclient00 
Oct 27 17:15:24 node1 kernel: [33451.742817] DEBUG 10463:do_unbind:994: sendtonr=-1996488704 endp=1465001012 dcid=-402625968 flags=FFFE9EE0 misc=F900850F lpid=-2078227325 vpid=193204160 nodeid=-2080374791 name= 
Oct 27 17:15:24 node1 kernel: [33451.742819] DEBUG 10463:do_unbind:999: WUNLOCK_PROC ep=30 count=28
Oct 27 17:15:24 node1 kernel: [33451.779843] BUG: unable to handle kernel paging request at 0f6c2474
Oct 27 17:15:24 node1 kernel: [33451.780693] IP: [<d40b47d2>] mutex_optimistic_spin+0x32/0x190
Oct 27 17:15:24 node1 kernel: [33451.782008] *pdpt = 0000000029c9b001 *pde = 0000000000000000 
Oct 27 17:15:24 node1 kernel: [33451.782169] 
Oct 27 17:15:24 node1 kernel: [33451.783035] Oops: 0000 [#1] SMP
Oct 27 17:15:24 node1 kernel: [33451.784156] Modules linked in: dvk(O) tun bridge stp llc iptable_filter fuse vmwgfx joydev evdev sg pcspkr vmw_balloon serio_raw ttm drm_kms_helper drm shpchp vmw_vmci ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache hid_generic usbhid hid sr_mod cdrom sd_mod ata_generic psmouse xhci_pci xhci_hcd ehci_pci uhci_hcd ehci_hcd ata_piix usbcore mptspi scsi_transport_spi pcnet32
Oct 27 17:15:24 node1 kernel: [33451.797511] do_exit: local_nodeid:1
Oct 27 17:15:24 node1 kernel: [33451.797516] DEBUG 10469:new_exit_unbind:267: code=-730393130
Oct 27 17:15:24 node1 kernel: [33451.797519] DEBUG 10469:new_exit_unbind:271: WLOCK_TASK pid=210 count=0
Oct 27 17:15:24 node1 kernel: [33451.797522] DEBUG 10469:new_exit_unbind:400: Process not bound pid=10469 systemd-journal
Oct 27 17:15:24 node1 kernel: [33451.797524] DEBUG 10469:new_exit_unbind:402: WUNLOCK_TASK pid=210 count=0
Oct 27 17:15:24 node1 kernel: [33451.805619]  mptscsih mii libata mptbase scsi_mod i2c_piix4
Oct 27 17:15:24 node1 kernel: [33451.806858] CPU: 3 PID: 10463 Comm: dvs_uml_switch Tainted: G           O    4.9.88-DVK #159
Oct 27 17:15:24 node1 kernel: [33451.808133] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 27 17:15:24 node1 kernel: [33451.810913] task: da1e2cc0 task.stack: eb3f2000
Oct 27 17:15:24 node1 kernel: [33451.812456] EIP: 0060:[<d40b47d2>] EFLAGS: 00010202 CPU: 3
Oct 27 17:15:24 node1 kernel: [33451.813960] EIP is at mutex_optimistic_spin+0x32/0x190
Oct 27 17:15:24 node1 kernel: [33451.815550] EAX: 0f6c2454 EBX: cc83bfd8 ECX: 00000000 EDX: 00000000
Oct 27 17:15:24 node1 kernel: [33451.817147] ESI: cc8481d8 EDI: 00000000 EBP: eb3f3d60 ESP: eb3f3d3c
Oct 27 17:15:24 node1 kernel: [33451.818779]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
Oct 27 17:15:24 node1 kernel: [33451.820425] CR0: 80050033 CR2: 0f6c2474 CR3: 1ae17260 CR4: 000006f0
Oct 27 17:15:24 node1 kernel: [33451.822181] Stack:
Oct 27 17:15:24 node1 kernel: [33451.824120]  00000000 00000000 da1e2cc0 00000000 d499ad00 00000000 cc83bfd8 cc8481d8
Oct 27 17:15:24 node1 kernel: [33451.826019]  da1e2cc0 eb3f3d88 d45bccbd cc848000 eb3f3d8c d40ba8e7 00000000 09f6056d
Oct 27 17:15:24 node1 kernel: [33451.827962]  cc83bfd8 cc8481d8 cc848000 eb3f3d94 d45bc51c cc83be00 eb3f3df8 f8895471
Oct 27 17:15:24 node1 kernel: [33451.829964] Call Trace:
Oct 27 17:15:24 node1 kernel: [33451.832336]  [<d45bccbd>] ? __mutex_lock_slowpath+0x2d/0x100
Oct 27 17:15:24 node1 kernel: [33451.834444]  [<d40ba8e7>] ? vprintk_default+0x37/0x40
Oct 27 17:15:24 node1 kernel: [33451.836773]  [<d45bc51c>] ? mutex_lock+0x1c/0x30
Oct 27 17:15:24 node1 kernel: [33451.839105]  [<f8895471>] ? do_unbind+0x136b/0x2721 [dvk]
Oct 27 17:15:24 node1 kernel: [33451.841313]  [<f889bc00>] ? new_unbind+0x284/0x1577 [dvk]
Oct 27 17:15:24 node1 kernel: [33451.843565]  [<d40ba4fe>] ? vprintk_emit+0x2ee/0x4f0
Oct 27 17:15:24 node1 kernel: [33451.845887]  [<f8890148>] ? io_unbind+0x4e/0xce [dvk]
Oct 27 17:15:24 node1 kernel: [33451.848404]  [<f888f1fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 27 17:15:24 node1 kernel: [33451.850761]  [<f888f16d>] ? dvk_write+0x49/0x49 [dvk]
Oct 27 17:15:24 node1 kernel: [33451.853059]  [<d41e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 27 17:15:24 node1 kernel: [33451.856067]  [<d41d316d>] ? vfs_write+0x15d/0x1c0
Oct 27 17:15:24 node1 kernel: [33451.858244]  [<d41e6d20>] ? SyS_ioctl+0x60/0x70
Oct 27 17:15:24 node1 kernel: [33451.860613]  [<d4003708>] ? do_fast_syscall_32+0x98/0x160
Oct 27 17:15:24 node1 kernel: [33451.862800]  [<d45bf682>] ? sysenter_past_esp+0x47/0x75
Oct 27 17:15:24 node1 kernel: [33451.864937]  [<d4086f50>] ? override_creds+0x30/0x30
Oct 27 17:15:24 node1 kernel: [33451.866887] Code: ec 18 0f 1f 44 00 00 89 c3 89 d7 89 4d e8 64 a1 48 12 97 d4 89 45 e4 8b 00 88 4d f0 a8 08 0f 85 b5 00 00 00 8b 43 10 85 c0 74 0b <8b> 50 20 85 d2 0f 84 a3 00 00 00 8d 43 14 89 45 e0 e8 c8 06 00
Oct 27 17:15:24 node1 kernel: [33451.873027] EIP: [<d40b47d2>] 
Oct 27 17:15:24 node1 kernel: [33451.873067] mutex_optimistic_spin+0x32/0x190
Oct 27 17:15:24 node1 kernel: [33451.875161]  SS:ESP 0068:eb3f3d3c
Oct 27 17:15:24 node1 kernel: [33451.877236] CR2: 000000000f6c2474
Oct 27 17:15:24 node1 kernel: [33451.902960] br0: port 1(tap0) entered disabled state
Oct 27 17:15:24 node1 kernel: [33451.925766] ---[ end trace ca7f7c747d1e2329 ]---
Oct 27 17:15:24 node1 kernel: [33451.927838] do_exit: local_nodeid:1
Oct 27 17:15:24 node1 kernel: [33451.930496] DEBUG 10463:new_exit_unbind:267: code=-730393130
Oct 27 17:15:24 node1 kernel: [33451.932542] DEBUG 10463:new_exit_unbind:271: WLOCK_TASK pid=10463 count=0
Oct 27 17:15:24 node1 kernel: [33451.934570] DEBUG 10463:new_exit_unbind:400: Process not bound pid=10463 dvs_uml_switch
Oct 27 17:15:24 node1 kernel: [33451.938544] DEBUG 10463:new_exit_unbind:402: WUNLOCK_TASK pid=10463 count=0
Oct 27 17:15:46 node1 kernel: [33474.326826] DEBUG 10466:sleep_proc:390: pending: sig[0]:0x00000000, sig[1]:0x00000000
Oct 27 17:15:46 node1 kernel: [33474.330544] DEBUG 10466:sleep_proc:393: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
Oct 27 17:15:46 node1 kernel: [33474.334075] DEBUG 10466:sleep_proc:399: endpoint=1 ret=0 p_rcode=0
Oct 27 17:15:46 node1 kernel: [33474.338196] DEBUG 10466:sleep_proc:400: endpoint=1 flags=8 cpuid=1
Oct 27 17:15:46 node1 kernel: [33474.341841] DEBUG 10466:sleep_proc:401: WLOCK_PROC ep=1 count=2
Oct 27 17:15:46 node1 kernel: [33474.345061] DEBUG 10466:sleep_proc:429: pid=10466 ret=-61
Oct 27 17:15:46 node1 kernel: [33474.348810] DEBUG 10466:sleep_proc:454: nr=1 endp=1 dcid=0 lpid=10466 p_cpumask=FFFFFFFF nodemap=2 name=dvs_uml_switch 
Oct 27 17:15:46 node1 kernel: [33474.355214] DEBUG 10466:sleep_proc:456: someone wakeups me: sem=0 p_rcode=-61
Oct 27 17:15:46 node1 kernel: [33474.358460] DEBUG 10466:new_mini_receive:533: WUNLOCK_PROC ep=1 count=2

root@node1:/usr/src/dvs/dvs-apps/dvs_uml_switch# ERROR: 1130:dvk_sendrec_T:946: rcode=-108
ERROR: 1130:dvk_sendrec_T:955: rcode=-108
ERROR: rmttap.c:rt_send_write_packet:168: rcode=-108
ERROR: rmttap.c:send_thread:381: rcode=-108
ERROR: 1130:dvk_sendrec_T:946: rcode=-105
ERROR: 1130:dvk_sendrec_T:955: rcode=-105
ERROR: rmttap.c:rt_send_write_packet:168: rcode=-105
ERROR: rmttap.c:send_thread:381: rcode=-105

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.806858] CPU: 3 PID: 10463 Comm: dvs_uml_switch Tainted: G           O    4.9.88-DVK #159

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.808133] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.810913] task: da1e2cc0 task.stack: eb3f2000

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.822181] Stack:

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.824120]  00000000 00000000 da1e2cc0 00000000 d499ad00 00000000 cc83bfd8 cc8481d8

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.826019]  da1e2cc0 eb3f3d88 d45bccbd cc848000 eb3f3d8c d40ba8e7 00000000 09f6056d

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.827962]  cc83bfd8 cc8481d8 cc848000 eb3f3d94 d45bc51c cc83be00 eb3f3df8 f8895471

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.829964] Call Trace:

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.832336]  [<d45bccbd>] ? __mutex_lock_slowpath+0x2d/0x100

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.834444]  [<d40ba8e7>] ? vprintk_default+0x37/0x40

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.836773]  [<d45bc51c>] ? mutex_lock+0x1c/0x30

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.839105]  [<f8895471>] ? do_unbind+0x136b/0x2721 [dvk]

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.841313]  [<f889bc00>] ? new_unbind+0x284/0x1577 [dvk]

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.843565]  [<d40ba4fe>] ? vprintk_emit+0x2ee/0x4f0

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.845887]  [<f8890148>] ? io_unbind+0x4e/0xce [dvk]

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.848404]  [<f888f1fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.850761]  [<f888f16d>] ? dvk_write+0x49/0x49 [dvk]

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.853059]  [<d41e6624>] ? do_vfs_ioctl+0x94/0x730

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.856067]  [<d41d316d>] ? vfs_write+0x15d/0x1c0

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.858244]  [<d41e6d20>] ? SyS_ioctl+0x60/0x70

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.860613]  [<d4003708>] ? do_fast_syscall_32+0x98/0x160

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.862800]  [<d45bf682>] ? sysenter_past_esp+0x47/0x75

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.864937]  [<d4086f50>] ? override_creds+0x30/0x30

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.866887] Code: ec 18 0f 1f 44 00 00 89 c3 89 d7 89 4d e8 64 a1 48 12 97 d4 89 45 e4 8b 00 88 4d f0 a8 08 0f 85 b5 00 00 00 8b 43 10 85 c0 74 0b <8b> 50 20 85 d2 0f 84 a3 00 00 00 8d 43 14 89 45 e0 e8 c8 06 00

Message from syslogd@node1 at Oct 27 17:15:24 ...
 kernel:[33451.873027] EIP: [<d40b47d2>] 

------------------------------------------------------------------------------------------------
Oct 31 20:04:45 node0 kernel: [ 1378.565244] DEBUG 757:dvk_ioctl:349: cmd=4004E30A arg=BFEB38A0
Oct 31 20:04:45 node0 kernel: [ 1378.565247] DEBUG 757:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
Oct 31 20:04:45 node0 kernel: [ 1378.565249] DEBUG 757:io_unbind:109: 
Oct 31 20:04:45 node0 kernel: [ 1378.565252] DEBUG 757:new_unbind:1912: dcid=0 proc_ep=1
Oct 31 20:04:45 node0 kernel: [ 1378.565256] DEBUG 757:check_caller:605: caller_pid=757 caller_tgid=757
Oct 31 20:04:45 node0 kernel: [ 1378.565257] ERROR: 757:check_caller:642: rcode=-310
Oct 31 20:04:45 node0 kernel: [ 1378.565259] DEBUG 757:new_unbind:1928: RLOCK_DC dc=0 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565261] DEBUG 757:new_unbind:1940: RUNLOCK_DC dc=0 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565262] DEBUG 757:new_unbind:1942: RLOCK_PROC ep=1 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565264] DEBUG 757:new_unbind:1959: RUNLOCK_PROC ep=1 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565265] DEBUG 757:new_unbind:1986: WLOCK_DC dc=0 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565266] DEBUG 757:new_unbind:1990: WLOCK_PROC ep=1 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565269] DEBUG 757:do_unbind:733: nr=1 endp=1 dcid=0 flags=1004 misc=0 lpid=-1 vpid=-1 nodeid=1 name=TAPserver01 
Oct 31 20:04:45 node0 kernel: [ 1378.565271] DEBUG 757:do_unbind:826: wakeup with error those processes trying to send a message to the proc
Oct 31 20:04:45 node0 kernel: [ 1378.565272] DEBUG 757:do_unbind:864: delete notify messages bits sent by the proc
Oct 31 20:04:45 node0 kernel: [ 1378.565277] DEBUG 757:do_unbind:876: Skip, self process
Oct 31 20:04:45 node0 kernel: [ 1378.565280] DEBUG 757:do_unbind:908: Check if another process is waiting to receive a message from the unbound process
Oct 31 20:04:45 node0 kernel: [ 1378.565281] DEBUG 757:do_unbind:936: WUNLOCK_PROC ep=1 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565282] DEBUG 757:do_unbind:937: WLOCK_PROC ep=0 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565284] DEBUG 757:do_unbind:938: WLOCK_PROC ep=1 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565285] DEBUG 757:do_unbind:970: WUNLOCK_PROC ep=0 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565286] DEBUG 757:do_unbind:926: Skip, self process

Oct 31 20:04:45 node0 kernel: [ 1378.565287] DEBUG 757:do_unbind:933: WLOCK_PROC ep=2 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565288] DEBUG 757:do_unbind:970: WUNLOCK_PROC ep=2 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565290] DEBUG 757:do_unbind:933: WLOCK_PROC ep=30 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565291] DEBUG 757:do_unbind:970: WUNLOCK_PROC ep=30 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565292] DEBUG 757:do_unbind:933: WLOCK_PROC ep=31 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565293] DEBUG 757:do_unbind:970: WUNLOCK_PROC ep=31 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565294] DEBUG 757:do_unbind:933: WLOCK_PROC ep=32 count=0
Oct 31 20:04:45 node0 kernel: [ 1378.565295] DEBUG 757:do_unbind:970: WUNLOCK_PROC ep=32 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.565444] DEBUG 757:do_unbind:974: nr=1 endp=1 dcid=0 flags=1004 misc=0 lpid=-1 vpid=-1 nodeid=1 name=TAPserver01 
Oct 31 20:04:45 node0 kernel: [ 1378.565447] DEBUG 757:do_unbind:994: sendtonr=0 endp=0 dcid=0 flags=0 misc=0 lpid=0 vpid=0 nodeid=0 name= 
Oct 31 20:04:45 node0 kernel: [ 1378.565448] DEBUG 757:do_unbind:999: WUNLOCK_PROC ep=1 count=0

Oct 31 20:04:45 node0 kernel: [ 1378.572146] BUG: unable to handle kernel NULL pointer dereference at   (null)
Oct 31 20:04:45 node0 kernel: [ 1378.572721] IP: [<d22d0b4b>] __list_add+0xb/0x120
Oct 31 20:04:45 node0 kernel: [ 1378.573285] *pdpt = 000000000eee0001 *pde = 0000000000000000 
Oct 31 20:04:45 node0 kernel: [ 1378.573349] 
Oct 31 20:04:45 node0 kernel: [ 1378.573867] Oops: 0000 [#1] SMP
Oct 31 20:04:45 node0 kernel: [ 1378.574435] Modules linked in: tun bridge stp llc dvk(O) iptable_filter fuse pcspkr joydev vmw_balloon serio_raw vmwgfx evdev ttm drm_kms_helper drm shpchp vmw_vmci sg ac button ip_tables x_tables autofs4 overlay ext4 crc16 jbd2 fscrypto mbcache sr_mod cdrom sd_mod ata_generic hid_generic usbhid hid psmouse uhci_hcd ata_piix xhci_pci xhci_hcd ehci_pci ehci_hcd libata pcnet32 mii usbcore mptspi scsi_transport_spi mptscsih mptbase i2c_piix4 scsi_mod
Oct 31 20:04:45 node0 kernel: [ 1378.579123] CPU: 0 PID: 757 Comm: dvs_uml_switch Tainted: G           O    4.9.88-DVK #159
Oct 31 20:04:45 node0 kernel: [ 1378.580225] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019
Oct 31 20:04:45 node0 kernel: [ 1378.582610] task: e123ea80 task.stack: e9ee0000
Oct 31 20:04:45 node0 kernel: [ 1378.583850] EIP: 0060:[<d22d0b4b>] EFLAGS: 00010246 CPU: 0
Oct 31 20:04:45 node0 kernel: [ 1378.585144] EIP is at __list_add+0xb/0x120 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< LIST ADD !
Oct 31 20:04:45 node0 kernel: [ 1378.587137] EAX: e9ee1d6c EBX: 00000000 ECX: e9ebf9e0 EDX: 00000000
Oct 31 20:04:45 node0 kernel: [ 1378.588915] ESI: e9ebf9dc EDI: e123ea80 EBP: e9ee1d88 ESP: e9ee1d3c
Oct 31 20:04:45 node0 kernel: [ 1378.590381]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
Oct 31 20:04:45 node0 kernel: [ 1378.591877] CR0: 80050033 CR2: 00000000 CR3: 22db1300 CR4: 000006f0
Oct 31 20:04:45 node0 kernel: [ 1378.593450] Stack:
Oct 31 20:04:45 node0 kernel: [ 1378.595003]  e123ea80 e9ebf9ec e123ea80 00000000 e9ebf9e8 00000000 e9ebf9d8 e9ec47d8
Oct 31 20:04:45 node0 kernel: [ 1378.596691]  e9ebf9d8 e9ebf9d8 d25bcd08 e9ebf9e0 e9ee1d8c d20ba8e7 00000000 26a25947
Oct 31 20:04:45 node0 kernel: [ 1378.598556]  e9ebf9d8 e9ec47d8 e9ec4600 e9ee1d94 d25bc51c e9ebf800 e9ee1df8 f896e471
Oct 31 20:04:45 node0 kernel: [ 1378.600389] Call Trace:
Oct 31 20:04:45 node0 kernel: [ 1378.602198]  [<d25bcd08>] ? __mutex_lock_slowpath+0x78/0x100
Oct 31 20:04:45 node0 kernel: [ 1378.604518]  [<d20ba8e7>] ? vprintk_default+0x37/0x40
Oct 31 20:04:45 node0 kernel: [ 1378.606446]  [<d25bc51c>] ? mutex_lock+0x1c/0x30
Oct 31 20:04:45 node0 kernel: [ 1378.608497]  [<f896e471>] ? do_unbind+0x136b/0x2721 [dvk]
Oct 31 20:04:45 node0 kernel: [ 1378.610763]  [<f8974c00>] ? new_unbind+0x284/0x1577 [dvk]
Oct 31 20:04:45 node0 kernel: [ 1378.612777]  [<d20ba4fe>] ? vprintk_emit+0x2ee/0x4f0
Oct 31 20:04:45 node0 kernel: [ 1378.614814]  [<f8969148>] ? io_unbind+0x4e/0xce [dvk]
Oct 31 20:04:45 node0 kernel: [ 1378.616934]  [<f89681fe>] ? dvk_ioctl+0x91/0x1a8 [dvk]
Oct 31 20:04:45 node0 kernel: [ 1378.619225]  [<f896816d>] ? dvk_write+0x49/0x49 [dvk]
Oct 31 20:04:45 node0 kernel: [ 1378.621881]  [<d21e6624>] ? do_vfs_ioctl+0x94/0x730
Oct 31 20:04:45 node0 kernel: [ 1378.624028]  [<d21d316d>] ? vfs_write+0x15d/0x1c0
Oct 31 20:04:45 node0 kernel: [ 1378.626252]  [<d21e6d20>] ? SyS_ioctl+0x60/0x70
Oct 31 20:04:45 node0 kernel: [ 1378.628431]  [<d2003708>] ? do_fast_syscall_32+0x98/0x160
Oct 31 20:04:45 node0 kernel: [ 1378.630682]  [<d25bf682>] ? sysenter_past_esp+0x47/0x75
Oct 31 20:04:45 node0 kernel: [ 1378.632827] Code: 00 89 ea c7 41 04 00 00 00 00 e9 ce fe ff ff 89 ea 83 cf 01 e9 c4 fe ff ff e8 82 69 d9 ff 66 90 53 83 ec 24 8b 59 04 39 d3 75 25 <8b> 1a 39 d9 75 6f 39 c2 0f 84 af 00 00 00 39 c1 0f 84 a7 00 00
Oct 31 20:04:45 node0 kernel: [ 1378.639663] EIP: [<d22d0b4b>] 

Oct 31 20:04:45 node0 kernel: [ 1378.639706] __list_add+0xb/0x120   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< LIST ADD ! 

Oct 31 20:04:45 node0 kernel: [ 1378.641897]  SS:ESP 0068:e9ee1d3c
Oct 31 20:04:45 node0 kernel: [ 1378.644018] CR2: 0000000000000000
Oct 31 20:04:45 node0 kernel: [ 1378.649965] ---[ end trace fbb25607f3cc277e ]---
Oct 31 20:04:45 node0 kernel: [ 1378.652663] do_exit: local_nodeid:0

==============================================================================================================
20201103:
		APARENTEMENTE EL PROBLEMA DEL UNBIND Y EVENTUALMENTE OTROS ESTA ES EN LAS SENTENCIAS:
		ERRONEA 
			rp = DC_PROC(dc_ptr,(_ENDPOINT_P(proc_ptr->p_usr.p_sendto) - dc_ptr->dc_usr.dc_nr_tasks)); <<<<< LA RESTA ES SUMA 
		CORRECTA 
			rp = ENDPOINT2PTR(dc_ptr, proc_ptr->p_usr.p_sendto);  <<<< ESTO EVITA ESE ERROR 

#define ENDPOINT2PTR(dc_ptr, ep) DC_PROC(dc_ptr,(_ENDPOINT_P(ep)+dc_ptr->dc_usr.dc_nr_tasks)) <<<<<<<<<<<<< SUMA, CORRECTO 


==============================================================================================================
DURANTE JULIO Y AGOSTO: Para LOAD BALANCER se tuvieron que modificar los codigos 
del modulo DVK para lo referente a proxies y acknowledges. Habia errores no detectados.

==============================================================================================================
20220128: MIGRACION A KERNEL 5.10.94

	Se creo la VM => DVS NODE0 kernel 5.10.94 (Debian 11)
	Se cargo Package linux-headers-5.10.0-10-686
	Se cargo Linux 
	https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.10.94.tar.xz
	Se copio el .config 
	root@node0a:~# make menuconfig
	root@node0a:~# sudo make -j 4 && sudo make modules_install -j 4 && sudo make install -j 4
	root@node0a:~# update-initramfs -c -k 5.10.94
	root@node0a:~# update-grub  
	root@node0a:~# update-grub2

	root@node0a:~# lsb_release -a
		No LSB modules are available.
		Distributor ID: Debian
		Description:    Debian GNU/Linux 11 (bullseye)
		Release:        11
		Codename:       bullseye
	root@node0a:~# uname -a
		Linux node0a 5.10.94 #1 SMP Fri Jan 28 12:23:15 -03 2022 i686 GNU/Linux

PROBLEMA:	No conservar lo relacionado al DVK en el .config !!! 
		# Linux/x86 4.9.88-DVK Kernel Configuration
		# CONFIG_DVKIPC is not set
		CONFIG_DVKIOCTL=y
		 

make menuconfig
sudo make -j 4 && sudo make modules_install -j 4 && sudo make install -j 4
update-initramfs -c -k 5.10.94-DVKIPC
update-grub  
update-grub2

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
HAY PROBLEMAS CON LOS DISPOSITIVOS Y LOS FILESYSTEMS
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


==============================================================================================================
20220206: 
	ESTA FALLANDO EL BIND DE UN THREAD DONDE 1178 ES EL THREAD Y 1175 ES EL MAIN 
		Feb  6 19:53:45 node0 kernel: [ 3186.053625] DEBUG 1178:dvk_ioctl:349: cmd=4004E309 arg=B73F02A8
		Feb  6 19:53:45 node0 kernel: [ 3186.053626] DEBUG 1178:dvk_ioctl:369: DVK_CALL=9 (io_bind) 
		Feb  6 19:53:45 node0 kernel: [ 3186.053626] DEBUG 1178:io_bind:97: 
		Feb  6 19:53:45 node0 kernel: [ 3186.053627] DEBUG 1178:new_bind:1605: oper=1 dcid=0 param_pid=1178 endpoint=21 nodeid=-1
		Feb  6 19:53:45 node0 kernel: [ 3186.053628] DEBUG 1178:new_bind:1628: dc_ptr=f8a0b000
		Feb  6 19:53:45 node0 kernel: [ 3186.053628] DEBUG 1178:new_bind:1630: RLOCK_DC dc=0 count=0
		Feb  6 19:53:45 node0 kernel: [ 3186.053629] DEBUG 1178:new_bind:1643: proc_ptr=ef826e00
		Feb  6 19:53:45 node0 kernel: [ 3186.053629] DEBUG 1178:new_bind:1644: WLOCK_PROC ep=21 count=0
		Feb  6 19:53:45 node0 kernel: [ 3186.053630] DEBUG 1178:new_bind:1664: WUNLOCK_PROC ep=21 count=0
		Feb  6 19:53:45 node0 kernel: [ 3186.053631] DEBUG 1178:new_bind:1665: RUNLOCK_DC dc=0 count=0
		Feb  6 19:53:45 node0 kernel: [ 3186.053631] ERROR: 1178:new_bind:1665: rcode=-305
		Feb  6 19:53:45 node0 kernel: [ 3186.053632] ERROR: 1178:dvk_ioctl:373: rcode=-305
		Feb  6 19:53:45 node0 kernel: [ 3186.062339] DEBUG 1175:new_wait4bind:2635: ret=-512
		Feb  6 19:53:45 node0 kernel: [ 3186.062340] DEBUG 1175:new_wait4bind:2637: pending: sig[0]:0x00000100, sig[1]:0x00000000
		Feb  6 19:53:45 node0 kernel: [ 3186.062341] DEBUG 1175:new_wait4bind:2640: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
		Feb  6 19:53:45 node0 kernel: [ 3186.062341] ERROR: 1175:new_wait4bind:2653: rcode=-4
		Feb  6 19:53:45 node0 kernel: [ 3186.062342] ERROR: 1175:dvk_ioctl:373: rcode=-4

==============================================================================================================
20220206: 
TODO:
		Cuando se migra desde un NODO1 a NODE2 y el cliente esta en NODE0 
		root@node0:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
			DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
			 0  10    10    -1/-1     2 1000 1000 27342 27342 27342 27342 migr_server    
			 0  11    11 29176/25     0    8 2020    10 27342 27342 27342 migr_client 
		Aparentemente el proceso cliente, cuando se hace migr_start queda congelado en la parte 
		SEND del SENDREC 
		misc = 2020 = MIS_GRPLEADER | MIS_ENQUEUED
		getf = 10 => Esta esperando respuesta del server.
		flag = 8  => BIT_RECEIVING

LISTO ARREGLADO:		
	EL PROCESO LOCAL test_receive ESTA ESPERANDO MENSAJE DEL ENDPOINT REMOTO 20
	root@node1:~# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  20    20    -1/-1     0 1000    0 27342 27342 27342 27342 server         
	 0  21    21   695/695    1    8   20    20 27342 27342 27342 test_receive   

	CUANDO MATAMOS AL PROXY LOCAL 
	root@node1:~# killall multi_proxy
	
	SE RESTITUYE EL FLAG A 0 Y EL GETF A NONE 
	root@node1:~# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  20    20    -1/-1     0 1000    0 27342 27342 27342 27342 server         
	 0  21    21   695/695    1    0   20 27342 27342 27342 27342 test_receive

==============================================================================================================
20220213:
		SE CAMBIO LA VERSION A FORMATO YYYYMM (com/config.h DVS_VERSION)

		root@node0:~# dmesg | grep dvs
		[    7.106794] DEBUG 664:dvk_ioctl:369: DVK_CALL=24 (io_dvs_init) 
		[    7.106795] DEBUG 664:io_dvs_init:261: 
		[    7.106797] DEBUG 664:new_dvs_init:53: d_name=DVS_IPC d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		[    7.106798] DEBUG 664:new_dvs_init:54: d_max_copybuf=65536 d_max_copylen=1048576
		[    7.106799] DEBUG 664:new_dvs_init:55: d_dbglvl=FFFFFFFF version=202202 flags=1 sizeof(proc)=0
		[    7.106801] DEBUG 664:new_dvs_init:64: d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		[    7.106802] DEBUG 664:new_dvs_init:65: d_max_copybuf=65536 d_max_copylen=1048576
		[    7.106803] DEBUG 664:new_dvs_init:66: d_dbglvl=0 version=0 flags=0 sizeof(proc)=0
		root@node0:~# cat /proc/dvs/info 
		name=TEST_CLUSTER
		nodeid=0
		nr_dcs=32
		nr_nodes=32
		max_nr_procs=221
		max_nr_tasks=35
		max_sys_procs=64
		max_copy_buf=65536
		max_copy_len=1048576
		dbglvl=0
		version=202202 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
		flags=101
		sizeof(proc)=452
		sizeof(proc) aligned=512
		sizeof(dc)=164
		sizeof(node)=92

==============================================================================================================
20220214:
LISTO:   Hacer que cuando hay batch en los proxies no haya compresion.
		Esto quiere decir que puede haber batch y puede haber compresion
		pero no puede comprimirse batched commands
		compresion solo de datos 
		buscar texto: NO_BATCH_COMPRESSION
	FALTA PROBAR 
	
	
	
ULTIMO_LOG
==============================================================================================================
==============================================================================================================
==============================================================================================================
==============================================================================================================

OPCIONES PARA VER INFO DE LA VM
   nproc => numero de procesadores 
   lscpu => info sobre la CPU
   free  => utilizacion de la memoria
   
   


DEFINIR LAS NUEVAS PRIMITIVAS QUE TRANSPORTEN MENSAJES Y DATOS 
PARA USO DE RPC Y DE CONVERSION DE PROTOCOLOS COMO HTML->M3IPC

len = dvk_rqst_data(dest,msg,ptr, buflen, timeout);
	Envia un mensaje al destino que puede o no contener datos 
	luego se bloquea
	Recibe la respuesta en un mensaje que puede o no contener datos 
	retorna
		len < 0 => ERROR
		len = 0 => mensajes OK pero sin datos 
		len > 0 => mensajes OK y la longitud de los datos 
len = dvk_rcv_data(src, msg, ptr, buflen, timeout);
	Recibe un mensaje desde un origen que puede o no contener datos 
		len < 0 => ERROR
		len = 0 => mensajes OK pero sin datos 
		len > 0 => mensajes OK y la longitud de los datos	
rcode = dvk_reply_data(dest,msg, ptr, buflen, timeout);
	Responde a un destino bloqueado con un mensaje que puede o no contener datos 
	
El area de datos trasportada es unica pero puede contener datos de diferentes formatos, ejemplo.

struct {
	int header;
	char pay[10];
} struct1;

struct {
	long IPaddr;
	char name[20];
} struct2;

struct {
	struct1 s1;
	struct2 s2;
} dualstruct;
typedef dualstruct dualstruct_t;

dualstruct_t ds;

en el rqst_data iria un mensaje, por ejemplo:
 m_ptr->m_type = TEST_STRUCTS
 m_ptr->m_m1i1 = sizeof(s1);
 m_ptr->m_m1i2 = sizeof(s2);
 m_ptr->m_m1p1 = &ds - &ds.s1; << es un offset respecto a la ubicacion del buffer ds.
 m_ptr->m_m1p2 = &ds - &ds.s2; << es un offset respecto a la ubicacion del buffer ds.
 
Cuando el receptor recibe el mensaje, tambien recibe los datos en el la direccion ptr.
y puede acceder a ellos porque tiene el offset respecto a ptr y
la longitud de cada uno 

HAY QUE MODIFICAR LOS PROXIES TAMBIEN Y get2rmt , put2lcl




	







NO SE ESTA ACTUALIZANDO LAS TRANSFERENCIA REMOTA DE DATOS rcopy
	root@client11:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats
	DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
	 0  20    20    -1/-1     0        0        0        0        0 m3ftpd0        
	 0  21    21    -1/-1     0        0        0        0        0 m3ftpd1        
	 0  22    22    -1/-1     0        0        0        0        0 m3ftpd2        
	 0  23    23    -1/-1     0        0        0        0        0 m3ftpd3        
	 0  24    24    -1/-1     0        0        0        0        0 m3ftpd4        
	 0  25    25    -1/-1     0        0        0        0        0 m3ftpd5        
	 0  26    26    -1/-1     0        0        0        0        0 m3ftpd6        
	 0  27    27    -1/-1     0        0        0        0        0 m3ftpd7        
	 0  28    28    -1/-1     0        0        0        0        0 m3ftpd8        
	 0  29    29    -1/-1     0        0        0        0        0 m3ftpd9        
	 0  60    60   665/665   11        0       75        0        0 m3ftp          
	 0  61    61   666/666   11        0       75        0        0 m3ftp          
	 0  62    62   667/667   11        0       75        0        0 m3ftp          
	 0  63    63   668/668   11        0       75        0        0 m3ftp          
	 0  64    64   669/669   11        0       75        0        0 m3ftp          
	 0  65    65   670/670   11        0       75        0        0 m3ftp          
	 0  66    66   671/671   11        0       75        0        0 m3ftp          
	 0  67    67   672/672   11        0       75        0        0 m3ftp          
	 0  68    68   673/673   11        0       75        0        0 m3ftp 

TODO:  SIGUEN QUEDANDO EN LOS DESCRIPTORES REMOTOS 
				LOS VALORES DE PROCESOS LOCALES DESPUES DE HABERLOS MATADO 
				EN ESTE EJEMPLO 20 25 24  26 ETC 
		root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
		DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		 0  60    60    -1/-1     0 1000    0    20 27342 27342 27342 m3ftp0         
		 0  61    61    -1/-1     0 1000    0    25 27342 27342 27342 m3ftp1         
		 0  62    62    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp2         
		 0  63    63    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp3         
		 0  64    64    -1/-1     0 1000    0    24 27342 27342 27342 m3ftp4         
		 0  65    65    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp5         
		 0  66    66    -1/-1     0 1000    0    26 27342 27342 27342 m3ftp6         
		 0  67    67    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp7         
		 0  68    68    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp8         
		 0  69    69    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftp9 
	SE MODIFICO, FALTA PROBAR 
	
TODO 
		CUANDO UN PROXY ENVIA UN MENSAJE A UN ENDPOINT DE UN NODO Y ESTO DA ERROR DE DSTDEAD O ALGO POR EL ESTILO
		EL PROXY DEBE HACER EL UNBIND LOCAL 

TODO: Falta actualizar las estadisticas en los proceso REMOTOS 

		
TODO:	ES NECESARIO QUE EL WAIT4BIND O WAIT4UNBIND SIEMPRE ESPEREN POR UN PROCESO LOCAL ¿¿¿??

TODO:   COMO HACER UN BIND AUTOMATICO ???
		HACER UN GRUPO SPREAD POR CADA DOMINIO 
		CADA SERVER QUE SE LEVANTA REGISTRA SU ENDPOINT, TIPO DE ENDPOINT Y NODO 
		Y SE BROADCASTEA A TODOS 
		LOS CLIENTES REGISTRAN ESOS ENDPOINTS EN EL DVK 
		SI SE CAE ALGUN ENDPOINT (DISCONNECT) LO UNBINDEA
		SI SE CAE UN HOST (NETWORK PARTTION) TODOS LOS ENDPOINTS DE UN NODO LOS DESCONECTA 
		QUIZAS ESTO SE PUEDA EMBEBER EN EL DVK ALGO ASI COMO EL PROXY PROPIO 
		
		OTRA FORMA ES QUE SI EL CLIENTE HACE UN SENDREC( SVR_EP, MSG), ESTE ENVIA UN MULTICAST PARA SABER
		DONDE ESTA ES ENDPOINT 
		
		OTRA FORMA ES QUE SI EL CLIENTE HACE UN SENDREC( SVR_EP, MSG) Y ESTE DA ERROR EDVSUNBIND 
		ENVIA UN MENSAJE TIPO CMD_PROCINFO POR TODOS LOS PROXIES
		SOLO EL PROXY QUE TIENE EL ENDPOINT COMO LOCAL PERMITE QUE EL PROXY HAGA EL RMTBIND 

		
		
	
		
		
		

 



 
		El migration COMMIT deberia finalizar con EDVSAGAIN O EDVSINTR 
		
TODO: Las estadisticas solo consideran a los procesos LOCALES, habria que extender a los procesos remotos. 

root@node2:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/stats 
DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
 0  10    10 27849/-1     2        0       16        0        0 DMTCP:migr_serv
 0  11    11    -1/-1     0        0        0        0        0 migr_client


TODO: Cuando se migra un proceso local a un remoto queda "vivo"
	root@node1:/usr/src/dvs/dvs-apps/dvs_run# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  10    10 29019/29019  2 1000   30 27342 27342 27342 27342 migr_server   <<<<< migrado a NODE2 pero el proceso aun esta vivo.  
	 0  11    11    -1/-1     0 1000    0 27342 27342 27342 27342 migr_client  
	Deberia terminar al proceso pero permanecer el endpoint registrado 


TODO: Marie configuro mal el archivo include.h 

typedef struct {long mBnr, mBmd5[MD5_SIZE];} mess_B; /////////////////////// ESTE ESTA MAL PORQUE HACE UN ARRAY DE long 
//typedef struct {long mBnr; char mBmd5[MD5_SIZE];} mess_B; <<<<<<<<<<<<<<<<<<<<<<<<<<<<< ESTE ESTA BIEN, PERO TEMPORALMENTE LO ANULO PARA HACER PRUEBAS CON LOAD BALANCER
// UNA VEZ PUESTO TODO BIEN HAY QUE RECOMPILAR TODO - Modulo de kernel, librerias, proxies y programas 
// al cambiar el TAMAÑO DE MENSAJE cambia tambien el tamaño del cmd_t usado en los proxies 
typedef struct {int mCi1, mCi2, mCi3, mCi4;char mCca1[MC_STRING];} mess_C;

HAY QUE CONFIGURAR BIEN LO DE MD5 y recompilar TODO.
KERNEL, LIBRERIA, aplicaciones y proxies

==============================================================================================================
==============================================================================================================
CAMBIO SUSTANTIVO
	Modificar los rastreos o scans de procesos por listas enlazadas
		FOR_EACH_PROC(dc_ptr, i) {
			rp = DC_PROC(dc_ptr,i);
		
	Por ejemplo, en el do_unbind hay que rastrea los procesos para ver si un proceso esta esperando:
		- recibir un mensaje del que esta finalizando
		- terminar una migracion
		- un bind o unbind 
		- realizando un vcopy
		
	Entonces, por ejemplo, cuando un proceso P1 de endpoint p1_ep hace un dvk_receive(p2_ep, msg)
	En el p_getfrom de P1: p2_ep (!= ANY) se debería hacer add_list(p2_ptr->p_recv_list, p1_ptr)
	Supongamos que P2 hace un dvk_receive(p3_ep, msg), P2 se coloca en la lista de P3 add_list(p3_ptr->p_recv_list, p2_ptr)	
	Pero si P2 hace un dvk_send(p1_ep, ) y p1->p_getfrom=p2_ep esta en la lista, ya sabe que hubo rendesvous
	y debe removerlo.
	la ventaja es que en el do_unbind, simplemente hay que notificar a los procesos de la lista de receive que 
	P2 ha muerto y no rastrear la lista completa para encontrar p->p_getfrom=p2_ep 
	
	
==============================================================================================================
==============================================================================================================



ATENCION: SEGUN VEO EN EL CODIGO, CUANDO UN PROXY RECEIVER HACE PUT2LCL se responden con ACKNOWLEDGES
SEND, NOTIFY, PERO NO SENDREC!! PORQUE ?? 
PORQUE FUNCIONA ??? SE SUPONE QUE TIENE LOS BITS SENDING Y RECEIVING PRENDIDOS
CREO QUE ES ASI: EL SENDING SE APAGA, CUANDO EL SENDER PROXY LO TOMA, Y EL RECEIVING SE APAGA CUANDO RECIBE UN SEND DEL REMOTO.
 

TEMA MIGRACION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

ERROR:
		Cuando de migra un proceso en el siguiente escenario:
		Antes
			NODE0: Nada
			NODE1: Client,Server
		Despues 
			NODE0: Server
			NODE1: Client
			
    En dvk_migrate, migr_start, cualquier proceso que haya ejecutado un IPC 
	send, sendrec, receive, wait4bind, etc, 
	deberia chequearese el bit misc  MIS_BIT_NEEDMIGR
	de esta forma aun cuando alguien le haya echo un UP, setearia el bit BIT_MIGRATE
	
	Es decir: si el proceso es local y esta ejecutando algo, cuando haga un RETURN
	con o sin error deberia verificarse el bit MIS_BIT_NEEDMIGR y setear el bit BIT_MIGRATE
	y quedar bloqueado.
	
RESUMIENDO:
		1) El proceso a migrar esta bloqueado en un IPC: Quien le haga UP debera chequear el bit y no hacer el UP.
		2) El propio proceso esta ejecutando y va a hacer el return, debe verificar ese bit antes
		
		Cuando se pone a dormir un proceso, deben verificarse la cola de los procesos que estan haciendo send al proceso migrante
		y 
		
EN long int migr_start(struct proc *proc_ptr)
		Se deberian rastrear todos los procesos que estan en SENDING en la cola del proceso migrante 
		para ponerlos en la cola de WAITING FOR OTHER PROCESS MIGRATION 
		O sino directamente devolverles un codigo de error y que el programa decida


CUANDO UN PROCESO LOCAL migro a un nodo REMOTE
SE EJECUTA 
long int old_node_migrate(struct proc *proc_ptr, int new_nodeid)

			/* Convert the LOCAL descriptor into a REMOTE descriptor */
			p_ptr->p_usr.p_lpid 	= PROC_NO_PID;	/* Update Linu PID	*/
			p_ptr->p_usr.p_vpid 	= PROC_NO_PID;	/* Update virtual PID	*/
			p_ptr->p_usr.p_nodeid 	= new_nodeid;
			thread_ptr->task_proc	= NULL;
			p_ptr->p_task 			= NULL;
			put_task_struct(thread_ptr);	 <<<<  /* decrement the reference count of the task struct */
			
	SEGURAMENTE el kernel libera la estructura thread_ptr y termina haciendo un unbind 
		y se pudre todo. 
		

			// ATENCION, ESTO NO ESTA RESUELTO 
			// Si el proceso es del sistema, puede tener acumulados mensajes de tipo NOTIFY 
			// Como transferirlos al remoto ??  (ver do_unbind()(	
			// ATENCION OTRO
			// que pasa con los mensajes que estan encolados en el proxy modo kernel listos para salir ??
			// Se deberia cambiar el nodo origne por new_node, pero el proxy receiver del otro nodo, lo 
			// deberia rechazar ya que el nodo origen no se corresponde con el nodo emisor 
			// Se deberia poner algun flag que indique que el proceso cambio de nodo
			// ATENCION !!! 
			// Ademas de envier mensajes y datos, se podria enviar al nodo remoto PROC_DESCRIPTOR 
			// ATENCION: Al migrar un proceso local hacia uno remoto, este queda como BACKUP 
			// Luego, cuando se hace el KILL del proceso BACKUP, tambien remueve al descriptor REMOTE!!! No deberia 
			
Cuando se hace old_node_migrate() y el proceso esta en estado MIS_BIT_NOMIGRATE
producto de que cuando se hace un send o sendrec o notify queda este bit seteado

PARA HACER UNA MIGRACION 
	1) El proceso esta fuera del DVK 
	2) El proceso esta dentro del DVK
	
	Si esta fuera del DVK no deberia haber ningun problema 
	Si esta dentro del DVK se deberia marcar con MIS_NEEDMIGR
		Si esta esperando un evento 


TEMA MIGRACION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
			
		
TODO:  SEGURIZAR EL CODIGO CON  
		BUG_ON(!list_empty(&new->list));
		Que seria equivalente a assert() 
		

TODO: 	MODIFICAR LOS PROXIES PARA QUE SI EL PAR QUEDA RENGO, SE REARRANCAN AMBOS 

PORQUE LOS PROXIES NO REGISTRAN LOS MENSAJES RECIBIDOS Y ENVIADOS ???
root@node1:/usr/src/dvs/dvs-apps/dvs_uml_switch# cat /proc/dvs/proxies/procs 
ID Type -lpid- -flag- -misc- -pxsent- -pxrcvd- -getf- -sendt -wmig- name
 0 send    703      0      1        0        0  27342  27342  27342 lz4tcp_proxy_ba
 0 recv    704      0      3        0        0  27342  27342  27342 lz4tcp_proxy_ba
 2 send    705      0      1        0        0  27342  27342  27342 lz4tcp_proxy_ba
 2 recv    707      0      1        0        0  27342  27342  27342 lz4tcp_proxy_ba


	ATENCION, NO HAY FORMA DE QUE SE CORTE LA COMUNICACION TCP AUN REARRANCANDO EL NODE1 

	TODO: PROBAR APLICACIONES CON IOCTL CON KERNEL CONFIG_DVKIOCTL
	DE ESTA FORMA SE PUEDEN PROBAR NUEVAS COSAS EN EL DVK.KO SIN TENER QUE RECOMPILAR EL KERNEL.
	TAMBIEN SE PUEDE PROBAR DMTCP PORQUE USA IOCTL 

TODO: RECOMPILAR EL KERNEL CON CONFIG_DVKIOCTL Y CONFIG_DVKIPC
TODO: PROBAR APLICACIONES CON IOCTL CON KERNEL CONFIG_DVKIOCTL
		



TODO:	Queda solucionar el tema de la carga del modulo para poder usar CONFIG_DVKIOCTL

Se implementa el CMD_PROCINFO el sproxy del CLIENT ---> SPROXY =====TCP=== RPROXY, lo informa como ERROR 
																			y lo reinserta en la cola del 
																			SPROXY.
																			Este obtiene el procinfo 
																			y lo envia al remoto
									RPROXY           < ==============TCP ==== 						
							lo inserta en el proceso 
							LOCAL 
		
Modificar la llamada dvk_procinfo que tenga un ultimo parametro que sea el nodeid:
			- LOCAL_NODEID (-1)
			- nodeid donde se quiere averiguar.
		1) el caller debe estar bindeado  ?? EN PRINCIPIO, SI! FACILITA LAS COSAS
				A FUTURO: puede utilizar la cola wait4bind()
				wait_queue_head_t p_wqhead;	/* LINUX process wait queue head		*/
			Darle un tratamiento similar al dvk_sendrec() y a la copia de datos remotos.
			
		3) El DVK call es 
			asmlinkage long new_getprocinfo_X(int dcid, int p_nr, struct proc_usr *proc_usr_ptr, int nodeid)
			caso particular LOCAL_NODEID
				#define new_getprocinfo(int dcid, int p_nr, struct proc_usr *proc_usr_ptr)
						new_getprocinfo_X(int dcid, int p_nr, struct proc_usr *proc_usr_ptr, LOCAL_NODEID)			
				
		4) El nuevo comando es CMD_PROCINFO 
				Cuando sale: 
						set_bit(BIT_SENDING, &caller_ptr->p_usr.p_rts_flags);
						set_bit(BIT_RECEIVING, &caller_ptr->p_usr.p_rts_flags);
						set_bit(BIT_RMTOPER, &caller_ptr->p_usr.p_rts_flags);
						set_bit(MIS_BIT_RMTINFO, &caller_ptr->p_usr.p_misc_flags);
						caller_ptr->p_usr.p_sendto  = p_nr;
						caller_ptr->p_usr.p_getfrom = p_nr;
		
				Los campos se completan
						caller_ptr->p_rmtcmd.c_cmd 		= CMD_PROCINFO;
						caller_ptr->p_rmtcmd.c_src 		= caller_ep;
						caller_ptr->p_rmtcmd.c_dst 		= p_nr;
						caller_ptr->p_rmtcmd.c_dcid		= caller_ptr->p_usr.p_dcid;
						caller_ptr->p_rmtcmd.c_snode  	= atomic_read(&local_nodeid);
						caller_ptr->p_rmtcmd.c_dnode  	= nodeid;
						caller_ptr->p_rmtcmd.c_rcode  	= OK;
						caller_ptr->p_rmtcmd.c_len  	= 0;
						
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_src   = caller_ep;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_dst   = p_nr;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_rqtr  = caller_ep;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_saddr = proc_usr_ptr;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_daddr = NULL;	
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_bytes = sizeof(proc_usr_t);
						
				EL PROXY SENDER, CUANDO EXTRAE EL MENSAJE CON get2rmt 
						clr_bit(BIT_SENDING, &caller_ptr->p_usr.p_rts_flags);
						caller_ptr->p_usr.p_sendto  = NONE;

		5) 	La vuelta trae 
						caller_ptr->p_rmtcmd.c_cmd 		= CMD_PROCINFO_ACK;
						caller_ptr->p_rmtcmd.c_src 		= p_nr;
						caller_ptr->p_rmtcmd.c_dst 		= caller_ep;
						caller_ptr->p_rmtcmd.c_dcid		= caller_ptr->p_usr.p_dcid;
						caller_ptr->p_rmtcmd.c_snode  	= atomic_read(&local_nodeid);
						caller_ptr->p_rmtcmd.c_dnode  	= nodeid; // del origen
						caller_ptr->p_rmtcmd.c_rcode  	= rcode;
						caller_ptr->p_rmtcmd.c_len  	= sizeof(proc_usr_t);	
						
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_src   = p_nr;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_dst   = caller_ep;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_rqtr  = caller_ep;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_saddr = NULL;		
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_daddr = proc_usr_ptr;	
						caller_ptr->p_rmtcmd.c_u.cu_vcopy.v_bytes = sizeof(proc_usr_t);
						
				EL PROXY RECEIVER:
					Si no hay error en el c_rcode, 
							copyin_rqst_rmt2lcl()
					Si hay error, no copia nada 		
						clr_bit(BIT_RECEIVING, &caller_ptr->p_usr.p_rts_flags);
						clr_bit(BIT_RMTOPER, &caller_ptr->p_usr.p_rts_flags);
						clr_bit(MIS_BIT_RMTINFO, &caller_ptr->p_usr.p_misc_flags);
						caller_ptr->p_usr.p_getfrom = NONE;
					retorna el codigo de error c_rcode
					

PROBAR EL KERNEL DEL HOST !! HUBO MUCHOS CAMBIOS 

TODO:		Modificar el formato del cmd_t de tal forma que tambien trasporte el PID del proceso local.
			Agregar los campos c_src_pid y c_dst_pid
			HAY que modificar dvk_sproxy.c 
			Si el tipo de comando NO ES UN ACK
				 c_ptr->c_src_pid = src_ptr->p_lpid;
				 c_ptr->c_dst_pid = PROC_NO_PID;
			Si el tipo de comando ES UN ACK
				 c_ptr->c_dst_pid = proc_ptr->p_lpid;
				 // c_ptr->c_src_pid SE MANTIENE EL ORIGINAL.

TODO:		SOPORTE DE PROCINFO EN PROXIES 	
			Comando:
				test_procinfo <dcid> <endpoint> [nodeid]
		Se podria modificar el 
			long dvk_getprocinfo_X(int dcid, int p_nr, proc_usr_t *p_usr, int nodeid);
			long dvk_getprocinfo(int dcid, int p_nr, proc_usr_t *p_usr) con NODEID=(-1);
		- Cuando el PROXY RECEIVER recibe un CMD_PROCINFO
		- responde OK, c_rcode y con proc_usr_t del proceso en payload si es que está bindeado.

		RESTRICCION: EL proceso que lo invoca debe estar bindeado de tal forma de tener asociada
			una cola de espera (por la respuesta). 


PROBLEMAS A RESOLVER:

1) De que manera el comando de usuario le pide al PROXY SENDER que solicite un  CMD_CHECKBIND

2) De que manera el PROXY RECEIVER del nodo del comando le informa al comando respecto al estado del process descriptor

	

	
TODO:	Hacer el nuevo dvk_getdcid() o modificar el getep() de tambien retornar el DCID 
		del PID de parametro.
		el new_getep() del kernel retornara tanto el endpoint como resultado 
		como el dcid como parametro x direccion.
		Si el &dcid es NULL no retorna el DCID.
		
		Entonces tendriamos 2 invocaciones: 
		ret = dvk_getep_X(pid, &dcid) // no usada por procesos de usuario 
		Por compatibilidad 
		ret = dvk_getep(pid)  => ret = dvk_getep_X(pid, NULL)
		Nuevo:
		ret = dvk_getep_dcid(pid, &dcid) => ret = dvk_getep_X(pid, &dcid) 
			
		RECOMPILAR TODO 
TODO:
		IDEM para wait4bind_X que no incluye el DCID.

TODO:	Habria que mejorar el tema del .config de tal forma de 
		CONFIG_DVKIPC es solo cuando se quiere compilar UML o KERNEL con soporte DVKIPC 
		CONFIG_DVKIOCTL es solo cuando se quiere compilar UML o KERNEL con soporte DVKIOCTL  
		nuevo CONFIG_UML_DVKIPC 	el cual solo se usaria en UML indica que los drivers usaran IPC 
		nuevo CONFIG_UML_DVKIOCTL 	el cual solo se usaria en UML indica que los drivers usaran IOCTL
		CONFIG_DVS ??? 
			Coincidencia en el fichero binario kernel/built-in.o
			Coincidencia en el fichero binario kernel/config.o
			kernel/skas/syscall.c:#ifdef CONFIG_DVS
			kernel/skas/syscall.c:#endif // CONFIG_DVS
			kernel/process.c:#ifdef CONFIG_DVS
			kernel/process.c:#endif //      CONFIG_DVS



TODO:		Modificicar los tipos de BIND que se usan en dvk_bind_X()

ACTUALMENTE SON:
#define SELF_BIND		0
#define LCL_BIND		1
#define RMT_BIND		2
#define BKUP_BIND		3
#define REPLICA_BIND	4
#define UNIKERNEL_BIND	5		// IDEM LOCAL pero set_bit(p_misc_flags, MIS_BIT_UNIKERNEL)
#define MAX_BIND_TYPE	UNIKERNEL_BIND


		
PROBLEMON:
			El problema es el siguiente. fundamentalmente en UML que seguro le manda un SIGALRM para que hacer
			el scheduling.
			De todos modos tambien es general.
			Si un proceso CLIENT hizo un sendrec() a un SERVER que hizo el receive() y el SERVER esta laburando
			, en la cola de procesos tratando de enviar del SERVER se inserta al proceso CLIENT.
			Si durante ese tiempo el CLIENT recibe un SIGNAL, pone su estado en RUNNING y retorna con -ERESTARTSYS.
			PERO LA LISTA DEL SERVER QUEDO CORRUPTA.
			
			EL CASO DE UML 
			SERVER esta en receive()
			CLIENT hace sendrec(), el SERVER se pone en RUNNING, y el CLIENT se pone RECEIVING
			CLIENTE recibe SIGNAL = -ERESTARTSYS 
			CLIENTE flags = RUNNING 
			y se vuelve a ejecutar sendrec()
			Ahora el SERVER no esta esperando, entonces el CLIENT se pone en la cola de sending del SERVER
			Cuando el server intenta hacer send() ve que el CLIENT no esta esperando el mensaje y se pone en la cola de CLIENT
			
POSIBLE SOLUCION:
			Dependiendo del lugar donde produjo el ERESTARTSYS, se setea un flag BIT_RESTART
			De esa forma, se dejan los flags anteriores en el mismo estado, pero se le agrega el BIT_RESTART.
			Una vez que se han comprobado todos los parametros de ingreso, 
		PROBLEMA: Ante cualquier error hay que salir borrando el BIT_RESTART 
		SOLUCION: Al entrar, luego que se ha chequeado, variable 
				lcl_restart = (BIT_RESTART)?1:0;
				clear_bit(BIT_RESTART)
				y luego seguir manejandose con el valor de lcl_restart;
		PROBLEMA: Que pasa si no vuelve a ejecutar la SYSCALL ??		
				
OTRA POSIBLE SOLUCION:
			Si la salidad del sleep_proc() es ERESTARTSYS entonces ver que signal fue la que lo provoco.
			si es una boludez, entonces ignorarla.
	
ATENCION: After the signal has been handled, the system call will be restarted (from the beginning), 
		and the user-space application need not deal with "interrupted system call" errors. 
		For cases where restarting is not appropriate, a -EINTR return status will cause a (post-signal) 
		return to user space without restarting the system call.	
	

TODO TODO TODO TODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO 

ATENCION!! EN dvk_calls.h estan los dvk_calls reales, y en kipc tambien estan definidos pero con los VOID 
Hay muchos programas que utilizan kipc.h !!!

NORMALIZAR TODO!!

TODO TODO TODO TODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO TODO TODOTODO 
	
	
TODO:  Probar proxies 

TODO:  Hacer un programa
		- test_proc_info  para ver los procesos
			test_priv_info <DC> <endpoint> 
			
			struct priv_usr {

  dvk_id_t 	priv_id;			/* index of this system structure */
  int		priv_warn;				/* process to warn when the process exit/fork */
  int		priv_level;			/* privilege level		*/

  short 	priv_trap_mask;		/* allowed system call traps */
  ipc_map_t priv_ipc_from;		/* allowed callers to receive from */
  ipc_map_t priv_ipc_to;		/* allowed destination processes */
  dvk_map_t	priv_dvk_allowed;		/* allowed dvk calls */

  dvktimer_t priv_alarm_timer;	/* synchronous alarm timer */ 

};

		- test_priv_info para ver los proxies 
		
		

TODO:   Incluir impresion de PRIVILEGIOS en /proc 
		- De Procesos 
			/proc/dvs/DCO/priv
			
		- De Proxies 
			root@node0:/usr/src/dvs/dvk-tests# cat /proc/dvs/proxies/priv
			ID Type -lpid- id  -warn- lvl ------name----- priv_dvk_allowed
														  3322222222221111111111000000000033222222222211111111110000000000	
			12 send 123456 12  123456 123 123456789012345 1098765432109876543210987654321010987654321098765432109876543210 	      
   	
	
		
		  dvk_id_t 	priv_id;			/* index of this system structure */
  int		priv_warn;				/* process to warn when the process exit/fork */
  int		priv_level;			/* privilege level		*/

  short 	priv_trap_mask;		/* allowed system call traps */
  ipc_map_t priv_ipc_from;		/* allowed callers to receive from */
  ipc_map_t priv_ipc_to;		/* allowed destination processes */
  dvk_map_t	priv_dvk_allowed;		/* allowed dvk calls */
  	
TODO:   dvk_sproxy. new_get2rmt()
		VERIFICAR SI SE HACE ESTO 
	ESTO HACE
		cuando el dvk detecta que el nodo del endpoint destino es diferente al que el cmd 
		retorna EDVSMIGRATE pero al send, sendrec, etc. 
	ESTO DEBERIA HACER 
		cuando el dvk detecta que el nodo del endpoint destino es diferente al que el cmd 
		tiene, entonces cambia el nodo en el cmd.
				
TODO:   dvk_rproxy. new_put2lcl()
		VERIFICAR SI SE HACE ESTO 
	ESTO HACE
		cuando el dvk detecta que el nodo del endpoint destino es diferente al que el cmd 
		retorna EDVSMIGRATE al nodoo remoto
	ESTO DEBERIA HACER 
		cuando el dvk detecta que el nodo del endpoint destino es diferente al que el cmd 
		tiene, entonces registra al endpoint remoto en el nuevo nodo.

TODO  ANALIZAR ESTAS SITUACIONES
Se pueden presentar las siguientes situaciones:
Si un proceso local ha enviado un mensaje hacia un endpoint de un proceso remoto, el DVK antes de entregárselo al proxy para su envío, verifica si el endpoint destino está registrado en un nodo diferente al del momento en que se envió el mensaje. De ser así, esto quiere decir que el proceso ha migrado. El DVK cambia el nodo destino original por el nodo destino actual.
Si un proceso local ha enviado un mensaje a un endpoint remoto y éste se encuentra en proceso de migración, el DVK verifica si el proceso (local) emisor ha establecido un tiempo de espera para enviar el mensaje (timeout). Si estableció un tiempo de espera, el proceso se bloquea durante ese tiempo y el DVK volverá a intentar el envío, de lo contrario, retornará un código de error EDVSMIGRATE.
Si el DVK del nodo local ha recibido a través de un proxy un mensaje desde un endpoint localizado en un nodo remoto y el DVK local lo tiene registrado en un nodo diferente, se cambiará en el nodo local la registración de ese endpoint en el nodo origen del mensaje.
Si el DVK del nodo local ha recibido a través de un proxy un mensaje desde un endpoint en un nodo remoto hacia un endpoint cuyo nodo no es el nodo local, le retorna al nodo remoto un mensaje EDVSMIGRATE.



		
TODO:  Probar Kernel con IPC y modulo con IOCTL juntos 

TODO:  Agregar al proceso, DC, DVS un campo de uptime que indique cuando se arranco.
		Agregar un campo de p_clr_time que indica cuando se limpiaron las estadisticas. 
		Tambien se deberia hacer un dvk_clrprocstats() y dvk_clrproxystats()
		tambien se podria hacer mediante el /proc
		Para limpiar las estadisticas.
		
TODO:  Cuando se ejecuta desde un modulo se debe prender el bit MIS_BIT_KTHREAD en p_misc_flags.
		Esto se hizo en el proxy de kernel 
		C:\PAP\UTN\INVESTIGACION\LAB-NTC\PROYECTOS\MOL-IPC\m3-ipc-mod\MoL_Module\ksocket-0.0.2\m3ipc_tcp_proxy
		PROBLEMA CON COPY_KRN_USR y viceversa.	
		ATENCION, hay que contemplar:
			- IPC: AMBOS ENDPOINTS ESTEN EN EL KERNEL
			- IPC: QUE UN ENDPOINT ESTEN EN EL KERNEL Y OTRO EN EL USER 
			- VCOPY: TODAS LAS COMBINACIONES USER-KERNEL PARA SRC-DST-RQTR
	
	POSIBLE SOLUCION: 
		Supongamos que un proceso de USER hace un dvk_receive() 
		queda esperando en modo kernel. 
		El proceso SRC es de KERNEL y con memcpy copia el mensaje al DST 
		y lo despierta, y este antes se fija si el emisor estaba KERNEL
		Si es asi, hace COPY_KRN_USR y luego vuelve.
		Es decir, hay una doble copia.


	/* enqueue the process descriptor at the TAIL of the sender proxy's caller_q */
	if( test_bit(BIT_SENDING, &proc_ptr->p_usr.p_rts_flags) 
	||  test_bit(BIT_RMTOPER, &proc_ptr->p_usr.p_rts_flags)) {
		DVKDEBUG(INTERNAL, "QUEUING ERROR " PROC_USR_FORMAT, PROC_USR_FIELDS(p_ptr));
	} 
	



