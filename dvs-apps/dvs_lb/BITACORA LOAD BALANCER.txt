BITACORA LOAD BALANCER	
===================

=====================================================================================
20210629:
		Cree el programa lb_dvs que es el main thread que:
		- inicializa spread
		- lee la configuracion del .cfg 
		- inicializa las estructuras de datos
			* sesiones (una tabla de sesiones por DC)
			* servers  (segun archivo de config) 
			* clients  (segun archivo de config)
		- por cada server 
			* arranca proxy sender como thread 
			* arranca proxy receiver como thread 
		- por cada client 
			* arranca proxy sender como thread 
			* arranca proxy receiver como thread 
		- arranca el thread del lb_monitor

		- hace el join del lb_monitor
		- hace el join de todos los proxies clients
		- hace el join de todos los proxies servers
			
		- Todos los threads, hasta ahora estan vacios
		

TODO:	Crear las message queues para cada sender proxy 

=====================================================================================
20210703:
		Ya hice SENDER y RECEIVER del SERVER 
		
		Incorpore el receive de la msgq en el SENDER 
		Incorpore el send a la msgq en el RECEIVER 

TODO:
		Hay que determinar cual es la sesion activa 
		Hay que modificar los campos del header para realizar el engaño.		
		En principio, crear una sesion fija y que obtenga los datos de alli.
		
ERROR: lb_svrpxy.c:svr_Sproxy_connect:364: rcode=-113 << 113	EHOSTUNREACH	No route to host
ERROR: lb_svrpxy.c:svr_Sproxy_connect:364: rcode=-111 << 111	ECONNREFUSED	Connection refused


EN NODE1 (SERVER)
tcp        0      0 192.168.0.101:3000      192.168.0.100:33612     ESTABLISHED <<<<< 
tcp        0      0 192.168.0.101:56204     192.168.0.100:3001      ESTABLISHED <<<<<
tcp        0      1 192.168.0.101:52054     192.168.0.102:3001      SYN_SENT   
tcp        0     52 192.168.0.101:22        192.168.0.196:60131     ESTABLISHED
		

EN NODE0 (LB) 
tcp        0      0 192.168.0.100:33612     192.168.0.101:3000      ESTABLISHED <<<<
tcp        0      0 192.168.0.100:3001      192.168.0.101:56204     ESTABLISHED <<<<< 

 998:lb_svrpxy.c:svr_Rproxy:26:SERVER_RPROXY[node1]: Initializing...
 998:lb_svrpxy.c:svr_Rproxy_init:39:SERVER_RPROXY(node1): Initializing proxy receiver
 998:lb_svrpxy.c:svr_Rproxy_setup:62:SERVER_RPROXY(node1): for node 1 running at port=3001
 998:lb_svrpxy.c:svr_Rproxy_setup:80:SERVER_RPROXY(node1): is bound to port=3001 socket=3
 998:lb_svrpxy.c:svr_Rproxy_loop:105:SERVER_RPROXY(node1): Waiting for connection.
 
 997:lb_svrpxy.c:svr_Sproxy:282:SERVER_SPROXY[0]: Initializing...
 997:lb_svrpxy.c:svr_Sproxy_init:296:SERVER_SPROXY(node1): Initializing proxy sender
 
 998:lb_svrpxy.c:svr_Rproxy_loop:111:SERVER_RPROXY(node1): [192.168.0.101]. Getting remote command.
 998:lb_svrpxy.c:svr_Rproxy_getmsg:145:SERVER_RPROXY(node1): 
 998:lb_svrpxy.c:svr_Rproxy_getmsg:150:SERVER_RPROXY(node1): About to receive header
 997:lb_svrpxy.c:svr_Sproxy_connect:344:SERVER_SPROXY(node1): remote host port=3000
 997:lb_svrpxy.c:svr_Sproxy_connect:354:SERVER_SPROXY(node1): remote host address 0: 192.168.0.101
 997:lb_svrpxy.c:svr_Sproxy_connect:362:SERVER_SPROXY(node1): running at IP=192.168.0.101
 997:lb_svrpxy.c:svr_Sproxy_serving:378:SERVER_SPROXY(node1): Reading message queue..
 
 997:lb_svrpxy.c:svr_Sproxy_mqrcv:523:SERVER_SPROXY(node1): reading from message queue

 998:lb_svrpxy.c:svr_Rproxy_rcvhdr:208:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 998:lb_svrpxy.c:svr_Rproxy_rcvhdr:211:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 998:lb_svrpxy.c:svr_Rproxy_rcvhdr:213:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 998:lb_svrpxy.c:svr_Rproxy_getmsg:164:SERVER_RPROXY(node1): NONE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< PERFECTO!!
 998:lb_svrpxy.c:svr_Rproxy_getmsg:150:SERVER_RPROXY(node1): About to receive header


=====================================================================================
20210704:

 875:lb_cltpxy.c:clt_Sproxy_connect:345:CLIENT_SPROXY(node2): remote host port=3000
 875:lb_cltpxy.c:clt_Sproxy_connect:354:CLIENT_SPROXY(node2): rmthost node2 IP 192.168.0.102
 875:lb_cltpxy.c:clt_Sproxy_connect:371:CLIENT_SPROXY(node2): running at IP=192.168.0.102
 875:lb_cltpxy.c:clt_Sproxy_serving:387:CLIENT_SPROXY(node2): Reading message queue..
 875:lb_cltpxy.c:clt_Sproxy_mqrcv:532:CLIENT_SPROXY(node2): reading from message queue
  
 seria esta sesion en  NODE0
 tcp        0      0 192.168.0.100:34164     192.168.0.102:3000      ESTABLISHED
 tcp        0      0 192.168.0.100:3002      192.168.0.102:60372     ESTABLISHED

 en NODE2 
 tcp        0      0 192.168.0.102:3000      192.168.0.100:34164     ESTABLISHED
 tcp        0      0 192.168.0.102:60372     192.168.0.100:3002      ESTABLISHED
 
 RECIBO LOS NONE DE EN AMBOS PROXY RECEIVERS
 874:lb_svrpxy.c:svr_Rproxy_getmsg:164:SERVER_RPROXY(node1): NONE
 876:lb_cltpxy.c:clt_Rproxy_getmsg:165:CLIENT_RPROXY(node2): NONE
 
=====================================================================================
20210705:

NODE0 

NODE1
run # . /dev/shm/DC0.sh
root@node1:/usr/src/dvs/dvk-tests# . /dev/shm/DC0.sh 
root@node1:/usr/src/dvs/dvk-tests# ./test_rmtbind 0 11 0 lb_client
root@node1:/usr/src/dvs/dvs-apps/dvs_run# nsenter -p -t$DC0 ./migr_server 0 10 &

NODE2
oot@node2:~# cd /usr/src/dvs/dvk-tests/
root@node2:/usr/src/dvs/dvk-tests# ./tests.sh 2 0
root@node2:/usr/src/dvs/dvk-tests# . /dev/shm/DC0.sh 
root@node2:/usr/src/dvs/dvk-tests# ./test_rmtbind 0 10 0 lb_server
DEBUG 709:dvk_open:108: Open dvk device file /dev/dvk
 dvk_rmtbind lb_server with p_nr=10 to DC0 on node=0
DEBUG 709:dvk_bind_X:1221: cmd=2 dcid=0 pid=-1077023163 endpoint=10 nodeid=0
DEBUG 709:dvk_bind_X:1235: ioctl ret=10 errno=0
DEBUG 709:dvk_bind_X:1248: ret=10 errno=0
DEBUG 709:dvk_bind_X:1253: ioctl ret=10
DEBUG 709:dvk_bind_X:1256: ret=10
root@node2:/usr/src/dvs/dvk-tests# cd ..
root@node2:/usr/src/dvs# cd dvs-apps/dvs_run/
root@node2:/usr/src/dvs/dvs-apps/dvs_run# nsenter -p -t$DC0  ./migr_client 0 11 10 1024 10 1

NODE0 
root@node0:/usr/src/dvs/dvs-apps/lb_dvs# ./lb_dvs lb_dvs.cfg > salida.out 2> salida.err

root@node0:/usr/src/dvs/dvs-apps/lb_dvs# ./lb_dvs lb_dvs.cfg > lb_dvs.out 2> lb_dvs.err
ViolaciÃ³n de segmento

 922:lb_cltpxy.c:clt_Rproxy_loop:119:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 922:lb_cltpxy.c:clt_Rproxy_getmsg:241:CLIENT_RPROXY(node2): 
 922:lb_cltpxy.c:clt_Rproxy_getmsg:246:CLIENT_RPROXY(node2): About to receive header
 922:lb_cltpxy.c:clt_Rproxy_rcvhdr:303:CLIENT_RPROXY(node2): lbp_csd=4 
 920:lb_svrpxy.c:svr_Rproxy_rcvhdr:208:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 920:lb_svrpxy.c:svr_Rproxy_rcvhdr:211:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 920:lb_svrpxy.c:svr_Rproxy_rcvhdr:213:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 920:lb_svrpxy.c:svr_Rproxy_getmsg:164:SERVER_RPROXY(node1): NONE
 920:lb_svrpxy.c:svr_Rproxy_getmsg:150:SERVER_RPROXY(node1): About to receive header
 922:lb_cltpxy.c:clt_Rproxy_rcvhdr:310:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 922:lb_cltpxy.c:clt_Rproxy_rcvhdr:313:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 922:lb_cltpxy.c:clt_Rproxy_rcvhdr:315:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=703 c_ack_seq=92
 922:lb_cltpxy.c:clt_Rproxy_getmsg:248:CLIENT_RPROXY(node2): header bytes=132
 922:lb_cltpxy.c:clt_Rproxy_getmsg:256:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 922:lb_cltpxy.c:clt_Rproxy_getmsg:258:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=703 c_ack_seq=92
 922:lb_cltpxy.c:clt_Rproxy_getmsg:260:CLIENT_RPROXY(node2): c_timestamp=1625451424.768984782
 922:lb_cltpxy.c:clt_Rproxy_getmsg:262:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=703 c_dst_pid=92
 922:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.
 922:lb_cltpxy.c:clt_Rproxy_2server:168:CLIENT_RPROXY(node2): 
 922:lb_cltpxy.c:clt_Rproxy_2server:221:CLIENT_RPROXY(node2): NEW Session Found with server node1
 922:lb_cltpxy.c:clt_Rproxy_2server:223:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 922:lb_cltpxy.c:clt_Rproxy_2server:225:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=0 se_svr_ep=10 se_svr_PID=-1
 922:lb_cltpxy.c:clt_Rproxy_2server:227:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 922:lb_cltpxy.c:clt_Rproxy_svrmq:365:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=0 rcode=0 len=0
 922:lb_cltpxy.c:clt_Rproxy_svrmq:370:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 
=====================================================================================
20210706:

838:lb_cltpxy.c:clt_Rproxy_rcvhdr:314:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 838:lb_cltpxy.c:clt_Rproxy_rcvhdr:317:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 838:lb_cltpxy.c:clt_Rproxy_rcvhdr:319:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 838:lb_cltpxy.c:clt_Rproxy_getmsg:252:CLIENT_RPROXY(node2): header bytes=132
 838:lb_cltpxy.c:clt_Rproxy_getmsg:260:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 838:lb_cltpxy.c:clt_Rproxy_getmsg:262:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 838:lb_cltpxy.c:clt_Rproxy_getmsg:264:CLIENT_RPROXY(node2): c_timestamp=1625587479.967510719
 838:lb_cltpxy.c:clt_Rproxy_getmsg:266:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=708 c_dst_pid=92
 838:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.
 838:lb_cltpxy.c:clt_Rproxy_2server:172:CLIENT_RPROXY(node2): 
 838:lb_cltpxy.c:clt_Rproxy_2server:225:CLIENT_RPROXY(node2): NEW Session Found with server node1
 838:lb_cltpxy.c:clt_Rproxy_2server:227:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 838:lb_cltpxy.c:clt_Rproxy_2server:229:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 838:lb_cltpxy.c:clt_Rproxy_2server:231:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 838:lb_cltpxy.c:clt_Rproxy_svrmq:369:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 838:lb_cltpxy.c:clt_Rproxy_svrmq:374:CLIENT_RPROXY(node2): sending 132 bytes to server (null) by mqid=0
 838:lb_cltpxy.c:clt_Rproxy_loop:119:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 838:lb_cltpxy.c:clt_Rproxy_getmsg:245:CLIENT_RPROXY(node2): 
 838:lb_cltpxy.c:clt_Rproxy_getmsg:250:CLIENT_RPROXY(node2): About to receive header
 838:lb_cltpxy.c:clt_Rproxy_rcvhdr:307:CLIENT_RPROXY(node2): lbp_csd=4
 
 835:lb_svrpxy.c:svr_Sproxy_mqrcv:609:SERVER_SPROXY(node1): 132 bytes received
 835:lb_svrpxy.c:svr_Sproxy_serving:465:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 835:lb_svrpxy.c:svr_Sproxy_serving:467:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 835:lb_svrpxy.c:svr_Sproxy_send:490:SERVER_SPROXY(node1): BEFORE cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 835:lb_svrpxy.c:svr_Sproxy_send:492:SERVER_SPROXY(node1): BEFORE c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 835:lb_svrpxy.c:svr_Sproxy_send:499:SERVER_SPROXY(node1): AFTER  cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 835:lb_svrpxy.c:svr_Sproxy_send:501:SERVER_SPROXY(node1): AFTER  c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 835:lb_svrpxy.c:svr_Sproxy_sndhdr:530:SERVER_SPROXY(node1): send header=132 
 835:lb_svrpxy.c:svr_Sproxy_sndhdr:534:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 835:lb_svrpxy.c:svr_Sproxy_sndhdr:536:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=708 c_ack_seq=92
 835:lb_svrpxy.c:svr_Sproxy_sndhdr:554:SERVER_SPROXY(node1): sent header=132 
 835:lb_svrpxy.c:svr_Sproxy_serving:458:SERVER_SPROXY(node1): Reading message queue..
 835:lb_svrpxy.c:svr_Sproxy_mqrcv:605:SERVER_SPROXY(node1): reading from mqid=0
 
 836:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 836:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66 <<<  CMD_COPYIN_DATA
 836:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=694 c_ack_seq=92
 836:lb_svrpxy.c:svr_Rproxy_getmsg:218:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66 <<< 66 ?????
 836:lb_svrpxy.c:svr_Rproxy_getmsg:220:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=694 c_ack_seq=92
 836:lb_svrpxy.c:svr_Rproxy_getmsg:222:SERVER_RPROXY(node1): c_timestamp=1625587479.995967781
 836:lb_svrpxy.c:svr_Rproxy_getmsg:224:SERVER_RPROXY(node1): c_flags=0x2 c_src_pid=694 c_dst_pid=92
 836:lb_svrpxy.c:svr_Rproxy_getmsg:238:SERVER_RPROXY(node1):src=10 dst=11 rqtr=10 saddr=0xb74a9000 daddr=0xb7477000 bytes=1024  <<<< 1024
 836:lb_svrpxy.c:svr_Rproxy_rcvpay:295:SERVER_RPROXY (node1): pl_size=66
 836:lb_svrpxy.c:svr_Rproxy_rcvpay:300:SERVER_RPROXY (node1): n:66 | received:66
 836:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 836:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
 836:lb_svrpxy.c:svr_Rproxy_2server:184:SERVER_RPROXY(node1): Active Session Found with client (null)
 836:lb_svrpxy.c:svr_Rproxy_2server:186:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 836:lb_svrpxy.c:svr_Rproxy_2server:188:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 836:lb_svrpxy.c:svr_Rproxy_2server:190:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 

SE CAMBIO de tal forma que el archivo de configuracion cada vez que hay un nodeid
es el index de las tablas server_tab y client_tab y no solamente un campo.
Es decir para localizar al server de nodeid=8 => server_tab[8]

NODE1 - SERVER 
 2:migr_server.c:main:204:SERVER RECEIVE:source=11 type=16077022 m1i1=1024 m1i2=0 m1i3=0 m1p1=0xb74b8000 m1p2=(nil) m1p3=(nil) 
DEBUG 2:dvk_vcopy:126: src_ep=10 dst_ep=11 bytes=1024
DEBUG 2:dvk_vcopy:142: ioctl ret=-1 errno=61
ERROR: 2:dvk_vcopy:145: rcode=-61
DEBUG 2:dvk_vcopy:154: ioctl ret=-61 <<<<<<<<<<<<<<<< EDVSTIMEDOUT
ERROR: 2:dvk_vcopy:157: rcode=-61

NODE2 - CLIENT
 2:migr_client.c:main:160:CLIENT SEND:source=0 type=16077022 m1i1=1024 m1i2=0 m1i3=0 m1p1=0xb74b8000 m1p2=(nil) m1p3=(nil) 
DEBUG 2:dvk_sendrec_T:931: endpoint=10 timeout=30000
DEBUG 2:dvk_sendrec_T:944: ioctl ret=-1 errno=64
ERROR: 2:dvk_sendrec_T:946: rcode=-64
DEBUG 2:dvk_sendrec_T:953: ioctl ret=-64
ERROR: 2:dvk_sendrec_T:955: rcode=-64
ERROR: migr_client.c:main:165: rcode=-64 
ERROR: migr_client.c:main:165: rcode=-64 <<<<<<<<<<<<< EDVSNOTCONN

NODE0- LB

SENDREC DESDE CLIENT->CLIENT_PROXY  
1304:lb_cltpxy.c:clt_Rproxy_rcvhdr:316:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 1304:lb_cltpxy.c:clt_Rproxy_rcvhdr:319:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 1304:lb_cltpxy.c:clt_Rproxy_rcvhdr:321:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:254:CLIENT_RPROXY(node2): header bytes=132
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:262:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:264:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:266:CLIENT_RPROXY(node2): c_timestamp=1625612995.320769029
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:268:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=705 c_dst_pid=92
 1304:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.
 1304:lb_cltpxy.c:clt_Rproxy_2server:171:CLIENT_RPROXY(node2): 
 1304:lb_cltpxy.c:clt_Rproxy_2server:227:CLIENT_RPROXY(node2): NEW Session with server node1
 1304:lb_cltpxy.c:clt_Rproxy_2server:229:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 1304:lb_cltpxy.c:clt_Rproxy_2server:231:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 1304:lb_cltpxy.c:clt_Rproxy_2server:233:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 1304:lb_cltpxy.c:clt_Rproxy_svrmq:371:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 1304:lb_cltpxy.c:clt_Rproxy_svrmq:376:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=65538
  1304:lb_cltpxy.c:clt_Rproxy_loop:119:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:247:CLIENT_RPROXY(node2): 
 1304:lb_cltpxy.c:clt_Rproxy_getmsg:252:CLIENT_RPROXY(node2): About to receive header
 1304:lb_cltpxy.c:clt_Rproxy_rcvhdr:309:CLIENT_RPROXY(node2): lbp_csd=4 
 
 SENDREC CLIENT_PROXY->MSGQ->SERVER_SPROXY
 1301:lb_svrpxy.c:svr_Sproxy_mqrcv:602:SERVER_SPROXY(node1): 132 bytes received
 1301:lb_svrpxy.c:svr_Sproxy_serving:465:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 1301:lb_svrpxy.c:svr_Sproxy_serving:467:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 1301:lb_svrpxy.c:svr_Sproxy_send:489:SERVER_SPROXY(node1): 
 1301:lb_svrpxy.c:svr_Sproxy_sndhdr:518:SERVER_SPROXY(node1): send header=132 
 1301:lb_svrpxy.c:svr_Sproxy_sndhdr:522:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 1301:lb_svrpxy.c:svr_Sproxy_sndhdr:524:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 1301:lb_svrpxy.c:svr_Sproxy_sndhdr:542:SERVER_SPROXY(node1): sent header=132 
 1301:lb_svrpxy.c:svr_Sproxy_serving:458:SERVER_SPROXY(node1): Reading message queue..
 1301:lb_svrpxy.c:svr_Sproxy_mqrcv:598:SERVER_SPROXY(node1): reading from mqid=65538
 
 CMD_COPYIN_DATA SERVER->SERVER_RPROXY  
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66 <<<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:218:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66  <<<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:220:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:222:SERVER_RPROXY(node1): c_timestamp=1625612995.327121540
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:224:SERVER_RPROXY(node1): c_flags=0x2 c_src_pid=681 c_dst_pid=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:238:SERVER_RPROXY(node1):src=10 dst=11 rqtr=10 saddr=0xb748d000 daddr=0xb74b8000 bytes=1024  <<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_rcvpay:295:SERVER_RPROXY (node1): pl_size=66 <<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_rcvpay:300:SERVER_RPROXY (node1): n:66 | received:66 <<<<<<<<< SEGURAMENTE ES PORQUE ESTA COMPRIMIDO!!! 
 1302:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 1302:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
 1302:lb_svrpxy.c:svr_Rproxy_2server:184:SERVER_RPROXY(node1): Active Session Found with client node2
 1302:lb_svrpxy.c:svr_Rproxy_2server:186:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 1302:lb_svrpxy.c:svr_Rproxy_2server:188:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 1302:lb_svrpxy.c:svr_Rproxy_2server:190:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11
 
 1302:lb_svrpxy.c:svr_Rproxy_cltmq:325:SERVER_RPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=2 rcode=0 len=0 <<<<<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_cltmq:331:SERVER_RPROXY(node1): sending 132 maxbytes to node2 from mqid=98307 <<<<<<<<<<<<<<<<<<<<<<
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:207:SERVER_RPROXY(node1): 
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 
 CMD_COPYIN_DATA MSGQ->CLIENT_SPROXY
 1303:lb_cltpxy.c:clt_Sproxy_mqrcv:652:CLIENT_SPROXY(node2): 132 bytes received
 1303:lb_cltpxy.c:clt_Sproxy_serving:511:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 1303:lb_cltpxy.c:clt_Sproxy_serving:513:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1303:lb_cltpxy.c:clt_Sproxy_send:536:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 1303:lb_cltpxy.c:clt_Sproxy_send:538:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1303:lb_cltpxy.c:clt_Sproxy_sndhdr:567:CLIENT_SPROXY(node2): send header=132 
 1303:lb_cltpxy.c:clt_Sproxy_sndhdr:571:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 1303:lb_cltpxy.c:clt_Sproxy_sndhdr:573:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1303:lb_cltpxy.c:clt_Sproxy_sndhdr:591:CLIENT_SPROXY(node2): sent header=132 
 1303:lb_cltpxy.c:clt_Sproxy_send:546:CLIENT_SPROXY(node2): send payload len=66
 1303:lb_cltpxy.c:clt_Sproxy_sndpay:614:CLIENT_SPROXY(node2): clt_Sproxy_sndpay bytesleft=66 
 1303:lb_cltpxy.c:clt_Sproxy_sndpay:618:CLIENT_SPROXY(node2): src=10 dst=11 rqtr=10 saddr=0xb748d000 daddr=0xb74b8000 bytes=1024 
 1303:lb_cltpxy.c:clt_Sproxy_sndpay:623:CLIENT_SPROXY(node2): payload sent=66 
 1303:lb_cltpxy.c:clt_Sproxy_sndpay:637:CLIENT_SPROXY(node2): sent payload=66 
 1303:lb_cltpxy.c:clt_Sproxy_serving:505:CLIENT_SPROXY(node2): Reading message queue..
 1303:lb_cltpxy.c:clt_Sproxy_mqrcv:648:CLIENT_SPROXY(node2): reading from mqid=98307
 
 
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:270:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:273:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 1302:lb_svrpxy.c:svr_Rproxy_rcvhdr:275:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:226:SERVER_RPROXY(node1): NONE
 1302:lb_svrpxy.c:svr_Rproxy_getmsg:212:SERVER_RPROXY(node1): About to receive header

NODE2 PROXY REAL  : SENDREC -> NODE0 
lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 684: Waiting a message
 lz4tcp_proxy_bat.c:ps_start_serving:899:SPROXY: 684 cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:900:SPROXY: 684 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 684 c_flags=0x0 c_src_pid=705 c_dst_pid=92
 lz4tcp_proxy_bat.c:ps_start_serving:933:SPROXY: source=11 type=16077022 m1i1=1024 m1i2=0 m1i3=0 m1p1=0xb74b8000 m1p2=(nil) m1p3=(nil) 
 lz4tcp_proxy_bat.c:ps_start_serving:939:SPROXY 684: Getting more messages
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 

NODE1 RECIBE EL SENDREC 
 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 660: Waiting a message
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 661 c_timestamp=1625612995.320769029
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 661 c_flags=0x0 c_src_pid=705 c_dst_pid=92
 lz4tcp_proxy_bat.c:pr_process_message:335:RPROXY: source=11 type=16077022 m1i1=1024 m1i2=0 m1i3=0 m1p1=0xb74b8000 m1p2=(nil) m1p3=(nil) 
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
  lz4tcp_proxy_bat.c:pr_process_message:458:RPROXY:cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:459:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=705 c_ack_seq=92
 
 NODE1 ENVIA EL CMD_COPYIN_DATA 
  lz4tcp_proxy_bat.c:ps_start_serving:899:SPROXY: 660 cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=1024
 lz4tcp_proxy_bat.c:ps_start_serving:900:SPROXY: 660 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 660 c_flags=0x0 c_src_pid=681 c_dst_pid=92
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=1024
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=1024
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_remote:809:SPROXY: c_len=1024
 lz4tcp_proxy_bat.c:compress_payload:645:SPROXY: raw_len=1024 comp_len=66 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< COMPRIMIENDO 
 lz4tcp_proxy_bat.c:ps_send_remote:814:SPROXY: c_len=1024 comp_len=66
 lz4tcp_proxy_bat.c:ps_send_remote:820:c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 lz4tcp_proxy_bat.c:ps_send_remote:831:SPROXY: send payload len=66
 lz4tcp_proxy_bat.c:ps_send_payload:719:SPROXY: ps_send_payload bytesleft=66 
 lz4tcp_proxy_bat.c:ps_send_payload:724:SPROXY: sent=66 
 lz4tcp_proxy_bat.c:ps_send_payload:738:SPROXY: socket=4 sent payload=66 

NODE2 RECIBE EL CMD_COPYIN_DATA
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=66
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x2 c_snd_seq=681 c_ack_seq=92
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 685 c_timestamp=1625612995.327121540
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 685 c_flags=0x2 c_src_pid=681 c_dst_pid=92
 lz4tcp_proxy_bat.c:pr_process_message:339:RPROXY: src=10 dst=11 rqtr=10 saddr=0xb748d000 daddr=0xb74b8000 bytes=1024 
 lz4tcp_proxy_bat.c:pr_receive_payload:267:pl_size=66
 lz4tcp_proxy_bat.c:pr_receive_payload:270:recv=66
 lz4tcp_proxy_bat.c:pr_receive_payload:272:RPROXY: n:66 | received:66
 lz4tcp_proxy_bat.c:decompress_payload:165:RPROXY: INPUT p_maxRsize=65536 p_maxCsize=65559 
 lz4tcp_proxy_bat.c:decompress_payload:170:RPROXY: INPUT comp_len=66 raw_len=65536 <<<<<<<<<<<<<<<<<<<
ERROR: lz4tcp_proxy_bat.c:decompress_payload:179: rcode=-13 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ERROR DE COMPRESION

=====================================================================================
20210707:

TODO: PROBAR CLIENT Y SERVER CONECTADOS DIRECTOS ENTRE SI Y VER QUE INFO SE TRANSFIEREN
EL PROBLEMA ESTA EN EL PAYLOAD posiblemente en la estructura proxy_msg_t el header y el payload esten separados por algun byte de relleno.
SERVER-->SERVER_RPROXY--MSGQ-->CLIENT_SPROXY--->CLIENT
IMPRIMIR LOS PRIMEROS CARACTERES DEL PAYLOAD
PROBAR SIN COMPRESION Y TRANSFERIR SOLO 20 BYTES 
PROBAR SOLO TRANSFERENCIAS DE MENSAJES, SIN COPIA DE DATOS

CALCULAR maxbytes como:
maxbytes = (int) ((hdr_ptr - pl_ptr) + c_len);  

FUNCIONO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

DIALOGO ENTRE CLIENTE Y SERVIDOR 
    CLIENT						SERVER 
	  |------>CMD_SNDREC_MSG--->
	  <------CMD_COPYIN_DATA----
	  |----->CMD_COPYIN_ACK---->
	  <------CMD_SEND_MSG-------
	  |------>CMD_SEND_ACK----->

CLIENT->CMD_SNDREC_MSG->CLIENT_RPROXY
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:316:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:319:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:321:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:254:CLIENT_RPROXY(node2): header bytes=132
 779:lb_cltpxy.c:clt_Rproxy_getcmd:262:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_getcmd:264:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:266:CLIENT_RPROXY(node2): c_timestamp=1625673354.400000191
 779:lb_cltpxy.c:clt_Rproxy_getcmd:268:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=741 c_dst_pid=92 <<<< 741 es el PID de CLIENT 
 779:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.
 779:lb_cltpxy.c:clt_Rproxy_2server:171:CLIENT_RPROXY(node2): 
 779:lb_cltpxy.c:clt_Rproxy_2server:227:CLIENT_RPROXY(node2): NEW Session with server node1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 779:lb_cltpxy.c:clt_Rproxy_2server:229:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:231:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:233:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 
 CLIENT_RPROXY--CMD_SNDREC_MSG->MQ (SERVER_SPROXY)
 779:lb_cltpxy.c:clt_Rproxy_svrmq:373:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_svrmq:383:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 
 MQ (SERVER_SPROXY)--CMD_SNDREC_MSG->SERVER_SPROXY 
 776:lb_svrpxy.c:svr_Sproxy_mqrcv:604:SERVER_SPROXY(node1): 132 bytes received
 
 SERVER_PROXY-CMD_SNDREC_MSG->SERVER 
 776:lb_svrpxy.c:svr_Sproxy_serving:467:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 776:lb_svrpxy.c:svr_Sproxy_serving:469:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_send:491:SERVER_SPROXY(node1): 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:520:SERVER_SPROXY(node1): send header=132 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:524:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:526:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:544:SERVER_SPROXY(node1): sent header=132 
 776:lb_svrpxy.c:svr_Sproxy_serving:460:SERVER_SPROXY(node1): Reading message queue..
 776:lb_svrpxy.c:svr_Sproxy_mqrcv:600:SERVER_SPROXY(node1): reading from mqid=0
 
EL SERVER RESPONDE CON CMD_COPYIN_DATA
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:266:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:269:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=22
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:271:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 777:lb_svrpxy.c:svr_Rproxy_getcmd:214:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=22
 777:lb_svrpxy.c:svr_Rproxy_getcmd:216:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 777:lb_svrpxy.c:svr_Rproxy_getcmd:218:SERVER_RPROXY(node1): c_timestamp=1625673354.407783932
 777:lb_svrpxy.c:svr_Rproxy_getcmd:220:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=799 c_dst_pid=92
 777:lb_svrpxy.c:svr_Rproxy_getcmd:234:SERVER_RPROXY(node1):src=10 dst=11 rqtr=10 saddr=0xb744e000 daddr=0xb7419000 bytes=22 
 777:lb_svrpxy.c:svr_Rproxy_rcvpay:291:SERVER_RPROXY (node1): pl_size=22
 777:lb_svrpxy.c:svr_Rproxy_rcvpay:296:SERVER_RPROXY (node1): n:22 | received:22
 777:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 
 ENCUENTRA UNA SESION ACTIVA Y SABE AHORA CUAL ES EL NODO DEL CLIENTE
 777:lb_svrpxy.c:svr_Rproxy_2server:164:SERVER_RPROXY(node1): 
 777:lb_svrpxy.c:svr_Rproxy_2server:180:SERVER_RPROXY(node1): Active Session Found with client node2
 777:lb_svrpxy.c:svr_Rproxy_2server:182:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 777:lb_svrpxy.c:svr_Rproxy_2server:184:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 777:lb_svrpxy.c:svr_Rproxy_2server:186:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 777:lb_svrpxy.c:svr_Rproxy_cltmq:323:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=22
 777:lb_svrpxy.c:svr_Rproxy_cltmq:330:SERVER_RPROXY(node1): maxbytes=154 my_bytes=154
 777:lb_svrpxy.c:svr_Rproxy_cltmq:333:SERVER_RPROXY(node1): sending 154 maxbytes to node2 from mqid=32769

SERVER_RPROXY--CMD_COPYIN_DATA->MQ (CLIENT_SPROXY)
 778:lb_cltpxy.c:clt_Sproxy_mqrcv:659:CLIENT_SPROXY(node2): 154 bytes received
 
 MQ (CLIENT_SPROXY)-CMD_COPYIN_DATA->CLIENT 
 778:lb_cltpxy.c:clt_Sproxy_serving:518:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=22
 778:lb_cltpxy.c:clt_Sproxy_serving:520:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_send:543:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=22
 778:lb_cltpxy.c:clt_Sproxy_send:545:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:574:CLIENT_SPROXY(node2): send header=132 
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:578:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=22
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:580:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:598:CLIENT_SPROXY(node2): sent header=132 
 778:lb_cltpxy.c:clt_Sproxy_send:553:CLIENT_SPROXY(node2): send payload len=22
 778:lb_cltpxy.c:clt_Sproxy_sndpay:621:CLIENT_SPROXY(node2): clt_Sproxy_sndpay bytesleft=22 
 778:lb_cltpxy.c:clt_Sproxy_sndpay:625:CLIENT_SPROXY(node2): src=10 dst=11 rqtr=10 saddr=0xb744e000 daddr=0xb7419000 bytes=22 
 778:lb_cltpxy.c:clt_Sproxy_sndpay:630:CLIENT_SPROXY(node2): payload sent=22 
 778:lb_cltpxy.c:clt_Sproxy_sndpay:644:CLIENT_SPROXY(node2): sent payload=22 
 778:lb_cltpxy.c:clt_Sproxy_serving:512:CLIENT_SPROXY(node2): Reading message queue..
 778:lb_cltpxy.c:clt_Sproxy_mqrcv:655:CLIENT_SPROXY(node2): reading from mqid=32769 

CLIENT----CMD_COPYIN_ACK-> CLIENT_RPROXY
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:316:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:319:CLIENT_RPROXY (node2): cmd=0x2005 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=22 len=0
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:321:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:254:CLIENT_RPROXY(node2): header bytes=132
 779:lb_cltpxy.c:clt_Rproxy_getcmd:262:CLIENT_RPROXY(node2): cmd=0x2005 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=22 len=0
 779:lb_cltpxy.c:clt_Rproxy_getcmd:264:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:266:CLIENT_RPROXY(node2): c_timestamp=1625673354.415999170
 779:lb_cltpxy.c:clt_Rproxy_getcmd:268:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=741 c_dst_pid=92
 779:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.

 ENCUENTRA SESION ACTIVA Y AHORA SABE EL NODO DEL SERVER
CLIENT_RPROXY -CMD_COPYIN_ACK--> MQ (SERVER_SPROXY)
 779:lb_cltpxy.c:clt_Rproxy_2server:171:CLIENT_RPROXY(node2): 
 779:lb_cltpxy.c:clt_Rproxy_2server:185:CLIENT_RPROXY(node2): Active Session Found with server node1
 779:lb_cltpxy.c:clt_Rproxy_2server:187:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:189:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:191:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 779:lb_cltpxy.c:clt_Rproxy_svrmq:373:CLIENT_RPROXY(node2): cmd=0x2005 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=22 len=0
 779:lb_cltpxy.c:clt_Rproxy_svrmq:383:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0

 MQ (SERVER_SPROXY)--CMD_COPYIN_ACK->SERVER_SPROXY 
 776:lb_svrpxy.c:svr_Sproxy_mqrcv:604:SERVER_SPROXY(node1): 132 bytes received
 
SERVER_SPROXY--CMD_COPYIN_ACK->SERVER 
 776:lb_svrpxy.c:svr_Sproxy_serving:467:SERVER_SPROXY(node1): cmd=0x2005 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=22 len=0
 776:lb_svrpxy.c:svr_Sproxy_serving:469:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_send:491:SERVER_SPROXY(node1): 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:520:SERVER_SPROXY(node1): send header=132 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:524:SERVER_SPROXY(node1): cmd=0x2005 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=22 len=0
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:526:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:544:SERVER_SPROXY(node1): sent header=132 
 
 SERVER_SPROXY-->CMD_SEND_MSG-->SERVER_RPROXY
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:266:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:269:SERVER_RPROXY (node1): cmd=0x1 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 777:lb_svrpxy.c:svr_Rproxy_rcvhdr:271:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 777:lb_svrpxy.c:svr_Rproxy_getcmd:214:SERVER_RPROXY(node1): cmd=0x1 dcid=0 src=10 dst=11 snode=1 dnode=0 rcode=0 len=0
 777:lb_svrpxy.c:svr_Rproxy_getcmd:216:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 777:lb_svrpxy.c:svr_Rproxy_getcmd:218:SERVER_RPROXY(node1): c_timestamp=1625673354.423783681
 777:lb_svrpxy.c:svr_Rproxy_getcmd:220:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=799 c_dst_pid=92
 777:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 777:lb_svrpxy.c:svr_Rproxy_2server:164:SERVER_RPROXY(node1): 
 
 ENCOTRO SESION ACTIVA, AHORA SABE EL NODO DEL CLIENT
 SERVER_RPROXY->CMD_SEND_MSG->MQ(CLIENT_SPROXY)
 777:lb_svrpxy.c:svr_Rproxy_2server:180:SERVER_RPROXY(node1): Active Session Found with client node2
 777:lb_svrpxy.c:svr_Rproxy_2server:182:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 777:lb_svrpxy.c:svr_Rproxy_2server:184:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 777:lb_svrpxy.c:svr_Rproxy_2server:186:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 777:lb_svrpxy.c:svr_Rproxy_cltmq:323:SERVER_RPROXY(node1): cmd=0x1 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=0
 777:lb_svrpxy.c:svr_Rproxy_cltmq:333:SERVER_RPROXY(node1): sending 132 maxbytes to node2 from mqid=32769
 
 MQ(CLIENT_SPROXY)->CMD_SEND_MSG-->CLIENT_SPROXY
  778:lb_cltpxy.c:clt_Sproxy_mqrcv:659:CLIENT_SPROXY(node2): 132 bytes received
  
 CLIENT_SPROXY->CMD_SEND_MSG-->CLIENT 
 778:lb_cltpxy.c:clt_Sproxy_serving:518:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=0
 778:lb_cltpxy.c:clt_Sproxy_serving:520:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_send:543:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=0
 778:lb_cltpxy.c:clt_Sproxy_send:545:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:574:CLIENT_SPROXY(node2): send header=132 
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:578:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=11 snode=0 dnode=2 rcode=0 len=0
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:580:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=799 c_ack_seq=92
 778:lb_cltpxy.c:clt_Sproxy_sndhdr:598:CLIENT_SPROXY(node2): sent header=132 
 778:lb_cltpxy.c:clt_Sproxy_serving:512:CLIENT_SPROXY(node2): Reading message queue..
 778:lb_cltpxy.c:clt_Sproxy_mqrcv:655:CLIENT_SPROXY(node2): reading from mqid=32769 
 
 CLIENT---->CMD_SEND_ACK---CLIENT_RPROXY
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:316:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:319:CLIENT_RPROXY (node2): cmd=0x2001 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:321:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:254:CLIENT_RPROXY(node2): header bytes=132
 779:lb_cltpxy.c:clt_Rproxy_getcmd:262:CLIENT_RPROXY(node2): cmd=0x2001 dcid=0 src=11 dst=10 snode=2 dnode=0 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_getcmd:264:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 779:lb_cltpxy.c:clt_Rproxy_getcmd:266:CLIENT_RPROXY(node2): c_timestamp=1625673354.427998405
 779:lb_cltpxy.c:clt_Rproxy_getcmd:268:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=741 c_dst_pid=92
 779:lb_cltpxy.c:clt_Rproxy_loop:124:CLIENT_RPROXY(node2):Message succesfully processed.
 779:lb_cltpxy.c:clt_Rproxy_2server:171:CLIENT_RPROXY(node2): 
 
 CLIENT_RPROXY ENCUENTRA UNA SESION ACTIVA 
 779:lb_cltpxy.c:clt_Rproxy_2server:185:CLIENT_RPROXY(node2): Active Session Found with server node1
 
 CLIENT_RPROXY-CMD_SEND_ACK-->MQ(SERVER_SPROXY)
 779:lb_cltpxy.c:clt_Rproxy_2server:187:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=11 se_clt_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:189:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 779:lb_cltpxy.c:clt_Rproxy_2server:191:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=11 
 779:lb_cltpxy.c:clt_Rproxy_svrmq:373:CLIENT_RPROXY(node2): cmd=0x2001 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 779:lb_cltpxy.c:clt_Rproxy_svrmq:383:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 779:lb_cltpxy.c:clt_Rproxy_loop:119:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 779:lb_cltpxy.c:clt_Rproxy_getcmd:247:CLIENT_RPROXY(node2): 
 779:lb_cltpxy.c:clt_Rproxy_getcmd:252:CLIENT_RPROXY(node2): About to receive header
 779:lb_cltpxy.c:clt_Rproxy_rcvhdr:309:CLIENT_RPROXY(node2): lbp_csd=5
 
 MQ(SERVER_SPROXY)--CMD_SEND_ACK-->SERVER_SPROXY
 776:lb_svrpxy.c:svr_Sproxy_mqrcv:604:SERVER_SPROXY(node1): 132 bytes received
 
 SERVER_SPROXY--CMD_SEND_ACK-->SERVER 
 776:lb_svrpxy.c:svr_Sproxy_serving:467:SERVER_SPROXY(node1): cmd=0x2001 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 776:lb_svrpxy.c:svr_Sproxy_serving:469:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_send:491:SERVER_SPROXY(node1): 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:520:SERVER_SPROXY(node1): send header=132 
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:524:SERVER_SPROXY(node1): cmd=0x2001 dcid=0 src=11 dst=10 snode=0 dnode=1 rcode=0 len=0
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:526:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=741 c_ack_seq=92
 776:lb_svrpxy.c:svr_Sproxy_sndhdr:544:SERVER_SPROXY(node1): sent header=132 
 776:lb_svrpxy.c:svr_Sproxy_serving:460:SERVER_SPROXY(node1): Reading message queue..
 776:lb_svrpxy.c:svr_Sproxy_mqrcv:600:SERVER_SPROXY(node1): reading from mqid=0

=====================================================================================
20210708: 
------------------------------------------------------------------------------------------------------------
LISTO: Hacer que una sesion se distinga por el PID del client y del Server

------------------------------------------------------------------------------------------------------------
TODO:  Cuando el proxy RECEIVER recibe un mensaje NONE, 
		lo copia en el la cola de mensajes del proxy sender correspondiente.
		invirtiendo nodos.
		
LADO SERVER
	RECIBE NONE 
 1233:lb_svrpxy.c:svr_Rproxy_rcvhdr:297:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1233:lb_svrpxy.c:svr_Rproxy_rcvhdr:300:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1233:lb_svrpxy.c:svr_Rproxy_rcvhdr:302:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1233:lb_svrpxy.c:svr_Rproxy_getcmd:243:SERVER_RPROXY(node1): NONE
 1233:lb_svrpxy.c:svr_Rproxy_getcmd:248:SERVER_RPROXY(node1): Replying NONE
 1233:lb_svrpxy.c:svr_Rproxy_getcmd:229:SERVER_RPROXY(node1): About to receive header
 
	RESPONDE NONE 
 1232:lb_svrpxy.c:svr_Sproxy_mqrcv:635:SERVER_SPROXY(node1): 132 bytes received
 1232:lb_svrpxy.c:svr_Sproxy_serving:498:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1232:lb_svrpxy.c:svr_Sproxy_serving:500:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1232:lb_svrpxy.c:svr_Sproxy_send:522:SERVER_SPROXY(node1): 
 1232:lb_svrpxy.c:svr_Sproxy_sndhdr:551:SERVER_SPROXY(node1): send header=132 
 1232:lb_svrpxy.c:svr_Sproxy_sndhdr:555:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1232:lb_svrpxy.c:svr_Sproxy_sndhdr:557:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1232:lb_svrpxy.c:svr_Sproxy_sndhdr:575:SERVER_SPROXY(node1): sent header=132 
 1232:lb_svrpxy.c:svr_Sproxy_serving:491:SERVER_SPROXY(node1): Reading message queue..
 1232:lb_svrpxy.c:svr_Sproxy_mqrcv:631:SERVER_SPROXY(node1): reading from mqid=0 
 
 EN EL PROXY DEL SERVER SE RECIBE EL NONE (como se que es el de NODE0 y no el de NODE2)
	 lz4tcp_proxy_bat.c:ps_start_serving:878:SPROXY: Sending HELLO 
	 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
	 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
	 lz4tcp_proxy_bat.c:update_stats:749:px_type=1
	 lz4tcp_proxy_bat.c:update_stats:751:MTX_LOCK px_mutex 
	 lz4tcp_proxy_bat.c:update_stats:788:snode=1 dnode=0 dcid=0 nr_msg=0 nr_data=0 nr_cmd=1
	 lz4tcp_proxy_bat.c:update_stats:789:MTX_UNLOCK px_mutex 
	 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 676: Waiting a message
	 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
	 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 lz4tcp_proxy_bat.c:pr_process_message:327:RPROXY: 677 NONE
	 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 
  
 
LADO CLIENT 
	RECIBE NONE 
 1235:lb_cltpxy.c:clt_Rproxy_rcvhdr:342:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 1235:lb_cltpxy.c:clt_Rproxy_rcvhdr:345:CLIENT_RPROXY (node2): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1235:lb_cltpxy.c:clt_Rproxy_rcvhdr:347:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1235:lb_cltpxy.c:clt_Rproxy_getcmd:270:CLIENT_RPROXY(node2): header bytes=132
 1235:lb_cltpxy.c:clt_Rproxy_getcmd:286:CLIENT_RPROXY(node2): NONE
 1235:lb_cltpxy.c:clt_Rproxy_getcmd:291:CLIENT_RPROXY(node2): Replying NONE
 1235:lb_cltpxy.c:clt_Rproxy_getcmd:268:CLIENT_RPROXY(node2): About to receive header
 1235:lb_cltpxy.c:clt_Rproxy_rcvhdr:335:CLIENT_RPROXY(node2): lbp_csd=4 
	RESPONDE NONE
 1234:lb_cltpxy.c:clt_Sproxy_mqrcv:685:CLIENT_SPROXY(node2): 132 bytes received
 1234:lb_cltpxy.c:clt_Sproxy_serving:544:CLIENT_SPROXY(node2): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1234:lb_cltpxy.c:clt_Sproxy_serving:546:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1234:lb_cltpxy.c:clt_Sproxy_send:569:CLIENT_SPROXY(node2): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1234:lb_cltpxy.c:clt_Sproxy_send:571:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1234:lb_cltpxy.c:clt_Sproxy_sndhdr:600:CLIENT_SPROXY(node2): send header=132 
 1234:lb_cltpxy.c:clt_Sproxy_sndhdr:604:CLIENT_SPROXY(node2): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 1234:lb_cltpxy.c:clt_Sproxy_sndhdr:606:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 1234:lb_cltpxy.c:clt_Sproxy_sndhdr:624:CLIENT_SPROXY(node2): sent header=132 
 1234:lb_cltpxy.c:clt_Sproxy_serving:538:CLIENT_SPROXY(node2): Reading message queue..
 1234:lb_cltpxy.c:clt_Sproxy_mqrcv:681:CLIENT_SPROXY(node2): reading from mqid=32769
 

------------------------------------------------------------------------------------------------------------

HECHO : Modificar la configuracion para que el LB tenga un endpoint y un DC en donde escucha 

# this is a comment 
lb LB_NAME {
	nodeid 		0;
};

server node1 {
	nodeid 		1;
};

#dcid puede ser un numero 0-(dvs_ptr->d_nr_dcs-1)
#si no se menciona dcid, entonces es ANY
#ext_ep es el endpoint presentado a los clientes.
#min_ep y max_ep es el rango de endpoints en los que se pueden arrancar los services  
service m3ftp {
	dcid		0;
	ext_ep		10;
	min_ep	 	10;
	max_ep	 	20;	
	bind		replica;
	prog		"/usr/src/dvs/dvs-apps/dvs_run/migr_server.sh" 
};

client node2 {
	nodeid 		2;
};

ATENCION!!!!!!!!!!!!!!!!!!!!!!! habia un error en lz4tcp_proxy_bat.c 

HECHO: Que pasa con el mensaje NONE que deberian enviar los SENDER ?? 
		Los proxies estandares ignoran el NONE enviado por el otro Sender Proxy del otro nodo.
		Ahora bien, el propio sender proxy genera en forma automatica el mensaje NONE al vencerse el timeout del dvk_get2rmt().
				case EDVSTIMEDOUT:
				PXYDEBUG("SPROXY: Sending HELLO \n");
				p_header->c_cmd   = CMD_NONE;
		Esto no seria posible en los proxies del LB ya que estan bloqueados en rcvmsg()
		Cuando el proxy RECEIVER recibe un mensaje NONE, 
		lo copia en el la cola de mensajes del  proxy sender correspondiente.
		invirtiendo nodos .

HECHO:  Los clientes tienen que arrancar con Endpoint de CLIENTE!!
		El destino de las sesiones debe ser un SERVER ENDPOINT !! 
		El origen de un server tiene que ser un SERVER y el destino un CLIENT 
		Controlarlo en el lb_cltpxy.c		
	
	
root@node0:/usr/src/dvs/dvs-apps/lb_dvs# grep Session salida.out 
PRIMERA SESION 
 1298:lb_cltpxy.c:clt_Rproxy_2server:317:CLIENT_RPROXY(node2): NEW Session with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 
 SEGUNDA SESION REARRANCANDO EL CLIENT, EXPIRA LA ANTERIOR
 1298:lb_cltpxy.c:clt_Rproxy_2server:228:CLIENT_RPROXY(node2): Expired Session Found se_clt_PID=708 c_src_pid=712
 1298:lb_cltpxy.c:clt_Rproxy_2server:317:CLIENT_RPROXY(node2): NEW Session with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 
 SEGUNDA SESION REARRANCANDO EL CLIENT Y EL SERVER, EXPIRA LA ANTERIOR
 1298:lb_cltpxy.c:clt_Rproxy_2server:228:CLIENT_RPROXY(node2): Expired Session Found se_clt_PID=712 c_src_pid=717
 1298:lb_cltpxy.c:clt_Rproxy_2server:317:CLIENT_RPROXY(node2): NEW Session with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
 1296:lb_svrpxy.c:svr_Rproxy_2server:236:SERVER_RPROXY(node1): Active Session Found with client node2
 1298:lb_cltpxy.c:clt_Rproxy_2server:238:CLIENT_RPROXY(node2): Active Session Found with server node1
	
LISTO:  Tanto los Client Receiver como los Server Receiver acceden a las sessiones y a los servers 
		Se deberia utilizar un semaforo.
	
=====================================================================================
20210709:
	
	TODO: Dar tratamiento al caso.
			CLIENT------------------>SERVER (PID1)
			CLIENT<-----------------SERVER (PID1)
			Se crea la sesion con PID1
			Luego muere el server y se arranca uno nuevo con PID2 
			CLIENT------------------>SERVER (PID2)
			Retornarle al CLIENT un mensaje EDVSDSTDIED

			PROBAR CON TIEMPOS DE MENSAJES DEL CLIENTE DE 30 SEGUNDOS PARA PODER MATAR AL SERVER Y VOLVER A ARRANCARLO
		
AL REARRANCAR EL SERVER (DURANTE UNA SESION ACTIVA)
		DETECTA LA NO COINCIDENCIA CON EL PID ACTUAL 
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:388:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:391:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=10 dst=70 snode=1 dnode=0 rcode=0 len=66
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:393:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=686 c_ack_seq=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:326:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=70 snode=1 dnode=0 rcode=0 len=66
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:328:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=686 c_ack_seq=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:330:SERVER_RPROXY(node1): c_timestamp=1625854506.041890729
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:332:SERVER_RPROXY(node1): c_flags=0x2 c_src_pid=686 c_dst_pid=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:356:SERVER_RPROXY(node1):src=10 dst=70 rqtr=10 saddr=0xb7400000 daddr=0xb7469000 bytes=1024 
	 993:lb_svrpxy.c:svr_Rproxy_rcvpay:413:SERVER_RPROXY (node1): pl_size=66
	 993:lb_svrpxy.c:svr_Rproxy_rcvpay:418:SERVER_RPROXY (node1): n:66 | received:66
	 993:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
	 993:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
	 993:lb_svrpxy.c:svr_Rproxy_2server:215:SERVER_RPROXY(node1): Expired Session Found se_svr_PID=683 c_src_pid=686 !!!!!!!!!!!!!!!!!
	ERROR: lb_svrpxy.c:svr_Rproxy_2server:279: rcode=-59
	ERROR: lb_svrpxy.c:svr_Rproxy_loop:126: rcode=-11
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:315:SERVER_RPROXY(node1): 
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:320:SERVER_RPROXY(node1): About to receive header
 
LUEGO DE VENCER LOS TIMEOUTS EN CLIENT Y SERVER
EL CLIENTE, CUANDO HACE UN PEDIDO, CREA INDIRECTAMENTE UNA NUEVA SESION Y TODO SIGUE IGUAL 
	 995:lb_cltpxy.c:clt_Rproxy_rcvhdr:427:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
	 995:lb_cltpxy.c:clt_Rproxy_rcvhdr:430:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=70 dst=10 snode=2 dnode=0 rcode=0 len=0
	 995:lb_cltpxy.c:clt_Rproxy_rcvhdr:432:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=716 c_ack_seq=-1
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:355:CLIENT_RPROXY(node2): header bytes=132
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:363:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=70 dst=10 snode=2 dnode=0 rcode=0 len=0
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:365:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=716 c_ack_seq=-1
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:367:CLIENT_RPROXY(node2): c_timestamp=1625854536.170635905
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:369:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=716 c_dst_pid=-1
	 995:lb_cltpxy.c:clt_Rproxy_loop:126:CLIENT_RPROXY(node2):Message succesfully processed.
	 995:lb_cltpxy.c:clt_Rproxy_2server:216:CLIENT_RPROXY(node2): 
	 995:lb_cltpxy.c:clt_Rproxy_2server:328:CLIENT_RPROXY(node2): NEW Session with server node1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	 995:lb_cltpxy.c:clt_Rproxy_2server:330:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=70 se_clt_PID=716
	 995:lb_cltpxy.c:clt_Rproxy_2server:332:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
	 995:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=70 
	 995:lb_cltpxy.c:clt_Rproxy_svrmq:496:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=70 dst=10 snode=0 dnode=1 rcode=0 len=0
	 995:lb_cltpxy.c:clt_Rproxy_svrmq:506:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
	 995:lb_cltpxy.c:clt_Rproxy_loop:121:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:348:CLIENT_RPROXY(node2): 
	 995:lb_cltpxy.c:clt_Rproxy_getcmd:353:CLIENT_RPROXY(node2): About to receive header
	 995:lb_cltpxy.c:clt_Rproxy_rcvhdr:420:CLIENT_RPROXY(node2): lbp_csd=4 

CUANDO EL SERVER NUEVO RESPONDE ENCUENTRA LA SESION CREADA 
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:388:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:391:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=10 dst=70 snode=1 dnode=0 rcode=0 len=66
	 993:lb_svrpxy.c:svr_Rproxy_rcvhdr:393:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=686 c_ack_seq=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:326:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=70 snode=1 dnode=0 rcode=0 len=66
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:328:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=686 c_ack_seq=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:330:SERVER_RPROXY(node1): c_timestamp=1625854537.321111395
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:332:SERVER_RPROXY(node1): c_flags=0x2 c_src_pid=686 c_dst_pid=-1
	 993:lb_svrpxy.c:svr_Rproxy_getcmd:356:SERVER_RPROXY(node1):src=10 dst=70 rqtr=10 saddr=0xb7400000 daddr=0xb7469000 bytes=1024 
	 993:lb_svrpxy.c:svr_Rproxy_rcvpay:413:SERVER_RPROXY (node1): pl_size=66
	 993:lb_svrpxy.c:svr_Rproxy_rcvpay:418:SERVER_RPROXY (node1): n:66 | received:66
	 993:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
	 993:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
	 993:lb_svrpxy.c:svr_Rproxy_2server:267:SERVER_RPROXY(node1): Active Session Found with client node2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	 993:lb_svrpxy.c:svr_Rproxy_2server:269:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=70 se_clt_PID=716
	 993:lb_svrpxy.c:svr_Rproxy_2server:271:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=686
	 993:lb_svrpxy.c:svr_Rproxy_2server:273:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=70 
	 993:lb_svrpxy.c:svr_Rproxy_cltmq:450:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=10 dst=70 snode=0 dnode=2 rcode=0 len=66
	 993:lb_svrpxy.c:svr_Rproxy_cltmq:457:SERVER_RPROXY(node1): maxbytes=198 my_bytes=198
	 993:lb_svrpxy.c:svr_Rproxy_cltmq:460:SERVER_RPROXY(node1): sending 198 maxbytes to node2 from mqid=32769
 

 EN LZ4 TCP PROXY HABIA UN ERROR 
  lz4tcp_proxy_bat.c:pr_process_message:391:RPROXY: src=50 <= dvs_ptr->d_nr_sysprocs
			
------------------------------------------------------------------------
	PROBAR 2 SESIONES 
		NODE2				NODE0			NODE1
		CLIENT(70)-------- LB(10)----------SERVER(10)
		CLIENT(71)-------- LB(10)----------SERVER(11)

NEW SESSION CON NODE1 ENDPOINT CLIENT(70) Y SERVER(10)
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:427:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:430:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=70 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:432:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=662 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:355:CLIENT_RPROXY(node2): header bytes=132
 667:lb_cltpxy.c:clt_Rproxy_getcmd:363:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=70 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_getcmd:365:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=662 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:367:CLIENT_RPROXY(node2): c_timestamp=1625870047.403222611
 667:lb_cltpxy.c:clt_Rproxy_getcmd:369:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=662 c_dst_pid=-1
 667:lb_cltpxy.c:clt_Rproxy_loop:126:CLIENT_RPROXY(node2):Message succesfully processed.
 667:lb_cltpxy.c:clt_Rproxy_2server:216:CLIENT_RPROXY(node2): 
 667:lb_cltpxy.c:clt_Rproxy_2server:328:CLIENT_RPROXY(node2): NEW Session with server node1
 667:lb_cltpxy.c:clt_Rproxy_2server:330:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=70 se_clt_PID=662 <<<
 667:lb_cltpxy.c:clt_Rproxy_2server:332:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1  <<< 
 667:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=70 
 667:lb_cltpxy.c:clt_Rproxy_svrmq:496:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=70 dst=10 snode=0 dnode=1 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_svrmq:506:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 667:lb_cltpxy.c:clt_Rproxy_loop:121:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 667:lb_cltpxy.c:clt_Rproxy_getcmd:348:CLIENT_RPROXY(node2): 
 667:lb_cltpxy.c:clt_Rproxy_getcmd:353:CLIENT_RPROXY(node2): About to receive header
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:420:CLIENT_RPROXY(node2): lbp_csd=8 
 
 NEW SESSION CLIENT(71) SERVER(11)
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:427:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:430:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=71 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:432:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=664 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:355:CLIENT_RPROXY(node2): header bytes=132
 667:lb_cltpxy.c:clt_Rproxy_getcmd:363:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=71 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_getcmd:365:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=664 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:367:CLIENT_RPROXY(node2): c_timestamp=1625870050.359220068
 667:lb_cltpxy.c:clt_Rproxy_getcmd:369:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=664 c_dst_pid=-1
 667:lb_cltpxy.c:clt_Rproxy_loop:126:CLIENT_RPROXY(node2):Message succesfully processed.
 667:lb_cltpxy.c:clt_Rproxy_2server:216:CLIENT_RPROXY(node2): 
 667:lb_cltpxy.c:clt_Rproxy_2server:328:CLIENT_RPROXY(node2): NEW Session with server node1
 667:lb_cltpxy.c:clt_Rproxy_2server:330:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=71 se_clt_PID=664 <<<<
 667:lb_cltpxy.c:clt_Rproxy_2server:332:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1 < <<<<
 667:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=71 
 667:lb_cltpxy.c:clt_Rproxy_svrmq:496:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=71 dst=11 snode=0 dnode=1 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_svrmq:506:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 
 
 		NODE2				NODE0			NODE1
		CLIENT(70)-------- LB(10)----------SERVER(10)  FUNCIONA OK 
		CLIENT(71)-------- LB(10)----------SERVER(11)  FALLA 
		
		ESTA TAMBIEN FALLA
		CLIENT(71)-------- LB(10)----------SERVER(10)  FALLA
 
 PROBAR LEVANTAR LOS PROXIES SIN AUTOBINDING
  
 CLIENT SENDREC---CLIENT_RPROXY-->MQ 
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:427:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:430:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=71 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:432:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=757 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:355:CLIENT_RPROXY(node2): header bytes=132
 667:lb_cltpxy.c:clt_Rproxy_getcmd:363:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=71 dst=10 snode=2 dnode=0 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_getcmd:365:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=757 c_ack_seq=-1
 667:lb_cltpxy.c:clt_Rproxy_getcmd:367:CLIENT_RPROXY(node2): c_timestamp=1625871829.348544982
 667:lb_cltpxy.c:clt_Rproxy_getcmd:369:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=757 c_dst_pid=-1
 667:lb_cltpxy.c:clt_Rproxy_loop:126:CLIENT_RPROXY(node2):Message succesfully processed.
 667:lb_cltpxy.c:clt_Rproxy_2server:216:CLIENT_RPROXY(node2): 
 667:lb_cltpxy.c:clt_Rproxy_2server:232:CLIENT_RPROXY(node2): Expired Session Found se_clt_PID=743 c_src_pid=757
 667:lb_cltpxy.c:clt_Rproxy_2server:328:CLIENT_RPROXY(node2): NEW Session with server node1
 667:lb_cltpxy.c:clt_Rproxy_2server:330:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=71 se_clt_PID=757
 667:lb_cltpxy.c:clt_Rproxy_2server:332:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1 DEBERIA SE LA 100
 667:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=71 
 667:lb_cltpxy.c:clt_Rproxy_svrmq:496:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=71 dst=11 snode=0 dnode=1 rcode=0 len=0
 667:lb_cltpxy.c:clt_Rproxy_svrmq:506:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 667:lb_cltpxy.c:clt_Rproxy_loop:121:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 667:lb_cltpxy.c:clt_Rproxy_getcmd:348:CLIENT_RPROXY(node2): 
 667:lb_cltpxy.c:clt_Rproxy_getcmd:353:CLIENT_RPROXY(node2): About to receive header
 667:lb_cltpxy.c:clt_Rproxy_rcvhdr:420:CLIENT_RPROXY(node2): lbp_csd=8 
 
 MQ->SERVER_SPROXY
 664:lb_svrpxy.c:svr_Sproxy_mqrcv:738:SERVER_SPROXY(node1): 132 bytes received
 664:lb_svrpxy.c:svr_Sproxy_serving:601:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=71 dst=11 snode=0 dnode=1 rcode=0 len=0
 664:lb_svrpxy.c:svr_Sproxy_serving:603:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=757 c_ack_seq=-1
 
 SERVER_SPROXY->SERVER  --- -src=71 dst=11 DEBERIA SER 10
 664:lb_svrpxy.c:svr_Sproxy_send:625:SERVER_SPROXY(node1): 
 664:lb_svrpxy.c:svr_Sproxy_sndhdr:654:SERVER_SPROXY(node1): send header=132 
 664:lb_svrpxy.c:svr_Sproxy_sndhdr:658:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=71 dst=11 snode=0 dnode=1 rcode=0 len=0 
 664:lb_svrpxy.c:svr_Sproxy_sndhdr:660:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=757 c_ack_seq=-1
 664:lb_svrpxy.c:svr_Sproxy_sndhdr:678:SERVER_SPROXY(node1): sent header=132 
 664:lb_svrpxy.c:svr_Sproxy_serving:594:SERVER_SPROXY(node1): Reading message queue..
 664:lb_svrpxy.c:svr_Sproxy_mqrcv:734:SERVER_SPROXY(node1): reading from mqid=0
 
 SERVER -109!!! EDVSDSTDIED
 665:lb_svrpxy.c:svr_Rproxy_rcvhdr:395:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 665:lb_svrpxy.c:svr_Rproxy_rcvhdr:398:SERVER_RPROXY (node1): cmd=0x2001 dcid=0 src=11 dst=71 snode=1 dnode=0 rcode=-109 len=0 !!!!!!!!!!!
 665:lb_svrpxy.c:svr_Rproxy_rcvhdr:400:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=-1 c_ack_seq=-1
 665:lb_svrpxy.c:svr_Rproxy_getcmd:333:SERVER_RPROXY(node1): cmd=0x2001 dcid=0 src=11 dst=71 snode=1 dnode=0 rcode=-109 len=0
 665:lb_svrpxy.c:svr_Rproxy_getcmd:335:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=-1 c_ack_seq=-1
 665:lb_svrpxy.c:svr_Rproxy_getcmd:337:SERVER_RPROXY(node1): c_timestamp=1625871829.355920717
 665:lb_svrpxy.c:svr_Rproxy_getcmd:339:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 665:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 665:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
 665:lb_svrpxy.c:svr_Rproxy_2server:267:SERVER_RPROXY(node1): Active Session Found with client node2
 665:lb_svrpxy.c:svr_Rproxy_2server:269:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=71 se_clt_PID=757
 665:lb_svrpxy.c:svr_Rproxy_2server:271:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 665:lb_svrpxy.c:svr_Rproxy_2server:273:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=71 
 665:lb_svrpxy.c:svr_Rproxy_cltmq:457:SERVER_RPROXY(node1): cmd=0x2001 dcid=0 src=11 dst=71 snode=0 dnode=2 rcode=-109 len=0
 665:lb_svrpxy.c:svr_Rproxy_cltmq:467:SERVER_RPROXY(node1): sending 132 maxbytes to node2 from mqid=32769
  
  
=====================================================================================
20210710:

SE PROBO 
 		NODE2				NODE0			NODE1
		CLIENT(71)-------- LB(10)----------SERVER(10)  FUNCIONA OK 
		CLIENT(70)-------- LB(10)----------SERVER(11)  FALLA 
		
 722:lb_svrpxy.c:svr_Rproxy_rcvhdr:395:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 722:lb_svrpxy.c:svr_Rproxy_rcvhdr:398:SERVER_RPROXY (node1): cmd=0x5 dcid=0 src=11 dst=70 snode=1 dnode=0 rcode=0 len=66
 722:lb_svrpxy.c:svr_Rproxy_rcvhdr:400:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 722:lb_svrpxy.c:svr_Rproxy_getcmd:333:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=11 dst=70 snode=1 dnode=0 rcode=0 len=66
 722:lb_svrpxy.c:svr_Rproxy_getcmd:335:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 722:lb_svrpxy.c:svr_Rproxy_getcmd:337:SERVER_RPROXY(node1): c_timestamp=1625925565.924074698
 722:lb_svrpxy.c:svr_Rproxy_getcmd:339:SERVER_RPROXY(node1): c_flags=0x2 c_src_pid=806 c_dst_pid=-1
 722:lb_svrpxy.c:svr_Rproxy_getcmd:363:SERVER_RPROXY(node1):src=11 dst=70 rqtr=11 saddr=0xb745a000 daddr=0xb7495000 bytes=1024 
 722:lb_svrpxy.c:svr_Rproxy_rcvpay:420:SERVER_RPROXY (node1): pl_size=66
 722:lb_svrpxy.c:svr_Rproxy_rcvpay:425:SERVER_RPROXY (node1): n:66 | received:66
 722:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 
 722:lb_svrpxy.c:svr_Rproxy_2server:168:SERVER_RPROXY(node1): 
 PRIMER SESION 71,10
 722:lb_svrpxy.c:svr_Rproxy_2server:190:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=71 se_clt_PID=813
 722:lb_svrpxy.c:svr_Rproxy_2server:192:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=803
 722:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=71 
 722:lb_svrpxy.c:svr_Rproxy_2server:197:XXX_SERVER_RPROXY(node1):cmd=0x5 dcid=0 src=11 dst=70 snode=1 dnode=0 rcode=0 len=66
 
 SEGUNDA SESION 70,11
 722:lb_svrpxy.c:svr_Rproxy_2server:199:XXX_SERVER_RPROXY(node1):c_flags=0x2 c_src_pid=806 c_dst_pid=-1
 722:lb_svrpxy.c:svr_Rproxy_2server:190:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=70 se_clt_PID=816
 722:lb_svrpxy.c:svr_Rproxy_2server:192:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 722:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=70 
 722:lb_svrpxy.c:svr_Rproxy_2server:197:XXX_SERVER_RPROXY(node1):cmd=0x5 dcid=0 src=11 dst=70 snode=1 dnode=0 rcode=0 len=66
 
 LA ENCONTRO 
 722:lb_svrpxy.c:svr_Rproxy_2server:199:XXX_SERVER_RPROXY(node1):c_flags=0x2 c_src_pid=806 c_dst_pid=-1
 722:lb_svrpxy.c:svr_Rproxy_2server:267:SERVER_RPROXY(node1): Active Session Found with client node2
 722:lb_svrpxy.c:svr_Rproxy_2server:269:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=70 se_clt_PID=816
 722:lb_svrpxy.c:svr_Rproxy_2server:271:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=806
 722:lb_svrpxy.c:svr_Rproxy_2server:273:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=70 
  722:lb_svrpxy.c:svr_Rproxy_cltmq:457:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66
 722:lb_svrpxy.c:svr_Rproxy_cltmq:464:SERVER_RPROXY(node1): maxbytes=198 my_bytes=198
 722:lb_svrpxy.c:svr_Rproxy_cltmq:467:SERVER_RPROXY(node1): sending 198 maxbytes to node2 from mqid=32769
 

 723:lb_cltpxy.c:clt_Sproxy_mqrcv:782:CLIENT_SPROXY(node2): 198 bytes received
 723:lb_cltpxy.c:clt_Sproxy_serving:641:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66
 723:lb_cltpxy.c:clt_Sproxy_serving:643:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 723:lb_cltpxy.c:clt_Sproxy_send:666:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66
 723:lb_cltpxy.c:clt_Sproxy_send:668:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 723:lb_cltpxy.c:clt_Sproxy_sndhdr:697:CLIENT_SPROXY(node2): send header=132 
 723:lb_cltpxy.c:clt_Sproxy_sndhdr:701:CLIENT_SPROXY(node2): cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66
 723:lb_cltpxy.c:clt_Sproxy_sndhdr:703:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 723:lb_cltpxy.c:clt_Sproxy_sndhdr:721:CLIENT_SPROXY(node2): sent header=132 
 723:lb_cltpxy.c:clt_Sproxy_send:676:CLIENT_SPROXY(node2): send payload len=66
 723:lb_cltpxy.c:clt_Sproxy_sndpay:744:CLIENT_SPROXY(node2): clt_Sproxy_sndpay bytesleft=66 
 723:lb_cltpxy.c:clt_Sproxy_sndpay:748:CLIENT_SPROXY(node2): src=11 dst=70 rqtr=11 saddr=0xb745a000 daddr=0xb7495000 bytes=1024 
 723:lb_cltpxy.c:clt_Sproxy_sndpay:753:CLIENT_SPROXY(node2): payload sent=66 
 723:lb_cltpxy.c:clt_Sproxy_sndpay:767:CLIENT_SPROXY(node2): sent payload=66 
 723:lb_cltpxy.c:clt_Sproxy_serving:635:CLIENT_SPROXY(node2): Reading message queue..
 723:lb_cltpxy.c:clt_Sproxy_mqrcv:778:CLIENT_SPROXY(node2): reading from mqid=32769 
 722:lb_svrpxy.c:svr_Rproxy_getcmd:322:SERVER_RPROXY(node1): 
 722:lb_svrpxy.c:svr_Rproxy_getcmd:327:SERVER_RPROXY(node1): About to receive header
 
 EN NODE2  EL PROXY REPORTA
 ERROR: lz4tcp_proxy_bat.c:pr_process_message:392: rcode=-310 EDVSNOTBIND

 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 769: Waiting a message
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66 !!! LLEGA DESDE EL 11 NO DEL 10!!
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x5 dcid=0 src=11 dst=70 snode=0 dnode=2 rcode=0 len=66
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x2 c_snd_seq=806 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 770 c_timestamp=1625925565.924074698
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 770 c_flags=0x2 c_src_pid=806 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:339:RPROXY: src=11 dst=70 rqtr=11 saddr=0xb745a000 daddr=0xb7495000 bytes=1024 
 lz4tcp_proxy_bat.c:pr_receive_payload:267:pl_size=66
 lz4tcp_proxy_bat.c:pr_receive_payload:270:recv=66
 lz4tcp_proxy_bat.c:pr_receive_payload:272:RPROXY: n:66 | received:66
 lz4tcp_proxy_bat.c:decompress_payload:165:RPROXY: INPUT p_maxRsize=65536 p_maxCsize=65559 
 lz4tcp_proxy_bat.c:decompress_payload:170:RPROXY: INPUT comp_len=66 raw_len=65536 
 lz4tcp_proxy_bat.c:decompress_payload:182:RPROXY: OUTPUT raw_len=1024 comp_len=66
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 770:dvk_put2lcl:478: 
DEBUG 770:dvk_put2lcl:490: ioctl ret=-1 errno=310
DEBUG 770:dvk_put2lcl:495: ioctl ret=-310
 lz4tcp_proxy_bat.c:pr_process_message:388:RPROXY: REMOTE CLIENT BINDING rcode=-310
 lz4tcp_proxy_bat.c:pr_process_message:391:RPROXY: src=11 <= dvs_ptr->d_nr_sysprocs
 lz4tcp_proxy_bat.c:pr_start_serving:540:RPROXY: Message processing failure [-310]
 
 
 LISTO. FUNCIONO!! 
  		NODE2				NODE0			NODE1
		CLIENT(70)-------- LB(10)----------SERVER(10)  FUNCIONA OK 
		CLIENT(71)-------- LB(10)----------SERVER(11)  FUNCIONA OK  
 
 SE PROBO MANTENER LOS CLIENTES ENVIANDO MENSAJES Y MATAR AMBOS SERVIDORES
 PARA DESPUES VOLVERLOS ARRANCAR Y TODO FUNCIONA BIEN!!!
  
  CUANDO SE CONECTA SPREAD se puede especificar un GRUPO PRIVADO. 
  En el ejemplo de RADAR 
  938:radar.c:connect_to_spread:225:rad_mbr_name RADAR00.01: connected to 4803 with private group "#RADAR00.01#node1"
   938:radar.c:radar_loop:307:RDISK0: sender=RDISK00 Private_group=#RADAR00.01#node1 service_type=4352
   rcode = SP_connect_timeout( Spread_name, r_ptr->rad_mbr_name , 0, 1, 
                               &r_ptr->rad_mbox, 
							   r_ptr->rad_priv_group,  <<<<<<<<<<<<<<<<<<<<<<<<<<
							   test_timeout );
							   
 private_name
This allows you to choose a specifc private name for your private mailbox. This name can be used for unicast messages.
 
  
TODO: En principio usar SSHPASS pero luego usar el mismo spread para que ejecute en forma remota y de paso hace el wait4unbind del proceso

Si no se especifica prog en el archivo de configuración, no hace nada, pero si se especifica entonces se ejecuta

Crear una shell en el server:
		- obtener el PID del DC0 (se puede obviar)
		- hacer el bind del client 
				cd /usr/src/dvs/dvk-tests/
				 ./test_rmtbind <dcid> <cltep> <nodeid> "clt_DD_EEE_NN" 
				 siendo DD el DCID 
				 EEE el endpoint
				 NN el nodeid 
				 cd /usr/src/dvs/dvs-apps/dvs_run/
				 ./migr_server <dcid> <svrep> 

ERROR: 
	Se arranca el programa en forma remota pero da error el client 
	posiblemente no este bindeado el server totalmente 
 
en el NODE1 SERVER se recibe el mensaje SENDREC del CLIENT 
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x3 dcid=0 src=70 dst=10 snode=0 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=707 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x3 dcid=0 src=70 dst=10 snode=0 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=707 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 658 c_timestamp=1625958411.989813442
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 658 c_flags=0x0 c_src_pid=707 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:335:RPROXY: source=70 type=16077022 m1i1=1024 m1i2=0 m1i3=0 m1p1=0xb742d000 m1p2=(nil) m1p3=(nil) 
 
 INTENGA PONER EL MENSAJE EN EL DVK 
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 658:dvk_put2lcl:478: 
DEBUG 658:dvk_put2lcl:490: ioctl ret=-1 errno=310 <<<<<<<<<<<<<<<<<<<<<<<<<   EDVSNOTBIND
DEBUG 658:dvk_put2lcl:495: ioctl ret=-310
 lz4tcp_proxy_bat.c:pr_process_message:388:RPROXY: REMOTE CLIENT BINDING rcode=-310 <<<<< REALIZA EL AUTOBINDING 
DEBUG 658:dvk_bind_X:1221: cmd=2 dcid=0 pid=5366858 endpoint=70 nodeid=0
DEBUG 658:dvk_bind_X:1235: ioctl ret=70 errno=0
DEBUG 658:dvk_bind_X:1248: ret=70 errno=0
DEBUG 658:dvk_bind_X:1253: ioctl ret=70
DEBUG 658:dvk_bind_X:1256: ret=70

INTENTA REENVIAR EL MENSAJE AL DVK 
lz4tcp_proxy_bat.c:pr_process_message:441:RPROXY: put2lcl (autobind)
DEBUG 658:dvk_put2lcl:478: 
DEBUG 658:dvk_put2lcl:490: ioctl ret=-1 errno=109 <<<< EDVSDSTDIED
DEBUG 658:dvk_put2lcl:495: ioctl ret=-109

ESTO QUIERE DECIR QUE TODAVIA NO SE ARRANCO LA SHELL run_server.sh 

=====================================================================================
20210711:
		Tratando de ejecutar /usr/src/dvs/dvs-apps/lb_dvs/run_server.sh

 
Si en el archivo de configuracion se define 
service m3ftp2 {
	ext_ep	21;
	min_ep	21;
	max_ep	30;	
	bind	external;
	#no de define prog 
};
Esto significa que es un servidor que atiende multiples sesiones 
De lo contrario, el servidor es arrancado y finalizado por cada sesion

Aca se puede ver como cambia el PID del server que es iniciado
automaticamente al iniciarse la secione del CLIENT 
y luego finaliza el CLIENT. Cuando la nueva sesion comienza
entonces MATA el server anterior y arranca el nuevo.
	root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  10    10   691/2      1    8   20 31438 27342 27342 27342 migr_server    
	 0  70    70    -1/-1     0 1000    0 27342 27342 27342 27342 clt_0_0_70     
	root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  10    10   716/3      1    8   20 31438 27342 27342 27342 migr_server    
	 0  70    70    -1/-1     0 1000    0 27342 27342 27342 27342 clt_0_0_70  

PRIMER SESION -> INICIA EL SERVER 
 1319:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2): se_sshpass=sshpass -p 'root' ssh root@node1 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR  /usr/src/dvs/dvs-apps/lb_dvs/run_server.sh 0 10 0 70 > /tmp/node1.out 2> /tmp/node1.err
 1319:lb_cltpxy.c:clt_Rproxy_2server:373:CLIENT_RPROXY(node2): NEW Session with server node1 

SEGUNDA SESION -> DETECTA SESION EXPIRADA (CLIENT PID DIFERENTE) -> HACE KILL AL SERVER 
1319:lb_cltpxy.c:clt_Rproxy_2server:232:CLIENT_RPROXY(node2): Expired Session Found se_clt_PID=708 c_src_pid=712
 1319:lb_cltpxy.c:clt_Rproxy_2server:247:CLIENT_RPROXY(node2): se_sshpass=sshpass -p 'root' ssh root@node1 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR  kill -s SIGKILL 691 >> /tmp/node1.out 2>> /tmp/node1.err
ARRANCA EL SERVER 
 1319:lb_cltpxy.c:clt_Rproxy_2server:334:CLIENT_RPROXY(node2): se_sshpass=sshpass -p 'root' ssh root@node1 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR  /usr/src/dvs/dvs-apps/lb_dvs/run_server.sh 0 10 0 70 > /tmp/node1.out 2> /tmp/node1.err
 1319:lb_cltpxy.c:clt_Rproxy_2server:373:CLIENT_RPROXY(node2): NEW Session with server node1

 
=====================================================================================
20210718:

LISTO: Agregar parametros a lb {lowwater, highwater, period} ue seran pasados por spread a los LBA 

LISTO: Embeber el codigo del LBM 

LISTO: Analizar la seleccion del nodo.
			si el primer nodo esta saturado, pasar al siguiente-
			Si todos estan saturados disparar una creacion de VM.
			
 797:lb_monitor.c:lb_monitor:23:LB MONITOR: Initializing...
 797:lb_monitor.c:connect_to_spread:82:spread_group=LBGROUP
 797:lb_monitor.c:connect_to_spread:86:lb_mbr_name=LBM.00
 797:lb_monitor.c:connect_to_spread:98:lb_mbr_name LBM.00: connected to 4803 with private group #LBM.00#node0
 797:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 797:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=0 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 797:lb_monitor.c:lbm_loop:134:SP_receive: node0
 797:lb_monitor.c:lbm_loop:143:ret=56
 797:lb_monitor.c:lbm_loop:165:node0: sender=LBGROUP Private_group=#LBM.00#node0 service_type=4352
 797:lb_monitor.c:lbm_loop:189:node0: Received REGULAR membership for group LBGROUP with 1 members, where I am member 0:
 797:lb_monitor.c:update_members:559:
 797:lb_monitor.c:update_members:564:	#LBM.00#node0
 797:lb_monitor.c:lbm_loop:199:node0: Due to the JOIN of #LBM.00#node0 service_type=4352
 797:lb_monitor.c:lbm_join:352:member=#LBM.00#node0
 797:lb_monitor.c:get_nodeid:251:mbr_string=#LBM.00#node0
 797:lb_monitor.c:get_nodeid:263:mbr_string=#LBM.00#node0 nid=0
 797:lb_monitor.c:lb_monitor:33:rcode=0
 797:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 797:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=0 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 797:lb_monitor.c:lbm_loop:134:SP_receive: node0
 	

=====================================================================================
20210720:	
		Se hizo el LB AGENT 

PARA ARRANCAR SPREAD 
mkdir /var/run/spread
/usr/local/sbin/spread -c /etc/spread.conf > /dev/shm/spread.txt &

JOIN DEL MONITOR 
 716:lb_monitor.c:lb_monitor:23:LB MONITOR: Initializing...
 716:lb_monitor.c:connect_to_spread:82:spread_group=LBGROUP
 716:lb_monitor.c:connect_to_spread:86:lb_mbr_name=LBM.00
 716:lb_monitor.c:connect_to_spread:98:lb_mbr_name LBM.00: connected to 4803 with private group #LBM.00#node0
 716:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 716:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=0 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lbm_SP_receive:134:node0
 716:lb_monitor.c:lbm_SP_receive:143:ret=56
 716:lb_monitor.c:lbm_SP_receive:165:node0: sender=LBGROUP Private_group=#LBM.00#node0 service_type=4352
 716:lb_monitor.c:lbm_SP_receive:189:node0: Received REGULAR membership for group LBGROUP with 1 members, where I am member 0:
 716:lb_monitor.c:lbm_udt_members:560:
 716:lb_monitor.c:lbm_udt_members:565:	#LBM.00#node0
 716:lb_monitor.c:lbm_SP_receive:199:node0: Due to the JOIN of #LBM.00#node0 service_type=4352
 716:lb_monitor.c:lbm_join:353:member=#LBM.00#node0
 716:lb_monitor.c:get_nodeid:251:mbr_string=#LBM.00#node0
 716:lb_monitor.c:get_nodeid:263:mbr_string=#LBM.00#node0 nid=0
 716:lb_monitor.c:lb_monitor:33:rcode=0
 716:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 716:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=0 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lbm_SP_receive:134:node0

JOIN DEL AGENT EN MONITOR 
 716:lb_monitor.c:lbm_SP_receive:143:ret=56
 716:lb_monitor.c:lbm_SP_receive:165:node0: sender=LBGROUP Private_group=#LBM.00#node0 service_type=4352
 716:lb_monitor.c:lbm_SP_receive:189:node0: Received REGULAR membership for group LBGROUP with 2 members, where I am member 0:
 716:lb_monitor.c:lbm_udt_members:560:
 716:lb_monitor.c:lbm_udt_members:565:	#LBM.00#node0
 716:lb_monitor.c:lbm_udt_members:565:	#LBA.01#node1
 716:lb_monitor.c:lbm_SP_receive:199:node0: Due to the JOIN of #LBA.01#node1 service_type=4352
 716:lb_monitor.c:lbm_join:353:member=#LBA.01#node1
 716:lb_monitor.c:get_nodeid:251:mbr_string=#LBA.01#node1
 716:lb_monitor.c:get_nodeid:263:mbr_string=#LBA.01#node1 nid=1
 716:lb_monitor.c:lbm_join:365:BEFORE JOIN lb_name=node0 lb_nodeid=0 lb_nr_nodes=0 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:mcast_thresholds:548:source=0 type=4369 m1i1=30 m1i2=70 m1i3=30 m1p1=0x194e008 m1p2=0x7d m1p3=0x48d000 
 716:lb_monitor.c:lbm_join:369:AFTER  JOIN lb_name=node0 lb_nodeid=0 lb_nr_nodes=3 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lb_monitor:33:rcode=0
 716:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 716:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=3 lb_nr_init=0 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lbm_SP_receive:134:node0
 716:lb_monitor.c:lbm_SP_receive:143:ret=76
 716:lb_monitor.c:lbm_SP_receive:165:node0: sender=#LBM.00#node0 Private_group=#LBM.00#node0 service_type=4
 716:lb_monitor.c:lbm_SP_receive:172:node0: FIFO message from #LBM.00#node0, of type 4369, (endian 0) to 1 groups (76 bytes)
 716:lb_monitor.c:get_nodeid:251:mbr_string=#LBM.00#node0
 716:lb_monitor.c:get_nodeid:263:mbr_string=#LBM.00#node0 nid=0
 716:lb_monitor.c:lbm_reg_msg:281:sender_ptr=#LBM.00#node0 agent_id=0
 716:lb_monitor.c:lb_monitor:33:rcode=0
 716:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 716:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=3 lb_nr_init=3 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lbm_SP_receive:134:node0
 716:lb_monitor.c:lbm_SP_receive:143:ret=56
 716:lb_monitor.c:lbm_SP_receive:165:node0: sender=LBGROUP Private_group=#LBM.00#node0 service_type=5120
 716:lb_monitor.c:lbm_SP_receive:189:node0: Received REGULAR membership for group LBGROUP with 1 members, where I am member 0:
 716:lb_monitor.c:lbm_udt_members:560:
 716:lb_monitor.c:lbm_udt_members:565:	#LBM.00#node0
 716:lb_monitor.c:lbm_SP_receive:210:node0: Due to the LEAVE or DISCONNECT of #LBA.01#node1
 716:lb_monitor.c:lbm_leave:382:member=#LBA.01#node1
 716:lb_monitor.c:get_nodeid:251:mbr_string=#LBA.01#node1
 716:lb_monitor.c:get_nodeid:263:mbr_string=#LBA.01#node1 nid=1
 716:lb_monitor.c:lb_monitor:33:rcode=0
 716:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 716:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=3 lb_nr_init=3 lb_bm_nodes=0 lb_bm_init=0
 716:lb_monitor.c:lbm_SP_receive:134:node0
 
 JOIN DEL AGENT 
  757:lb_agent.c:main:79:local_nodeid=1
 757:lb_agent.c:main:83:d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
 757:lb_agent.c:init_spread:131:Spread library version is 5.0.1
 757:lb_agent.c:connect_to_spread:148:spread_group=LBGROUP
 757:lb_agent.c:connect_to_spread:152:lba_mbr_name=LBA.01
 757:lb_agent.c:connect_to_spread:164:lba_mbr_name LBA.01: connected to 4803 with private group #LBA.01#node1
 757:lb_agent.c:main:93:lba_mbr_name=LBA.01 lba_monitor=-1 lba_lowwater=-1 lba_highwater=-1 lba_period=-1
 757:lb_agent.c:main:94:lba_mbr_name=LBA.01 lba_monitor=-1 lba_nr_nodes=0 lba_nr_init=0 lba_bm_nodes=0 lba_bm_init=0
 757:lb_agent.c:lba_SP_receive:209:LBA.01
 757:lb_agent.c:lba_SP_receive:217:ret=56
 757:lb_agent.c:lba_SP_receive:239:LBA.01: sender=LBGROUP Private_group=#LBA.01#node1 service_type=4352
 757:lb_agent.c:lba_SP_receive:264:LBA.01: Received REGULAR membership for group LBGROUP with 2 members, where I am member 1:
 757:lb_agent.c:lba_udt_members:587:
 757:lb_agent.c:lba_udt_members:592:	#LBM.00#node0
 757:lb_agent.c:lba_udt_members:592:	#LBA.01#node1
 757:lb_agent.c:lba_SP_receive:274:LBA.01: Due to the JOIN of #LBA.01#node1 service_type=4352
 757:lb_agent.c:lba_join:430:member=#LBA.01#node1
 757:lb_agent.c:get_nodeid:326:mbr_string=#LBA.01#node1
 757:lb_agent.c:get_nodeid:338:mbr_string=#LBA.01#node1 nid=1
 757:lb_agent.c:lba_join:432:BEFORE lba_mbr_name=LBA.01 lba_monitor=-1 lba_nr_nodes=0 lba_nr_init=0 lba_bm_nodes=0 lba_bm_init=0
 757:lb_agent.c:main:97:rcode=0
 757:lb_agent.c:main:93:lba_mbr_name=LBA.01 lba_monitor=-1 lba_lowwater=-1 lba_highwater=-1 lba_period=-1
 757:lb_agent.c:main:94:lba_mbr_name=LBA.01 lba_monitor=-1 lba_nr_nodes=1 lba_nr_init=0 lba_bm_nodes=2 lba_bm_init=0
 
 757:lb_agent.c:lba_SP_receive:209:LBA.01
 758:lb_agent.c:get_metrics:607:lba_mbr_name=LBA.01 lba_monitor=-1 lba_lowwater=-1 lba_highwater=-1 lba_period=-1
 758:lb_agent.c:get_metrics:611:
 757:lb_agent.c:lba_SP_receive:217:ret=76
 757:lb_agent.c:lba_SP_receive:239:LBA.01: sender=#LBM.00#node0 Private_group=#LBA.01#node1 service_type=4
 757:lb_agent.c:lba_SP_receive:246:LBA.01: FIFO message from #LBM.00#node0, of type 4369, (endian 0) to 1 groups (76 bytes)
 757:lb_agent.c:get_nodeid:326:mbr_string=#LBM.00#node0
 757:lb_agent.c:get_nodeid:338:mbr_string=#LBM.00#node0 nid=0
 757:lb_agent.c:lba_reg_msg:356:sender_ptr=#LBM.00#node0 node_id=0
 757:lb_agent.c:lba_reg_msg:376:source=0 type=4369 m1i1=30 m1i2=70 m1i3=30 m1p1=0x194e008 m1p2=0x7d m1p3=0x48d000 

user: normal processes executing in user mode
nice: niced processes executing in user mode
system: processes executing in kernel mode
idle: twiddling thum
iowait: waiting for I/O to complete
irq: servicing interrupts
softirq: servicing softirqs


 1025:lb_monitor.c:lbm_SP_receive:143:ret=76
 1025:lb_monitor.c:lbm_SP_receive:165:node0: sender=#LBA.01#node1 Private_group=#LBM.00#node0 service_type=4
 1025:lb_monitor.c:lbm_SP_receive:172:node0: FIFO message from #LBA.01#node1, of type 8738, (endian 0) to 1 groups (76 bytes)
 1025:lb_monitor.c:get_nodeid:251:mbr_string=#LBA.01#node1
 1025:lb_monitor.c:get_nodeid:263:mbr_string=#LBA.01#node1 nid=1
 1025:lb_monitor.c:lbm_reg_msg:283:sender_ptr=#LBA.01#node1 agent_id=1 msg_type=8738
 1025:lb_monitor.c:lbm_reg_msg:307:source=1 type=8738 m1i1=2 m1i2=100 m1i3=0 m1p1=0xe19008 m1p2=0x71 m1p3=0x43d000 
 
 UTILIZACION 100% - LVL_SATURATED
 1025:lb_monitor.c:lbm_lvlchg_msg:331:source=1 type=8738 m1i1=2 m1i2=100 m1i3=0 m1p1=0xe19008 m1p2=0x71 m1p3=0x43d000 
 
 1025:lb_monitor.c:lb_monitor:33:rcode=0
 1025:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 1025:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=1 lb_nr_init=1 lb_bm_nodes=2 lb_bm_init=2
 1025:lb_monitor.c:lbm_SP_receive:134:node0
 1025:lb_monitor.c:lbm_SP_receive:143:ret=76
 1025:lb_monitor.c:lbm_SP_receive:165:node0: sender=#LBA.01#node1 Private_group=#LBM.00#node0 service_type=4
 1025:lb_monitor.c:lbm_SP_receive:172:node0: FIFO message from #LBA.01#node1, of type 8738, (endian 0) to 1 groups (76 bytes)
 1025:lb_monitor.c:get_nodeid:251:mbr_string=#LBA.01#node1
 1025:lb_monitor.c:get_nodeid:263:mbr_string=#LBA.01#node1 nid=1
 1025:lb_monitor.c:lbm_reg_msg:283:sender_ptr=#LBA.01#node1 agent_id=1 msg_type=8738
 1025:lb_monitor.c:lbm_reg_msg:307:source=1 type=8738 m1i1=0 m1i2=9 m1i3=0 m1p1=0xe19008 m1p2=0x6f m1p3=0x43d000 
 
 UTILIZACION 9% - LVL_IDLE
 1025:lb_monitor.c:lbm_lvlchg_msg:331:source=1 type=8738 m1i1=0 m1i2=9 m1i3=0 m1p1=0xe19008 m1p2=0x6f m1p3=0x43d000 
 
 1025:lb_monitor.c:lb_monitor:33:rcode=0
 1025:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 1025:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=1 lb_nr_init=1 lb_bm_nodes=2 lb_bm_init=2
 1025:lb_monitor.c:lbm_SP_receive:134:node0

ESTO SE VE EN EL AGENT 
 1699:lb_agent.c:get_CPU_usage:720:cpu_usage=100 cpu_idle=0
 1699:lb_agent.c:get_CPU_usage:720:cpu_usage=9 cpu_idle=91

LISTO: Modificar el monitor de tal forma que cuando se conecta el MONITOR despues del AGENT 
el AGENT le envia un mensaje con las métricas.
eL MONITOR entonces lo agrega en el bitmap 
debe verificar que es efectivamente un server.

=====================================================================================
20210721:

		
		1) Primero arranca el MONITOR luego el AGENT 
			Se probaron caidas en el AGENT sin problemas.
			Probar caida del Monitor
			
		2) Primero arranca el MONITOR luego el AGENT 
			Se probaron caidas en el AGENT sin problemas.		
			
		3) Se probo NETWORK PARTITION Y NETWORK MERGE
		

=====================================================================================
20210729:   NODE1(CLIENT)----NODE0(LB)----NODE2(SERVER)

 
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:497:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:500:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:502:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 778:lb_cltpxy.c:clt_Rproxy_getcmd:425:CLIENT_RPROXY(node2): header bytes=132
 778:lb_cltpxy.c:clt_Rproxy_getcmd:433:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
 778:lb_cltpxy.c:clt_Rproxy_getcmd:435:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 778:lb_cltpxy.c:clt_Rproxy_getcmd:437:CLIENT_RPROXY(node2): c_timestamp=1627568463.649465317
 778:lb_cltpxy.c:clt_Rproxy_getcmd:439:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=725 c_dst_pid=-1
 778:lb_cltpxy.c:clt_Rproxy_loop:130:CLIENT_RPROXY(node2):Message succesfully processed.
 778:lb_cltpxy.c:clt_Rproxy_2server:220:CLIENT_RPROXY(node2): 
 778:lb_cltpxy.c:clt_Rproxy_2server:333:CLIENT_RPROXY(node2): NEW Session with server node1
 778:lb_cltpxy.c:clt_Rproxy_2server:335:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=725
 778:lb_cltpxy.c:clt_Rproxy_2server:337:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 778:lb_cltpxy.c:clt_Rproxy_2server:339:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
 778:lb_cltpxy.c:clt_Rproxy_svrmq:573:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
 778:lb_cltpxy.c:clt_Rproxy_svrmq:583:CLIENT_RPROXY(node2): sending 132 bytes to server node1 by mqid=0
 778:lb_cltpxy.c:clt_Rproxy_loop:125:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 778:lb_cltpxy.c:clt_Rproxy_getcmd:418:CLIENT_RPROXY(node2): 
 778:lb_cltpxy.c:clt_Rproxy_getcmd:423:CLIENT_RPROXY(node2): About to receive header
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:490:CLIENT_RPROXY(node2): lbp_csd=5 
 
 775:lb_svrpxy.c:svr_Sproxy_mqrcv:769:SERVER_SPROXY(node1): 132 bytes received
 775:lb_svrpxy.c:svr_Sproxy_serving:632:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
 775:lb_svrpxy.c:svr_Sproxy_serving:634:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 775:lb_svrpxy.c:svr_Sproxy_send:656:SERVER_SPROXY(node1): 
 775:lb_svrpxy.c:svr_Sproxy_sndhdr:685:SERVER_SPROXY(node1): send header=132 
 775:lb_svrpxy.c:svr_Sproxy_sndhdr:689:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
 775:lb_svrpxy.c:svr_Sproxy_sndhdr:691:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 
 776:lb_svrpxy.c:svr_Rproxy_rcvhdr:413:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 776:lb_svrpxy.c:svr_Rproxy_rcvhdr:416:SERVER_RPROXY (node1): cmd=0x4 dcid=0 src=10 dst=50 snode=1 dnode=0 rcode=0 len=0
 776:lb_svrpxy.c:svr_Rproxy_rcvhdr:418:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=731 c_ack_seq=-1
 776:lb_svrpxy.c:svr_Rproxy_getcmd:351:SERVER_RPROXY(node1): cmd=0x4 dcid=0 src=10 dst=50 snode=1 dnode=0 rcode=0 len=0
 776:lb_svrpxy.c:svr_Rproxy_getcmd:353:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=731 c_ack_seq=-1
 776:lb_svrpxy.c:svr_Rproxy_getcmd:355:SERVER_RPROXY(node1): c_timestamp=1627568463.670745673
 776:lb_svrpxy.c:svr_Rproxy_getcmd:357:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=731 c_dst_pid=-1
 776:lb_svrpxy.c:svr_Rproxy_loop:122:SERVER_RPROXY(node1):Message succesfully processed.
 776:lb_svrpxy.c:svr_Rproxy_2server:169:SERVER_RPROXY(node1): 
 776:lb_svrpxy.c:svr_Rproxy_2server:191:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=725
 776:lb_svrpxy.c:svr_Rproxy_2server:193:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 776:lb_svrpxy.c:svr_Rproxy_2server:195:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
 776:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):cmd=0x4 dcid=0 src=10 dst=50 snode=1 dnode=0 rcode=0 len=0
 776:lb_svrpxy.c:svr_Rproxy_2server:200:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=731 c_dst_pid=-1
 776:lb_svrpxy.c:svr_Rproxy_2server:285:SERVER_RPROXY(node1): Active Session Found with client node2
 776:lb_svrpxy.c:svr_Rproxy_2server:287:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=725
 776:lb_svrpxy.c:svr_Rproxy_2server:289:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=731
 776:lb_svrpxy.c:svr_Rproxy_2server:291:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
 776:lb_svrpxy.c:svr_Rproxy_cltmq:488:SERVER_RPROXY(node1): cmd=0x4 dcid=0 src=10 dst=50 snode=0 dnode=2 rcode=0 len=0
 776:lb_svrpxy.c:svr_Rproxy_cltmq:498:SERVER_RPROXY(node1): sending 132 maxbytes to node2 from mqid=32769
 776:lb_svrpxy.c:svr_Rproxy_getcmd:340:SERVER_RPROXY(node1): 
 776:lb_svrpxy.c:svr_Rproxy_getcmd:345:SERVER_RPROXY(node1): About to receive header
 
 777:lb_cltpxy.c:clt_Sproxy_mqrcv:859:CLIENT_SPROXY(node2): 132 bytes received
 777:lb_cltpxy.c:clt_Sproxy_serving:718:CLIENT_SPROXY(node2): cmd=0x4 dcid=0 src=10 dst=50 snode=0 dnode=2 rcode=0 len=0
 777:lb_cltpxy.c:clt_Sproxy_serving:720:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=731 c_ack_seq=-1
 777:lb_cltpxy.c:clt_Sproxy_send:743:CLIENT_SPROXY(node2): cmd=0x4 dcid=0 src=10 dst=50 snode=0 dnode=2 rcode=0 len=0
 777:lb_cltpxy.c:clt_Sproxy_send:745:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=731 c_ack_seq=-1
 777:lb_cltpxy.c:clt_Sproxy_sndhdr:774:CLIENT_SPROXY(node2): send header=132 
 777:lb_cltpxy.c:clt_Sproxy_sndhdr:778:CLIENT_SPROXY(node2): cmd=0x4 dcid=0 src=10 dst=50 snode=0 dnode=2 rcode=0 len=0
 777:lb_cltpxy.c:clt_Sproxy_sndhdr:780:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=731 c_ack_seq=-1
 
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:497:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:500:CLIENT_RPROXY (node2): cmd=0x2001 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
 778:lb_cltpxy.c:clt_Rproxy_rcvhdr:502:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 778:lb_cltpxy.c:clt_Rproxy_getcmd:425:CLIENT_RPROXY(node2): header bytes=132
 778:lb_cltpxy.c:clt_Rproxy_getcmd:433:CLIENT_RPROXY(node2): cmd=0x2001 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
 778:lb_cltpxy.c:clt_Rproxy_getcmd:435:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=725 c_ack_seq=-1
 778:lb_cltpxy.c:clt_Rproxy_getcmd:437:CLIENT_RPROXY(node2): c_timestamp=1627568463.669464982
 778:lb_cltpxy.c:clt_Rproxy_getcmd:439:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=725 c_dst_pid=-1
 778:lb_cltpxy.c:clt_Rproxy_loop:130:CLIENT_RPROXY(node2):Message succesfully processed.
 778:lb_cltpxy.c:clt_Rproxy_2server:220:CLIENT_RPROXY(node2): 
 
 775:lb_svrpxy.c:svr_Sproxy_sndhdr:709:SERVER_SPROXY(node1): sent header=132 
 775:lb_svrpxy.c:svr_Sproxy_serving:625:SERVER_SPROXY(node1): Reading message queue..
 775:lb_svrpxy.c:svr_Sproxy_mqrcv:765:SERVER_SPROXY(node1): reading from mqid=0
 777:lb_cltpxy.c:clt_Sproxy_sndhdr:798:CLIENT_SPROXY(node2): sent header=132 
 777:lb_cltpxy.c:clt_Sproxy_serving:712:CLIENT_SPROXY(node2): Reading message queue..
 777:lb_cltpxy.c:clt_Sproxy_mqrcv:855:CLIENT_SPROXY(node2): reading from mqid=32769 
 
 
 ERROR:  EN EL NODE1 (SERVER) DESPUES DE EJECUTAR UNA TRANSFERENCIA DESAPARECE EL ENDPOINT REMOTO DEL CLIENTE
 root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10   714/714    1    8   20 31438 27342 27342 27342 latency_server 
 0  21    21   715/715    1  400 1020 27342 27342 27342 27342 m3ftpd         
 0  50    50    -1/-1     0 1000    0 27342 27342 27342 27342 latency_client 
 0  51    51    -1/-1     0 1408 2000    21 27342 27342 27342 m3ftp   <<<<<<<<<<<<<<<<<<       
root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10   714/714    1    8   20 31438 27342 27342 27342 latency_server 
 0  21    21   715/715    1    8   20 31438 27342 27342 27342 m3ftpd         
 0  50    50    -1/-1     0 1000    0 27342 27342 27342 27342 latency_client 

PROBLEMA CON EL TAMAÑO DE LA COLA DE MENSAJES
 1043:lb_svrpxy.c:svr_Rproxy_cltmq:488:SERVER_RPROXY(node1): cmd=0x5 dcid=0 src=21 dst=51 snode=0 dnode=2 rcode=0 len=64512
 1043:lb_svrpxy.c:svr_Rproxy_cltmq:495:SERVER_RPROXY(node1): maxbytes=64644 my_bytes=64644 !!!!! ESTO ES (MAXCOPYBUF - 1024 + sizeof header)
 1043:lb_svrpxy.c:svr_Rproxy_cltmq:498:SERVER_RPROXY(node1): sending 64644 maxbytes to node2 from mqid=32769
ERROR: lb_svrpxy.c:svr_Rproxy_cltmq:501: rcode=-22
 1043:lb_svrpxy.c:svr_Rproxy_getcmd:340:SERVER_RPROXY(node1): 
 1043:lb_svrpxy.c:svr_Rproxy_getcmd:345:SERVER_RPROXY(node1): About to receive heade
 
EN M3FTP Y M3FTPD 
 #define MAXBUFLEN			4096
CUANDO SE BAJO AL TAMAÑO DE BUFFER DE DATOS DEL M3FTP A 4096 , DA COMO RESULTADO 
  1043:lb_svrpxy.c:svr_Rproxy_cltmq:495:SERVER_RPROXY(node1): maxbytes=4228 my_bytes=4228
 1043:lb_svrpxy.c:svr_Rproxy_cltmq:498:SERVER_RPROXY(node1): sending 4228 maxbytes to node2 from mqid=32769
 ENTONCES 4228= (4192+sizeof(header_t)) < 8192  (se probo con 8192 y NO FUNCIONO)
 
 
=====================================================================================

TODO: RESOLVER EL PROBLEMA DE PORQUE BORRA EL ENDPOINT REMOTO DEL M3FTP EN EL SERVER CUANDO FALLA LA TRANSFERENCIA?
TODO: RESOLVER EL TEMA DE PROXIES CON BATCH Y LZ4 
TODO: RESOLVER EL PROBLEMA DEL TAMAÑO DE MENSAJE EN COLA DE MENSAJES
	EN M3FTP Y M3FTPD 
	 #define MAXBUFLEN			4096
	CUANDO SE BAJO AL TAMAÑO DE BUFFER DE DATOS DEL M3FTP A 4096 , DA COMO RESULTADO 
	  1043:lb_svrpxy.c:svr_Rproxy_cltmq:495:SERVER_RPROXY(node1): maxbytes=4228 my_bytes=4228
	 1043:lb_svrpxy.c:svr_Rproxy_cltmq:498:SERVER_RPROXY(node1): sending 4228 maxbytes to node2 from mqid=32769
	 ENTONCES 4228= (4192+sizeof(header_t)) < 8192  (se probo con 8192 y NO FUNCIONO)


TODO: 	Resolver el problema de ARRANQUE DE spread 
Conf_load_conf_file: My name: node0, id: 25584897, addr: 192.168.0.100, port: 4803
Spread: SECURITY RISK! running as root, but unix domain socket is not in a root-only writable directory. May risk denial of service or malicious deletion of unexpected file in directory: /tmp
Spread daemon exiting normally!
		
TODO:	Controlar el estado del proxy antes de seleccionar e informarlo junto con la carga

TODO: Mencionar en el paper elasticidad 
	 Tambien que se puede utilizar como Function As A Service 
	 Que pasa cuando un cliente arranca un programa para usar una funcion y luego muere?
	 El server queda cargado sin uso. Como matarlo?
			- Timeout 
			- Viendo si el cliente sigue vivo: para eso se requiere que el proxy del cliente
			  maneje el process status 	
      Se puede utilizar con servers persistentes o efimeros 


	
=====================================================================================
20210731:
		Se quitaros los multiples locks/unlocks de las sesiones de lb_client.c_ack_seq
		
		+ Como mejorar el tema del limite de las message queues
		El proxy receiver podria hacer un malloc del buffer
		apuntarlo en el header y no usarlo mas. Se puede usar c_snd_seq​ para apuntar al buffer
		El header va por message queue
		El proxy sender recibe el header y ya sabe donde esta el buffer
		despues de enviado le hace free	

NODE0 - LB
	 NODE2(CLIENT)->SENDREC->LB(NODE0)
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:510:CLIENT_RPROXY (node2): n:92 | received:92 | HEADER_SIZE:92
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:513:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:515:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=713 c_ack_seq=-1
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:434:CLIENT_RPROXY(node2): header bytes=92
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:442:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=50 dst=10 snode=2 dnode=0 rcode=0 len=0
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:444:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=713 c_ack_seq=-1
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:446:CLIENT_RPROXY(node2): c_timestamp=1627764336.123830682
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:448:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=713 c_dst_pid=-1
	 613:lb_cltpxy.c:clt_Rproxy_loop:132:CLIENT_RPROXY(node2):Message succesfully processed.
	 613:lb_cltpxy.c:clt_Rproxy_2server:222:CLIENT_RPROXY(node2): 
	 613:lb_cltpxy.c:clt_Rproxy_2server:224:MTX_LOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_2server:287:MTX_UNLOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_2server:302:MTX_LOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:select_server:363:MTX_LOCK svr_ptr->svr_mutex 
	 613:lb_cltpxy.c:select_server:409:MTX_UNLOCK svr_ptr->svr_mutex 
	 613:lb_cltpxy.c:select_server:363:MTX_LOCK svr_ptr->svr_mutex 
	 613:lb_cltpxy.c:select_server:404:MTX_UNLOCK svr_ptr->svr_mutex 
	 613:lb_cltpxy.c:select_server:416:CLIENT_RPROXY(node2): server name=node1
	 613:lb_cltpxy.c:clt_Rproxy_2server:336:MTX_UNLOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_2server:339:CLIENT_RPROXY(node2): NEW Session with server node1
	 613:lb_cltpxy.c:clt_Rproxy_2server:341:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=713
	 613:lb_cltpxy.c:clt_Rproxy_2server:343:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
	 613:lb_cltpxy.c:clt_Rproxy_2server:345:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
	 613:lb_cltpxy.c:clt_Rproxy_loop:159:MTX_LOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_loop:161:MTX_UNLOCK sess_table[dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_svrmq:564:MTX_LOCK sess_table[hdr_ptr->c_dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_svrmq:581:MTX_UNLOCK sess_table[hdr_ptr->c_dcid].st_mutex 
	 613:lb_cltpxy.c:clt_Rproxy_svrmq:583:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
	 613:lb_cltpxy.c:clt_Rproxy_svrmq:586:CLIENT_RPROXY(node2): sending HEADER 92 bytes to server node1 by mqid=0
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:779:SERVER_SPROXY(node1): 92 bytes received
	 
	 NODE0(LB)->SENDREC->NODE1(SERVER)
	 610:lb_svrpxy.c:svr_Sproxy_serving:634:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_serving:636:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=713 c_ack_seq=-1
	 610:lb_svrpxy.c:svr_Sproxy_send:659:SERVER_SPROXY(node1): 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:695:SERVER_SPROXY(node1): send header=92 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:699:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:701:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=713 c_ack_seq=-1
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:719:SERVER_SPROXY(node1): sent header=92 
	 610:lb_svrpxy.c:svr_Sproxy_serving:627:SERVER_SPROXY(node1): Reading message queue..
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:775:SERVER_SPROXY(node1): reading from mqid=0
	 
	 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	 613:lb_cltpxy.c:clt_Rproxy_loop:127:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:427:CLIENT_RPROXY(node2): 
	 613:lb_cltpxy.c:clt_Rproxy_getcmd:432:CLIENT_RPROXY(node2): About to receive header
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:503:CLIENT_RPROXY(node2): lbp_csd=5 
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:510:CLIENT_RPROXY (node2): n:40 | received:40 | HEADER_SIZE:92 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
	 613:lb_cltpxy.c:clt_Rproxy_rcvhdr:519:CLIENT_RPROXY (node2): Header partially received. There are 52 bytes still to get<<<<<<<<<<
	 
	 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:415:SERVER_RPROXY (node1): n:24 | received:92 | HEADER_SIZE:92 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:418:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:420:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:361:SERVER_RPROXY(node1): NONE
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:366:SERVER_RPROXY(node1): Replying NONE
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:347:SERVER_RPROXY(node1): About to receive header
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:415:SERVER_RPROXY (node1): n:92 | received:92 | HEADER_SIZE:92
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:418:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:420:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:361:SERVER_RPROXY(node1): NONE
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:366:SERVER_RPROXY(node1): Replying NONE
	 611:lb_svrpxy.c:svr_Rproxy_getcmd:347:SERVER_RPROXY(node1): About to receive header
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:415:SERVER_RPROXY (node1): n:16 | received:16 | HEADER_SIZE:92
	 611:lb_svrpxy.c:svr_Rproxy_rcvhdr:424:SERVER_RPROXY (node1): Header partially received. There are 76 bytes still to get <<<<<<<<<<<
	 
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:779:SERVER_SPROXY(node1): 92 bytes received
	 610:lb_svrpxy.c:svr_Sproxy_serving:634:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_serving:636:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 610:lb_svrpxy.c:svr_Sproxy_send:659:SERVER_SPROXY(node1): 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:695:SERVER_SPROXY(node1): send header=92 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:699:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:701:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:719:SERVER_SPROXY(node1): sent header=92 
	 610:lb_svrpxy.c:svr_Sproxy_serving:627:SERVER_SPROXY(node1): Reading message queue..
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:775:SERVER_SPROXY(node1): reading from mqid=0
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:779:SERVER_SPROXY(node1): 92 bytes received
	 610:lb_svrpxy.c:svr_Sproxy_serving:634:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_serving:636:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 610:lb_svrpxy.c:svr_Sproxy_send:659:SERVER_SPROXY(node1): 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:695:SERVER_SPROXY(node1): send header=92 
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:699:SERVER_SPROXY(node1): cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:701:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 610:lb_svrpxy.c:svr_Sproxy_sndhdr:719:SERVER_SPROXY(node1): sent header=92 
	 610:lb_svrpxy.c:svr_Sproxy_serving:627:SERVER_SPROXY(node1): Reading message queue..
	 610:lb_svrpxy.c:svr_Sproxy_mqrcv:775:SERVER_SPROXY(node1): reading from mqid=0

PROXY NODE1 
	 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 730: Waiting a message
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:92 | received:92 | HEADER_SIZE:132 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 lz4tcp_proxy_bat.c:pr_receive_header:299:RPROXY: Header partially received. There are 40 bytes still to get
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:40 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 lz4tcp_proxy_bat.c:pr_process_message:327:RPROXY: 731 NONE
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:52 | received:52 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:299:RPROXY: Header partially received. There are 80 bytes still to get

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EL PROBLEMA ESTA EN LOS PROXIES VERDADEROS QUE AL CAMBIAR ipc.h POR EL ERROR DE MARIE 
SE ACHICO EL TAMAÑO DEL HEADER !!!
SUPONGO QUE HAY QUE COMPILARE EL MODULO KERNEL TAMBIEN!!
POR AHORA VUELVO ATRAS EL CAMBIO DE CORRECION DEL ERROR DE MARIE COMO PARA HACER ESTAS PRUEBAS.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

PARA VER LA MEMORIA UTILIZADA Y COMO ESTA UTILIZADAA
pmap <PID>
Controlar como usa la memoria el lb_dvs para ver si adquire y libera correctamente

 =====================================================================================
20210801:

diff pmap_before.txt pmap_after.txt 
6c6
< 01caa000    400K rw---   [ anon ]
---
> 01caa000    576K rw---   [ anon ]
9,10c9,10
< b4c00000    136K rw---   [ anon ]
< b4c22000    888K -----   [ anon ]
---
> b4c00000    144K rw---   [ anon ]
> b4c24000    880K -----   [ anon ]
60c60
<  total    46796K
---

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "Payload address" lb_dvs.out
 630:lb_cltpxy.c:clt_Rproxy_init:57:CLIENT_RPROXY(node2): Payload address=0xb4c02000
 628:lb_svrpxy.c:svr_Rproxy_init:50:SERVER_RPROXY(node1): Payload address=0x1cf8000
 630:lb_cltpxy.c:clt_Rproxy_svrmq:594:CLIENT_RPROXY(node2): Payload address=0xb4c13000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1cf8000
 628:lb_svrpxy.c:svr_Rproxy_cltmq:503:SERVER_RPROXY(node2): Payload address=0x1d09000

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep FREE lb_dvs.out
 627:lb_svrpxy.c:svr_Sproxy_send:677:FREE 0xb4c02000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1d09000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000
 629:lb_cltpxy.c:clt_Sproxy_send:772:FREE 0x1cf8000


ATENCION: 
Cuando el m3ftp hace GET HAY PROBLEMAS
Cuando el m3ftp hace PUT NO HAY PROBLEMAS

DIFERENCIA ENTRE ARRRANCAR Y GET 
	diff pmap_before.txt pmap_after.txt 
	6c6
	< 01caa000    400K rw---   [ anon ]
	---
	> 01caa000    576K rw---   [ anon ]
	9,10c9,10
	< b4c00000    136K rw---   [ anon ]
	< b4c22000    888K -----   [ anon ]
	---
	> b4c00000    144K rw---   [ anon ]
	> b4c24000    880K -----   [ anon ]
	60c60
	<  total    46796K
	---

DIFERENCIA ENTRE EL GET Y EL PUT 
	root@node0:/usr/src/dvs/dvs-apps/dvs_lb# diff pmap_after.txt pmap_after2.txt 
	
DIFERENCIA ENTRE EL PUT Y UN NUEVO GET 
	root@node0:/usr/src/dvs/dvs-apps/dvs_lb# diff pmap_after.txt pmap_after3.txt 
	6c6
	< 01caa000    576K rw---   [ anon ]
	---
	> 01caa000    440K rw---   [ anon ]
	60c60
	<  total    46972K
	---
	>  total    46836K

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep total pmap_after*
pmap_after.txt: total     46972K
pmap_after2.txt: total    46972K
pmap_after3.txt: total    46836K
pmap_after4.txt: total    46836K
pmap_after5.txt: total    46836K

FUNCIONA!! 

THROUGPUT 
	FTP_PUT: t_start=1627851892.83 t_stop=1627851896.10 t_total=3.27
	FTP_PUT: total_bytes=1048576
	FTP_PUT: Throughput = 321120.222482 [bytes/s]
	FTP_GET: t_start=1627851954.37 t_stop=1627851957.78 t_total=3.41
	FTP_GET: total_bytes=1048576
	FTP_GET: Throughput = 307875.891004 [bytes/s]

MEDICION DE LATENCIA 
	Time total elapsed in loop 999: 0.0069968700 
	***********************
	Total average latency in 1000 loops: 0.0087558341 [s] 
	Message throughput in 1000 loops: 228.42 [msg/s]
	***********************
=====================================================================================
20210808;
	Cuando se probaron 3 clients contra 3 servers, todos van a parar al mismo svr_ep


 754:lb_cltpxy.c:select_server:420:CLIENT_RPROXY(node2): server name=node1 new_ep=10
 754:lb_cltpxy.c:clt_Rproxy_2server:337:MTX_UNLOCK sess_table[dcid].st_mutex 
 754:lb_cltpxy.c:clt_Rproxy_2server:340:CLIENT_RPROXY(node2): NEW Session with server node1
 754:lb_cltpxy.c:clt_Rproxy_2server:342:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=52 se_clt_PID=720
 754:lb_cltpxy.c:clt_Rproxy_2server:344:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 754:lb_cltpxy.c:clt_Rproxy_2server:346:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=52 

 754:lb_cltpxy.c:select_server:420:CLIENT_RPROXY(node2): server name=node1 new_ep=11
 754:lb_cltpxy.c:clt_Rproxy_2server:337:MTX_UNLOCK sess_table[dcid].st_mutex 
 754:lb_cltpxy.c:clt_Rproxy_2server:340:CLIENT_RPROXY(node2): NEW Session with server node1
 754:lb_cltpxy.c:clt_Rproxy_2server:342:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=719
 754:lb_cltpxy.c:clt_Rproxy_2server:344:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 754:lb_cltpxy.c:clt_Rproxy_2server:346:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51  
 
  754:lb_cltpxy.c:select_server:420:CLIENT_RPROXY(node2): server name=node1 new_ep=12
 754:lb_cltpxy.c:clt_Rproxy_2server:337:MTX_UNLOCK sess_table[dcid].st_mutex 
 754:lb_cltpxy.c:clt_Rproxy_2server:340:CLIENT_RPROXY(node2): NEW Session with server node1
 754:lb_cltpxy.c:clt_Rproxy_2server:342:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=718
 754:lb_cltpxy.c:clt_Rproxy_2server:344:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 754:lb_cltpxy.c:clt_Rproxy_2server:346:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 

EL latency en svr_ep=10 loguea
	 latency_server.c:run_server:36:SERVER REQUEST msg:source=52 type=10 m1i1=0 m1i2=2 m1i3=3 m1p1=0x3 m1p2=(nil) m1p3=(nil) 
	DEBUG 730:dvk_send_T:866: endpoint=52 timeout=30000
	DEBUG 730:dvk_send_T:879: ioctl ret=0 errno=0
	DEBUG 730:dvk_send_T:888: ioctl ret=0

	 latency_server.c:run_server:36:SERVER REQUEST msg:source=51 type=10 m1i1=0 m1i2=2 m1i3=3 m1p1=0x3 m1p2=(nil) m1p3=(nil) 
	DEBUG 730:dvk_send_T:866: endpoint=51 timeout=30000
	DEBUG 730:dvk_send_T:879: ioctl ret=0 errno=0
	DEBUG 730:dvk_send_T:888: ioctl ret=0

	 latency_server.c:run_server:36:SERVER REQUEST msg:source=50 type=10 m1i1=0 m1i2=2 m1i3=3 m1p1=0x3 m1p2=(nil) m1p3=(nil) 
	DEBUG 730:dvk_send_T:866: endpoint=50 timeout=30000
	DEBUG 730:dvk_send_T:879: ioctl ret=0 errno=0
	DEBUG 730:dvk_send_T:888: ioctl ret=0

ACA SE DEMUESTRA QUE SOLO TRABAJO UN SERVER
root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats 
DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
 0  10    10   748/748    1        0      200        0        0 latency_server <<<<<<<<<<<<<<<<
 0  11    11   749/749    1        0        0        0        0 latency_server 
 0  12    12   750/750    1        0        0        0        0 latency_server 

EL PROBLEMA ESTA EN select_server QUE ELIGE EL new_ep PERO NO LO RETORNA EN NINGUN LADO.

 835:lb_cltpxy.c:select_server:421:CLIENT_RPROXY(node2): server name=node1 new_ep=10
 835:lb_cltpxy.c:clt_Rproxy_2server:338:MTX_UNLOCK sess_table[dcid].st_mutex 
 835:lb_cltpxy.c:clt_Rproxy_2server:341:CLIENT_RPROXY(node2): NEW Session with server node1
 835:lb_cltpxy.c:clt_Rproxy_2server:343:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=52 se_clt_PID=727
 835:lb_cltpxy.c:clt_Rproxy_2server:345:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 835:lb_cltpxy.c:clt_Rproxy_2server:347:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=52 
 
  835:lb_cltpxy.c:select_server:421:CLIENT_RPROXY(node2): server name=node1 new_ep=11
 835:lb_cltpxy.c:clt_Rproxy_2server:338:MTX_UNLOCK sess_table[dcid].st_mutex 
 835:lb_cltpxy.c:clt_Rproxy_2server:341:CLIENT_RPROXY(node2): NEW Session with server node1
 835:lb_cltpxy.c:clt_Rproxy_2server:343:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=726
 835:lb_cltpxy.c:clt_Rproxy_2server:345:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 835:lb_cltpxy.c:clt_Rproxy_2server:347:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51
 
  835:lb_cltpxy.c:select_server:421:CLIENT_RPROXY(node2): server name=node1 new_ep=12
 835:lb_cltpxy.c:clt_Rproxy_2server:338:MTX_UNLOCK sess_table[dcid].st_mutex 
 835:lb_cltpxy.c:clt_Rproxy_2server:341:CLIENT_RPROXY(node2): NEW Session with server node1
 835:lb_cltpxy.c:clt_Rproxy_2server:343:CLIENT_RPROXY(node2):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=725
 835:lb_cltpxy.c:clt_Rproxy_2server:345:CLIENT_RPROXY(node2):se_dcid=0 se_svr_nodeid=1 se_svr_ep=12 se_svr_PID=-1
 835:lb_cltpxy.c:clt_Rproxy_2server:347:CLIENT_RPROXY(node2):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 

SERVER(11)->LB(51) (SEND)
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:416:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:419:SERVER_RPROXY (node1): cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:421:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 833:lb_svrpxy.c:svr_Rproxy_getcmd:354:SERVER_RPROXY(node1): cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_getcmd:356:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 833:lb_svrpxy.c:svr_Rproxy_getcmd:358:SERVER_RPROXY(node1): c_timestamp=1628425945.545520141
 833:lb_svrpxy.c:svr_Rproxy_getcmd:360:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_loop:125:SERVER_RPROXY(node1):Message succesfully processed.
 833:lb_svrpxy.c:svr_Rproxy_2server:172:SERVER_RPROXY(node1): 
 833:lb_svrpxy.c:svr_Rproxy_2server:186:MTX_LOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=52 se_clt_PID=727
 833:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=722
 833:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=52 
 833:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=726
 833:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51 
 833:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:286:MTX_UNLOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_2server:288:SERVER_RPROXY(node1): Active Session Found with client node2
 833:lb_svrpxy.c:svr_Rproxy_2server:290:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=726
 833:lb_svrpxy.c:svr_Rproxy_2server:292:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=723
 833:lb_svrpxy.c:svr_Rproxy_2server:294:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51
 
 LB(10)->CLIENT(51) SEND 
 834:lb_cltpxy.c:clt_Sproxy_serving:733:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 834:lb_cltpxy.c:clt_Sproxy_serving:735:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 834:lb_cltpxy.c:clt_Sproxy_send:759:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 834:lb_cltpxy.c:clt_Sproxy_send:761:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 834:lb_cltpxy.c:clt_Sproxy_sndhdr:796:CLIENT_SPROXY(node2): send header=132 
 834:lb_cltpxy.c:clt_Sproxy_sndhdr:800:CLIENT_SPROXY(node2): cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 834:lb_cltpxy.c:clt_Sproxy_sndhdr:802:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 834:lb_cltpxy.c:clt_Sproxy_sndhdr:820:CLIENT_SPROXY(node2): sent header=132 
 834:lb_cltpxy.c:clt_Sproxy_serving:727:CLIENT_SPROXY(node2): Reading message queue..
 834:lb_cltpxy.c:clt_Sproxy_mqrcv:878:CLIENT_SPROXY(node2): reading from mqid=32769 
 
 LOS LATENCY CLIENT ESTAN ESPERANDO 
 root@node2:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs 
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10    -1/-1     0 1000    0 27342 27342 27342 27342 latency_server 
 0  21    21    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftpd         
 0  50    50   725/725    2    8 2020    10 27342 27342 27342 latency_client 
 0  51    51   726/726    2    8 2020    10 27342 27342 27342 latency_client 

HAY UN PROBLEMA EN EL CLIENT 

 lz4tcp_proxy_bat.c:pr_start_serving:537:RPROXY: Message succesfully processed.
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 704 c_timestamp=1628425945.545520141
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 704 c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:335:RPROXY: source=11 type=11 m1i1=0 m1i2=2 m1i3=3 m1p1=0x3 m1p2=(nil) m1p3=(nil) <<<<< 11!!!!!
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 704:dvk_put2lcl:478: 
DEBUG 704:dvk_put2lcl:490: ioctl ret=-1 errno=329
DEBUG 704:dvk_put2lcl:495: ioctl ret=-329


EN EL RPROXY DEL SERVER CUANDO ENVIA LA RESPUESTA SERVER(11)->LB(51) DEBE CONVERTIR EL HEADER Y EL MENSAJE A LB(10)->CLIENT(51) 
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:416:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:419:SERVER_RPROXY (node1): cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_rcvhdr:421:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 833:lb_svrpxy.c:svr_Rproxy_getcmd:354:SERVER_RPROXY(node1): cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_getcmd:356:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=723 c_ack_seq=-1
 833:lb_svrpxy.c:svr_Rproxy_getcmd:358:SERVER_RPROXY(node1): c_timestamp=1628425945.545520141
 833:lb_svrpxy.c:svr_Rproxy_getcmd:360:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_loop:125:SERVER_RPROXY(node1):Message succesfully processed.
 833:lb_svrpxy.c:svr_Rproxy_2server:172:SERVER_RPROXY(node1): 
  833:lb_svrpxy.c:svr_Rproxy_2server:186:MTX_LOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=52 se_clt_PID=727
 833:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=722
 833:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=52 
 833:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=726
 833:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51 
 833:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x1 dcid=0 src=11 dst=51 snode=1 dnode=0 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=723 c_dst_pid=-1
 833:lb_svrpxy.c:svr_Rproxy_2server:286:MTX_UNLOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_2server:288:SERVER_RPROXY(node1): Active Session Found with client node2
 833:lb_svrpxy.c:svr_Rproxy_2server:290:SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=726
 833:lb_svrpxy.c:svr_Rproxy_2server:292:SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=723
 833:lb_svrpxy.c:svr_Rproxy_2server:294:SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=51 
 833:lb_svrpxy.c:svr_Rproxy_loop:133:MTX_LOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_loop:135:MTX_UNLOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_cltmq:473:MTX_LOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_cltmq:490:MTX_UNLOCK sess_table[dcid].st_mutex 
 833:lb_svrpxy.c:svr_Rproxy_cltmq:491:SERVER_RPROXY(node1): cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 833:lb_svrpxy.c:svr_Rproxy_cltmq:494:SERVER_RPROXY(node1): sending HEADER 132 bytes to node2 from mqid=32769
 833:lb_svrpxy.c:svr_Rproxy_getcmd:343:SERVER_RPROXY(node1): 
 833:lb_svrpxy.c:svr_Rproxy_getcmd:348:SERVER_RPROXY(node1): About to receive header
 834:lb_cltpxy.c:clt_Sproxy_mqrcv:882:CLIENT_SPROXY(node2): 132 bytes received
 
-----------------------------------------------------------------------------------------
UNA VEZ CORREGIDO ALGO CAMBIO- 
EN EL SERVER   
root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats 
DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
 0  10    10   716/716    1        0      100        0        0 latency_server 
 0  11    11   717/717    1        0        1        0        0 latency_server 
 0  12    12   718/718    1        0       98        0        0 latency_server 

EN EL CLIENTE 
root@node2:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10    -1/-1     0 1000    0 27342 27342 27342 27342 latency_server 
 0  21    21    -1/-1     0 1000    0 27342 27342 27342 27342 m3ftpd         
 0  50    50   721/721    2    8 2020    10 27342 27342 27342 latency_client 
 0  51    51   722/722    2    8 2020    10 27342 27342 27342 latency_client 
 
ERROR: 700:dvk_put2lcl:492: rcode=-329
ERROR: 700:dvk_put2lcl:497: rcode=-329
ERROR: lz4tcp_proxy_bat.c:pr_process_message:379: rcode=-329
ERROR: 700:dvk_put2lcl:492: rcode=-329
ERROR: 700:dvk_put2lcl:497: rcode=-329
ERROR: lz4tcp_proxy_bat.c:pr_process_message:379: rcode=-329

 lz4tcp_proxy_bat.c:pr_start_serving:537:RPROXY: Message succesfully processed.
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=717 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x1 dcid=0 src=10 dst=51 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=717 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 700 c_timestamp=1628428918.630459109
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 700 c_flags=0x0 c_src_pid=717 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:335:RPROXY: source=10 type=11 m1i1=1 m1i2=2 m1i3=3 m1p1=0x3 m1p2=(nil) m1p3=(nil) 
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 700:dvk_put2lcl:478: 
DEBUG 700:dvk_put2lcl:490: ioctl ret=-1 errno=329
DEBUG 700:dvk_put2lcl:495: ioctl ret=-329
 
#define EDVSOVERRUN	 (_SIGN 330)  	/* An operation couse an overrun of some system resource */

HUSTON WE HAVE A PROBLEM!!!!
Esto sucede porque se utiliza el process descriptor del proceso remoto (ep=10) para 
encolar el mensaje en la cola del proceso receptor.
Como varios procesos (3 clientes en este caso) estan esperando respuesta del mismo endpoint
entonces uno de ellos se puede encolar y los otros fallan.

OPCION COMPLICADA: Cambia los proxies VERDADEROS para que todos los mensajes recibidos se encolen
como hace lo hace el LB. El problema sigue porque,  CUANDO ENTREGAMOS EL MENSAJE ?? 

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
VOLVER A PENSAR PORQUE AHORA SE TIENEN QUE EXPONER TODOS LOS ENDPOINTS DE LOS SERVERS. 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


PARTICULARIDAD: Puede usar el mismo extep pero desde distintos nodes 
Entonces si viene una sesion de un cliente cuya tupla <dcid, src_node, dst_ep> ya tiene una asignado una sesion
se rechaza.

LOAD BALANCER AL EJECUTAR EN EL CLIENTE M3FTP 
 947:lb_cltpxy.c:clt_Rproxy_rcvhdr:532:CLIENT_RPROXY (node2): n:132 | received:132 | HEADER_SIZE:132
 947:lb_cltpxy.c:clt_Rproxy_rcvhdr:535:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0
 947:lb_cltpxy.c:clt_Rproxy_rcvhdr:537:CLIENT_RPROXY (node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=775 c_ack_seq=-1
 947:lb_cltpxy.c:clt_Rproxy_getcmd:457:CLIENT_RPROXY(node2): header bytes=132
 947:lb_cltpxy.c:clt_Rproxy_getcmd:465:CLIENT_RPROXY(node2): cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0
 947:lb_cltpxy.c:clt_Rproxy_getcmd:467:CLIENT_RPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=775 c_ack_seq=-1
 947:lb_cltpxy.c:clt_Rproxy_getcmd:469:CLIENT_RPROXY(node2): c_timestamp=1628460255.158602377
 947:lb_cltpxy.c:clt_Rproxy_getcmd:471:CLIENT_RPROXY(node2): c_flags=0x0 c_src_pid=775 c_dst_pid=-1
 947:lb_cltpxy.c:clt_Rproxy_loop:133:CLIENT_RPROXY(node2):Message succesfully processed.
 947:lb_cltpxy.c:clt_Rproxy_error:197:CLIENT_RPROXY(node2): Replying -310 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 947:lb_cltpxy.c:clt_Rproxy_loop:128:CLIENT_RPROXY(node2): [192.168.0.102]. Getting remote command.
 947:lb_cltpxy.c:clt_Rproxy_getcmd:450:CLIENT_RPROXY(node2): 
 947:lb_cltpxy.c:clt_Rproxy_getcmd:455:CLIENT_RPROXY(node2): About to receive header
 947:lb_cltpxy.c:clt_Rproxy_rcvhdr:525:CLIENT_RPROXY(node2): lbp_csd=5
 
 946:lb_cltpxy.c:clt_Sproxy_serving:750:CLIENT_SPROXY(node2): cmd=0x2003 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=-310 len=0 <<<<<<<<<
 946:lb_cltpxy.c:clt_Sproxy_serving:752:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=789 c_ack_seq=-1
 946:lb_cltpxy.c:clt_Sproxy_send:776:CLIENT_SPROXY(node2): cmd=0x2003 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=-310 len=0
 946:lb_cltpxy.c:clt_Sproxy_send:778:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=789 c_ack_seq=-1
 946:lb_cltpxy.c:clt_Sproxy_sndhdr:813:CLIENT_SPROXY(node2): send header=132 
 946:lb_cltpxy.c:clt_Sproxy_sndhdr:817:CLIENT_SPROXY(node2): cmd=0x2003 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=-310 len=0
 946:lb_cltpxy.c:clt_Sproxy_sndhdr:819:CLIENT_SPROXY(node2): c_batch_nr=0 c_flags=0x0 c_snd_seq=789 c_ack_seq=-1
 946:lb_cltpxy.c:clt_Sproxy_sndhdr:837:CLIENT_SPROXY(node2): sent header=132 
 946:lb_cltpxy.c:clt_Sproxy_serving:744:CLIENT_SPROXY(node2): Reading message queue..
 946:lb_cltpxy.c:clt_Sproxy_mqrcv:895:CLIENT_SPROXY(node2): reading from mqid=32769
 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ATENCION: El cliente envia SENDREC y espera un SEND que lo rehabilite
 Pero si hay un error, el proxy esta devolviendo SENDRECE | ACKNOWLEDGE 
 y esto puede que no este conteplado en los proxies verdaderos !!!!
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 
 ESTO RECIBE EL CLIENTE 
 CLIENT->LB SENDREC 
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 709 c_flags=0x0 c_src_pid=784 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=784 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=784 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=784 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 lz4tcp_proxy_bat.c:update_stats:749:px_type=1
 lz4tcp_proxy_bat.c:update_stats:751:MTX_LOCK px_mutex 
 lz4tcp_proxy_bat.c:update_stats:788:snode=2 dnode=0 dcid=0 nr_msg=401 nr_data=0 nr_cmd=801
 lz4tcp_proxy_bat.c:update_stats:789:MTX_UNLOCK px_mutex 
 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 709: Waiting a message
 
 LB-->CLIENT CMD_COPYOUT_RQST
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=736 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=736 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 710 c_timestamp=1628466266.982872633
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 710 c_flags=0x0 c_src_pid=736 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 710:dvk_put2lcl:478: 
DEBUG 710:dvk_put2lcl:490: ioctl ret=0 errno=0
DEBUG 710:dvk_put2lcl:495: ioctl ret=0
 lz4tcp_proxy_bat.c:pr_start_serving:537:RPROXY: Message succesfully processed.
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
DEBUG 709:dvk_get2rmt_T:1089: timeout=30000
DEBUG 709:dvk_get2rmt_T:1102: ioctl ret=-1 errno=304 <<<<<<<<<<<<<<<<<<<<<< EDVSBADPROC 
DEBUG 709:dvk_get2rmt_T:1111: ioctl ret=-304
DEBUG 709:dvk_proxy_conn:586: pxid=0
DEBUG 709:dvk_proxy_conn:598: ioctl ret=0 errno=0
DEBUG 709:dvk_proxy_conn:603: ioctl ret=0
 
 
  1109:lb_svrpxy.c:svr_Rproxy_rcvhdr:416:SERVER_RPROXY (node1): n:132 | received:132 | HEADER_SIZE:132
 1109:lb_svrpxy.c:svr_Rproxy_rcvhdr:419:SERVER_RPROXY (node1): cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_rcvhdr:421:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=-1 c_ack_seq=-1
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:354:SERVER_RPROXY(node1): cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:356:SERVER_RPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=-1 c_ack_seq=-1
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:358:SERVER_RPROXY(node1): c_timestamp=1628470857.766461267
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:360:SERVER_RPROXY(node1): c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_loop:125:SERVER_RPROXY(node1):Message succesfully processed.
 1109:lb_svrpxy.c:svr_Rproxy_2server:172:SERVER_RPROXY(node1): 
 1109:lb_svrpxy.c:svr_Rproxy_2server:186:MTX_LOCK sess_table[dcid].st_mutex 
 1109:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=51 se_clt_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=11 se_lbsvr_ep=51 
 1109:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=52 se_clt_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=11 se_svr_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=12 se_lbsvr_ep=52 
 1109:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=12 se_svr_PID=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
 1109:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=60 se_clt_PID=1261
 1109:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=20 se_svr_PID=726
 1109:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=20 se_lbsvr_ep=60 
 1109:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x2001 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=-108 len=0
 1109:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=-1 c_dst_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:219:SERVER_RPROXY(node1): Expired Session Found se_svr_PID=726 c_src_pid=-1
 1109:lb_svrpxy.c:svr_Rproxy_2server:298:MTX_UNLOCK sess_table[dcid].st_mutex 
ERROR: lb_svrpxy.c:svr_Rproxy_2server:299: rcode=-59
 1109:lb_svrpxy.c:svr_Rproxy_error:317:SERVER_RPROXY(node1): Replying 111411202
ERROR: lb_svrpxy.c:svr_Rproxy_error:322: rcode=-11
ERROR: lb_svrpxy.c:svr_Rproxy_2server:302: rcode=-11
ERROR: lb_svrpxy.c:svr_Rproxy_loop:129: rcode=-11
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:343:SERVER_RPROXY(node1): 
 1109:lb_svrpxy.c:svr_Rproxy_getcmd:348:SERVER_RPROXY(node1): About to receive header
 

=====================================================================================
20210810:
		EJECUTANDO SIN EL LB ENTRE NODE1(SERVER) Y NODE2(CLIENT)
		Al ejecutar el FTP quedan a la espera
		
root@node2:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  10    10    -1/-1     1 1000    0 27342 27342 27342 27342 latency_server 
 0  11    11    -1/-1     1 1000    0 27342 27342 27342 27342 latency_server 
 0  12    12    -1/-1     1 1000    0 27342 27342 27342 27342 latency_server 
 0  21    21    -1/-1     1 1000 1000 27342 27342 27342 27342 m3ftpd         
 0  22    22    -1/-1     1 1000 1000 27342 27342 27342 27342 m3ftpd         
 0  23    23    -1/-1     1 1000 1000 27342 27342 27342 27342 m3ftpd         
 0  60    60   840/840    2    8 2020    21 27342 27342 27342 m3ftp          
 0  61    61   841/841    2    8 2020    22 27342 27342 27342 m3ftp          
 0  62    62   842/842    2    8 2020    23 27342 27342 27342 m3ftp 
 
 ALGUNOS ERRORES EN NODE1  
ERROR: m3ftpd.c:ftpd_reply:45: rcode=-55 <<<<<<<<<<<<< EDVSBADDEST
ERROR: m3ftpd.c:ftpd_reply:47: rcode=-22
M3FTPD: invalid request 3
ERROR: 816:dvk_send_T:881: rcode=-61
ERROR: 816:dvk_send_T:890: rcode=-61
ERROR: m3ftpd.c:ftpd_reply:44: rcode=-61
ERROR: 816:dvk_send_T:881: rcode=-55
ERROR: 816:dvk_send_T:890: rcode=-55
ERROR: m3ftpd.c:ftpd_reply:45: rcode=-55
ERROR: m3ftpd.c:ftpd_reply:47: rcode=-22

PROXY EN NODE1 RESPECTO A NODE2

SERVER->CLIENT COPYIN 
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 791 c_flags=0x0 c_src_pid=814 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=814 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=814 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=814 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 lz4tcp_proxy_bat.c:ps_send_remote:831:SPROXY: send payload len=4096
 lz4tcp_proxy_bat.c:ps_send_payload:719:SPROXY: ps_send_payload bytesleft=4096 
 lz4tcp_proxy_bat.c:ps_send_payload:724:SPROXY: sent=4096 
 lz4tcp_proxy_bat.c:ps_send_payload:738:SPROXY: socket=4 sent payload=4096 
 lz4tcp_proxy_bat.c:update_stats:749:px_type=1
 lz4tcp_proxy_bat.c:update_stats:751:MTX_LOCK px_mutex 
 lz4tcp_proxy_bat.c:update_stats:788:snode=1 dnode=2 dcid=0 nr_msg=6221 nr_data=24252416 nr_cmd=12171
 lz4tcp_proxy_bat.c:update_stats:789:MTX_UNLOCK px_mutex 
 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 791: Waiting a message
 CLIENT->SERVER SENDREC !!!!!!!! ?????
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 792 c_timestamp=1628643758.156783065
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 792 c_flags=0x0 c_src_pid=969 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:335:RPROXY: source=60 type=3 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbff29e2c m1p2=0x44f180 m1p3=(nil) 
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
 CLIENT->SERVER COPYIN ACK !!!!! 
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=-329 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=-329 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 792 c_timestamp=1628643758.156783065
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 792 c_flags=0x0 c_src_pid=969 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 792:dvk_put2lcl:480: 
DEBUG 792:dvk_put2lcl:492: ioctl ret=-1 errno=329
DEBUG 792:dvk_put2lcl:497: ioctl ret=-329 EDVSENQUEUED	 (_SIGN 329)  	/* The process descriptor is enqueued and cant be used *

EN NODE2 RESPECTO A NODE1 
CLIENT->SERVER SENDREC 
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 803 c_flags=0x0 c_src_pid=969 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 lz4tcp_proxy_bat.c:update_stats:749:px_type=1
 lz4tcp_proxy_bat.c:update_stats:751:MTX_LOCK px_mutex 
 lz4tcp_proxy_bat.c:update_stats:788:snode=2 dnode=1 dcid=0 nr_msg=6230 nr_data=117 nr_cmd=18393
 lz4tcp_proxy_bat.c:update_stats:789:MTX_UNLOCK px_mutex 
 lz4tcp_proxy_bat.c:ps_start_serving:868:SPROXY 803: Waiting a message
 SERVER->CLIENT COPYIN 
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=814 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=814 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 804 c_timestamp=1628643758.136656683
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 804 c_flags=0x0 c_src_pid=814 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:339:RPROXY: src=21 dst=60 rqtr=21 saddr=0x48f1a0 daddr=0x44f180 bytes=4096 
 lz4tcp_proxy_bat.c:pr_receive_payload:267:pl_size=4096
 lz4tcp_proxy_bat.c:pr_receive_payload:270:recv=4096
 lz4tcp_proxy_bat.c:pr_receive_payload:272:RPROXY: n:4096 | received:4096
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 804:dvk_put2lcl:478: 
DEBUG 804:dvk_put2lcl:490: ioctl ret=0 errno=0
DEBUG 804:dvk_put2lcl:495: ioctl ret=0
 lz4tcp_proxy_bat.c:pr_start_serving:537:RPROXY: Message succesfully processed.
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
DEBUG 803:dvk_get2rmt_T:1089: timeout=30000
DEBUG 803:dvk_get2rmt_T:1102: ioctl ret=0 errno=0
DEBUG 803:dvk_get2rmt_T:1111: ioctl ret=0
 CLIENT->SERVER SENDREC !!!!!!!! ?????
 lz4tcp_proxy_bat.c:ps_start_serving:899:SPROXY: 803 cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:900:SPROXY: 803 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
DEBUG 803:dvk_getprocinfo:1177: dcid=0 p_nr=60 
DEBUG 803:dvk_getprocinfo:1190: ioctl ret=0 errno=0
DEBUG 803:dvk_getprocinfo:1195: ioctl ret=0
DEBUG 803:dvk_getprocinfo:1177: dcid=0 p_nr=21 
DEBUG 803:dvk_getprocinfo:1190: ioctl ret=92 errno=0
DEBUG 803:dvk_getprocinfo:1195: ioctl ret=92
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 803 c_flags=0x0 c_src_pid=969 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x3 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=0 len=0
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 
CLIENT->SERVER COPYIN ACK 
 lz4tcp_proxy_bat.c:ps_start_serving:899:SPROXY: 803 cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=-329 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:900:SPROXY: 803 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
DEBUG 803:dvk_getprocinfo:1177: dcid=0 p_nr=60 
DEBUG 803:dvk_getprocinfo:1190: ioctl ret=0 errno=0
DEBUG 803:dvk_getprocinfo:1195: ioctl ret=0
DEBUG 803:dvk_getprocinfo:1177: dcid=0 p_nr=21 
DEBUG 803:dvk_getprocinfo:1190: ioctl ret=92 errno=0
DEBUG 803:dvk_getprocinfo:1195: ioctl ret=92
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 803 c_flags=0x0 c_src_pid=969 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=-329 len=0
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=969 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=-329 len=0

COMO PUEDE SER ESTO ENTONCES ???? 
HAY QUE ARREGLAR EL FTP !!

LA SECUENCIA QUE SE ME OCURRE ES:
		SERVER				CLIENT 
		=======				=======
							SENDREC 
		COPYIN				
							COPYIN ACK 
							SENDREC TIMEOUT 
							SENDREC 						
QUE PASA SI QUITO LOS TIMOUTS ????

 
=====================================================================================
20210814:

FTPD RECIBE UNA PETICION DE GNEXT 
 749:m3ftpd.c:main:228:M3FTPD: source=60 type=3 m1i1=12 m1i2=4096 m1i3=69632 m1p1=0xbfee1e2c m1p2=0x4cb180 m1p3=(nil) 
 749:m3ftpd.c:main:236:M3FTPD: FTPDLEN=4096 rlen=4096
 FALLA CUANDO HACE EL VCOPY 
DEBUG 749:dvk_vcopy:128: src_ep=35534 dst_ep=60 bytes=4096
DEBUG 749:dvk_vcopy:144: ioctl ret=-1 errno=61
DEBUG 749:dvk_vcopy:156: ioctl ret=-61
ERROR: m3ftpd.c:main:241: rcode=-61

EN EL PROXY DEL NODE1 SUCEDE LO SIGUIENTE
FTPD(21)->FTP(60)  VCOPY 
 lz4tcp_proxy_bat.c:ps_start_serving:922:SPROXY: 719 c_flags=0x0 c_src_pid=749 c_dst_pid=-1
 lz4tcp_proxy_bat.c:ps_start_serving:988:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_start_serving:989:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=749 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_remote:801:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_send_remote:802:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=749 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:681:SPROXY: send header=132 
 lz4tcp_proxy_bat.c:ps_send_header:684:SPROXY:cmd=0x5 dcid=0 src=21 dst=60 snode=1 dnode=2 rcode=0 len=4096
 lz4tcp_proxy_bat.c:ps_send_header:685:SPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=749 c_ack_seq=-1
 lz4tcp_proxy_bat.c:ps_send_header:703:SPROXY: socket=4 sent header=132 
 lz4tcp_proxy_bat.c:ps_send_remote:831:SPROXY: send payload len=4096
 lz4tcp_proxy_bat.c:ps_send_payload:719:SPROXY: ps_send_payload bytesleft=4096 
 lz4tcp_proxy_bat.c:ps_send_payload:724:SPROXY: sent=4096 
 lz4tcp_proxy_bat.c:ps_send_payload:738:SPROXY: socket=4 sent payload=4096 
 lz4tcp_proxy_bat.c:update_stats:749:px_type=1
 lz4tcp_proxy_bat.c:update_stats:751:MTX_LOCK px_mutex 
 lz4tcp_proxy_bat.c:update_stats:788:snode=1 dnode=2 dcid=0 nr_msg=2776 nr_data=10735616 nr_cmd=5553
 lz4tcp_proxy_bat.c:update_stats:789:MTX_UNLOCK px_mutex 

FTP(60)->FTPD(21)  VCOPY ACKNOWLEDGE
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
 lz4tcp_proxy_bat.c:pr_receive_header:292:RPROXY: n:132 | received:132 | HEADER_SIZE:132
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=4096 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=783 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x2005 dcid=0 src=60 dst=21 snode=2 dnode=1 rcode=4096 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=783 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 720 c_timestamp=1628949907.319068811
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 720 c_flags=0x0 c_src_pid=783 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 720:dvk_put2lcl:480: 
DEBUG 720:dvk_put2lcl:492: ioctl ret=-1 errno=324     <<< EDVSPROCSTS
DEBUG 720:dvk_put2lcl:497: ioctl ret=-324
 lz4tcp_proxy_bat.c:pr_start_serving:540:RPROXY: Message processing failure [-324]
 lz4tcp_proxy_bat.c:pr_process_message:316:RPROXY: About to receive header
 lz4tcp_proxy_bat.c:pr_receive_header:287:socket=6
 
 ESTO SE DA EN 
long generic_ack_rmt2lcl(int ack, struct proc *rmt_ptr, struct proc *lcl_ptr, int rcode)

if(!test_bit(BIT_ONCOPY, &lcl_ptr->p_usr.p_rts_flags))  {ret= EDVSPROCSTS; break;}
if(!test_bit(BIT_ONCOPY, &rmt_ptr->p_usr.p_rts_flags))  {ret= EDVSPROCSTS; break;}


FTPD HACE EL COPYIN Y QUEDA A LA ESPERA DEL ACKNOWLEDGE
Aug 14 11:35:22 node1 kernel: [  114.364291] DEBUG 728:new_vcopy:1483: CMD_COPYIN_DATA copylen=4096
Aug 14 11:35:22 node1 kernel: [  114.364292] DEBUG 728:new_vcopy:1505: CMD_COPYIN_DATA cmd=0x5 dcid=0 src=22 dst=61 snode=1 dnode=2 rcode=0 len=4096
Aug 14 11:35:22 node1 kernel: [  114.364293] DEBUG 728:new_vcopy:1506: CMD_COPYIN_DATA src=22 dst=61 rqtr=22 saddr=004f51a0 daddr=00445180 bytes=4096 
Aug 14 11:35:22 node1 kernel: [  114.364294] DEBUG 728:sproxy_enqueue:28: nr=22 endp=22 dcid=0 flags=400 misc=1020 lpid=728 vpid=728 nodeid=1 name=m3ftpd 
Aug 14 11:35:22 node1 kernel: [  114.364295] DEBUG 728:sproxy_enqueue:30: cmd=0x5 dcid=0 src=22 dst=61 snode=1 dnode=2 rcode=0 len=4096

FTPD SALE POR TIME OUT!!!
Aug 14 11:35:53 node1 kernel: [  144.556214] DEBUG 728:sleep_proc2:880: pending: sig[0]:0x00000000, sig[1]:0x00000000
Aug 14 11:35:53 node1 kernel: [  144.556215] DEBUG 728:sleep_proc2:883: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
Aug 14 11:35:53 node1 kernel: [  144.556215] DEBUG 728:sleep_proc2:890: endpoint=22 ret=0 p_rcode=0
Aug 14 11:35:53 node1 kernel: [  144.556216] DEBUG 728:sleep_proc2:891: endpoint=22 flags=2400 cpuid=0
Aug 14 11:35:53 node1 kernel: [  144.556217] DEBUG 728:sleep_proc2:893: WLOCK_PROC ep=22 count=0
Aug 14 11:35:53 node1 kernel: [  144.556218] DEBUG 728:sleep_proc2:893: WLOCK_PROC ep=61 count=0
Aug 14 11:35:53 node1 kernel: [  144.556218] DEBUG 728:sleep_proc2:922: pid=728 ret=-61
Aug 14 11:35:53 node1 kernel: [  144.556220] DEBUG 728:sleep_proc2:947: nr=22 endp=22 dcid=0 lpid=728 p_cpumask=FFFFFFFF nodemap=2 name=m3ftpd 
Aug 14 11:35:53 node1 kernel: [  144.556220] DEBUG 728:sleep_proc2:949: someone wakeups me: sem=0 p_rcode=-61
Aug 14 11:35:53 node1 kernel: [  144.556222] DEBUG 728:new_vcopy:1702: WUNLOCK_PROC ep=61 count=0
Aug 14 11:35:53 node1 kernel: [  144.556223] DEBUG 728:new_vcopy:1709: WUNLOCK_PROC ep=22 count=0
Aug 14 11:35:53 node1 kernel: [  144.556224] ERROR: 728:new_vcopy:1711: rcode=-61
Aug 14 11:35:53 node1 kernel: [  144.556224] ERROR: 728:dvk_ioctl:373: rcode=-61

SALIO EL VCOPY COPYIN DESDE FTP A FTPD  SALE 30 SEGUNTOS DESPUES 	
Aug 14 11:35:53 node1 kernel: [  144.556771] DEBUG 696:new_get2rmt:549: cmd=0x5 dcid=0 src=22 dst=61 snode=1 dnode=2 rcode=0 len=4096
Aug 14 11:35:53 node1 kernel: [  144.556772] DEBUG 696:new_get2rmt:550: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	
EN EL ACK DESDE FTP A FTPD SE VE CUANDO FALLA 30 SEGUNDOS DESPUES !!!! 
Aug 14 11:35:53 node1 kernel: [  144.558035] DEBUG 697:new_put2lcl:952: CMD_COPYIN_ACK dcid=0 rmt_ep=61 rmt_nr=61 lcl_ep=22 lcl_nr=22
Aug 14 11:35:53 node1 kernel: [  144.558037] DEBUG 697:generic_ack_rmt2lcl:186: dcid=0 src_ep=61 dst_ep=22 ack=8197 rcode=4096
Aug 14 11:35:53 node1 kernel: [  144.558038] DEBUG 697:generic_ack_rmt2lcl:205: REMOTE PROC nr=61 endp=61 dcid=0 flags=1008 misc=2000 lpid=-1 vpid=-1 nodeid=2 name=m3ftp1 
Aug 14 11:35:53 node1 kernel: [  144.558040] DEBUG 697:generic_ack_rmt2lcl:207: LOCAL  PROC nr=22 endp=22 dcid=0 flags=8 misc=1020 lpid=728 vpid=728 nodeid=1 name=m3ftpd 
Aug 14 11:35:53 node1 kernel: [  144.558040] ERROR: 697:generic_ack_rmt2lcl:208: rcode=-324

-----------------------------------------------------------
EN EL NODE2 

SE RECIBE EL COPYIN 
Aug 14 11:35:53 node2 kernel: [  153.012627] DEBUG 709:new_put2lcl:594: WLOCK_PROC ep=27342 count=0
Aug 14 11:35:53 node2 kernel: [  153.012628] DEBUG 709:new_put2lcl:620: WUNLOCK_PROC ep=27342 count=0
Aug 14 11:35:53 node2 kernel: [  153.012641] DEBUG 709:new_put2lcl:621: cmd=0x5 dcid=0 src=22 dst=61 snode=1 dnode=2 rcode=0 len=4096
Aug 14 11:35:53 node2 kernel: [  153.012643] DEBUG 709:new_put2lcl:627: dcid=0

Y ENSEGUIDA ES ENVIADO POR EL SPROXY EL COPYIN ACKNOWLEDGE
Aug 14 11:35:53 node2 kernel: [  153.012727] DEBUG 708:new_get2rmt:167: nr=22 endp=22 dcid=0 flags=3000 misc=1000 lpid=-1 vpid=-1 nodeid=1 name=m3ftpd 
Aug 14 11:35:53 node2 kernel: [  153.012728] DEBUG 708:new_get2rmt:173: cmd=0x2005 dcid=0 src=61 dst=22 snode=2 dnode=1 rcode=4096 len=0
Aug 14 11:35:53 node2 kernel: [  153.012730] DEBUG 708:new_get2rmt:176: source=0 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=  (null) m1p2=  (null) m1p3=  (null) 
Aug 14 11:35:53 node2 kernel: [  153.012731] DEBUG 708:new_get2rmt:180: n_nodeid=1 n_proxies=1 n_flags=E n_dcs=1 n_name=node1

EL BIT #define BIT_ONCOPY		10
ES DECIR sus flag deberian tener OR con 0x0400

ENTONCES EL PROBLEMA PARECE ESTAR EN EL NODE1 PORQUE ORDEN EL vcopy 
Y EL ENVIO SE REALIZO 30 SEGUNDOS DESPUES
UNA RAZON PUEDE SER LA SIGUIENTE 
   - EL PROCESO QUE ENVIA HACE UN LOCK DE ALGUN MUTEX
   - EL PROXY NECESITA ESE MUTEX POR LO QUE QUEDA BLOQUEADO 
   - CUANDO AL PROCESO EMISOR SE LE VENCE EL TIMEOUT, LIBERA EL MUTEX 
   - ENTONCES EL PROXY PUEDE ENVIAR EL COPYIN 
   - LUEGO EL REMOTO RESPONDE CON COPYIN ACK PERO EL PROCESO SE ENCUENTRA EN OTRO ESTADO.

1- BUSCAR EL LOCK DEL SPROXY PARA SABER PORQUE SE BLOQUEA 

UNA RAZON PUEDE SER LA SIGUIENTE 
   - EL SENDER PROXY SE ESTA POR IR A DORMIR 
   - EL PROCESO QUE ENVIA HACE UN LOCK MUTEX PROXY
   - COMO VE QUE EL PROXY AUN NO SE DURMIO NO HACE EL WAKEUP
   - LIBERA EL MUTEX DEL PROXY 
   - EL PROXY SE VA A DORMIR POR 30S 
   - ENTONCES EL PROXY PUEDE ENVIAR EL COPYIN 
   - LUEGO EL REMOTO RESPONDE CON COPYIN ACK PERO EL PROCESO SE ENCUENTRA EN OTRO ESTADO.
   
HABIA PROBLEMAS EN EL SPROXY !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
SE SOLUCIONO!!

-------------------------------------------------------------
PRUEBA CON LB 

EL NODE2 SE CUELGA CUANDO SE HACE UN M3FTP

Ahora bien, el NODE1 FTPD  recibe el pedido, pero cuando intenta hacer vcopy da TIMEOUT

NODE1 SERVER 
736:m3ftpd.c:main:179:M3FTPD: source=60 type=1 m1i1=11 m1i2=4096 m1i3=0 m1p1=0xbfd3fe2e m1p2=0x466180 m1p3=(nil) 
DEBUG 736:dvk_vcopy:128: src_ep=60 dst_ep=35534 bytes=11
DEBUG 736:dvk_vcopy:144: ioctl ret=-1 errno=61
DEBUG 736:dvk_vcopy:156: ioctl ret=-61

ERROR: lb_svrpxy.c:svr_Rproxy_2server:299: rcode=-59 <<<<<<<<<<<<< EDVSCONNREFUSED /* connection refused */
ERROR: lb_svrpxy.c:svr_Rproxy_error:322: rcode=-11
ERROR: lb_svrpxy.c:svr_Rproxy_2server:302: rcode=-11
ERROR: lb_svrpxy.c:svr_Rproxy_loop:129: rcode=-11


------------------------------------------------------------
LOG EN EL LB 
EL CLIENTE ENVIA SENDREC 
 1119:lb_cltpxy.c:clt_Rproxy_rcvhdr:540:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0

EL LB LO REENVIA AL SERVER 
 1116:lb_svrpxy.c:svr_Sproxy_serving:646:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=60 dst=20 snode=0 dnode=1 rcode=0 len=0

EL SERVER RESPONDE CON UN COMANDO cmd=0x6 CMD_COPYOUT_RQST PARA QUE LE ENVIE EL FILENAME
 1117:lb_svrpxy.c:svr_Rproxy_rcvhdr:419:SERVER_RPROXY (node1): cmd=0x6 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=0 len=0

EL LB LO RETRANSMITE AL CLIENTE  
 1118:lb_cltpxy.c:clt_Sproxy_serving:755:CLIENT_SPROXY(node2): cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
Y AHI SE PUDRE TODO 

------------------------------------------------------------
EL CLIENTE ENVIA SENDREC 
 874:lb_cltpxy.c:clt_Rproxy_rcvhdr:540:CLIENT_RPROXY (node2): cmd=0x3 dcid=0 src=60 dst=20 snode=2 dnode=0 rcode=0 len=0

EL LB LO REENVIA AL SERVER 
 871:lb_svrpxy.c:svr_Sproxy_serving:646:SERVER_SPROXY(node1): cmd=0x3 dcid=0 src=60 dst=20 snode=0 dnode=1 rcode=0 len=0

EL SERVER RESPONDE CON UN COMANDO cmd=0x6 CMD_COPYOUT_RQST PARA QUE LE ENVIE EL FILENAME
 872:lb_svrpxy.c:svr_Rproxy_rcvhdr:419:SERVER_RPROXY (node1): cmd=0x6 dcid=0 src=20 dst=60 snode=1 dnode=0 rcode=0 len=0

EL LB LO RETRANSMITE AL CLIENTE  
 873:lb_cltpxy.c:clt_Sproxy_serving:755:CLIENT_SPROXY(node2): cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0

EVIDENTEMENTE HAY UN PROBLEMA EN ALGUNO DE LOS PROXIES.
SE PODRIA IMPLEMENTAR EN EL LB  QUE SI EL CMD TRANSPORTA DATOS, MOSTRAR LA ESTRUCTURA 

		CMD_COPYIN_DATA, 5	/* Request and DATA to copy data to remote process 		*/
		CMD_COPYOUT_RQST, 6	/* The remote process send to local process the data requested 	*/
		CMD_COPYLCL_RQST, 7
		
		CMD_COPYRMT_RQST, 8	/* REQUESTER to SENDER to copy data out to RECEIVER */
		CMD_COPYIN_RQST, 9	/* SENDER to RECEIVER *
				
=====================================================================================
=====================================================================================
ERROR:	EL problemas puede estar en el rproxy cuando recibe el COPYOUT_RQST
        O puede estar en el sproxy, cuando tienen que devolver el ACK con los datos 
=====================================================================================
=====================================================================================
	

despues de ejecutar 3 M3FTPD en NODE1 y 3 M3FTP en NODE2
root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  21    21   805/805    1    4   20 27342    60 27342 27342 m3ftpd         
 0  22    22   806/806    1  400 1020 27342 27342 27342 27342 m3ftpd         
 0  23    23   808/808    1    4   20 27342    62 27342 27342 m3ftpd          
 0  60    60    -1/-1     2 100C 2000    21    21 27342 27342 m3ftp0         
 0  61    61    -1/-1     2 140C 2000    22    22 27342 27342 m3ftp1         
 0  62    62    -1/-1     2 100C 2000    23    23 27342 27342 m3ftp2   

root@node2:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  21    21    -1/-1     1 3000 1000 27342 27342 27342     1 m3ftpd         
 0  22    22    -1/-1     1 3000 1000 27342 27342 27342     1 m3ftpd         
 0  23    23    -1/-1     1 3000 1000 27342 27342 27342     1 m3ftpd         
 0  60    60   864/864    2    8 2020    21 27342 27342 27342 m3ftp          
 0  61    61   865/865    2 200C 2020    22    22 27342     1 m3ftp          
 0  62    62   866/866    2    8 2020    23 27342 27342 27342 m3ftp 
 
 
EN NODE2 FTP GET 

ENVIA FTP_GNEXT SOLICITANTO 4096 BYTES Y RETORNA OK 4096 BYTES
 895:m3ftp.c:ftp_request:30:M3FTP: request source=21 type=3 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 
DEBUG 895:dvk_sendrec_T:931: endpoint=21 timeout=30000
DEBUG 895:dvk_sendrec_T:944: ioctl ret=0 errno=0
DEBUG 895:dvk_sendrec_T:953: ioctl ret=0
 895:m3ftp.c:ftp_request:38:M3FTP: reply source=21 type=0 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 

ENVIA FTP_GNEXT SOLICITANTO 4096 BYTES Y RETORNA TIMEOUT PERO LUEGO RETORNA OK 4096 BYTES
 895:m3ftp.c:ftp_request:27:ftp_request=3
 895:m3ftp.c:ftp_request:30:M3FTP: request source=21 type=3 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 
DEBUG 895:dvk_sendrec_T:931: endpoint=21 timeout=30000
DEBUG 895:dvk_sendrec_T:944: ioctl ret=-1 errno=61
DEBUG 895:dvk_sendrec_T:953: ioctl ret=-61
ERROR: m3ftp.c:ftp_request:34: rcode=-61
DEBUG 895:dvk_sendrec_T:931: endpoint=21 timeout=30000
DEBUG 895:dvk_sendrec_T:944: ioctl ret=0 errno=0
DEBUG 895:dvk_sendrec_T:953: ioctl ret=0
 895:m3ftp.c:ftp_request:38:M3FTP: reply source=21 type=0 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 

ENVIA FTP_GNEXT SOLICITANTO 4096 BYTES Y RETORNA TIMEOUT PERO LUEGO RETORNA OK 4096 BYTES
 895:m3ftp.c:ftp_request:27:ftp_request=3
 895:m3ftp.c:ftp_request:30:M3FTP: request source=21 type=3 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 
DEBUG 895:dvk_sendrec_T:931: endpoint=21 timeout=30000
DEBUG 895:dvk_sendrec_T:944: ioctl ret=-1 errno=61
DEBUG 895:dvk_sendrec_T:953: ioctl ret=-61
ERROR: m3ftp.c:ftp_request:34: rcode=-61
DEBUG 895:dvk_sendrec_T:931: endpoint=21 timeout=30000
DEBUG 895:dvk_sendrec_T:944: ioctl ret=0 errno=0
DEBUG 895:dvk_sendrec_T:953: ioctl ret=0
 895:m3ftp.c:ftp_request:38:M3FTP: reply source=21 type=-22 m1i1=12 m1i2=4096 m1i3=0 m1p1=0xbfd5ce27 m1p2=0x450180 m1p3=(nil) 
ERROR: m3ftp.c:main:226: rcode=-22 

=====================================================================================
EN EL PROXY RECEIVER DEL NODE2 LO ULTIMO ES 
RECIBE UN CMD COPYOUT_RQST PARA EL FILENAME
 lz4tcp_proxy_bat.c:pr_receive_header:294:RPROXY: cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_receive_header:295:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=752 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:321:RPROXY:cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
 lz4tcp_proxy_bat.c:pr_process_message:322:RPROXY:c_batch_nr=0 c_flags=0x0 c_snd_seq=752 c_ack_seq=-1
 lz4tcp_proxy_bat.c:pr_process_message:323:RPROXY: 701 c_timestamp=1628980398.144313871
 lz4tcp_proxy_bat.c:pr_process_message:325:RPROXY: 701 c_flags=0x0 c_src_pid=752 c_dst_pid=-1
 lz4tcp_proxy_bat.c:pr_process_message:367:RPROXY: put2lcl
DEBUG 701:dvk_put2lcl:478: 
DEBUG 701:dvk_put2lcl:490: ioctl ret=-1 errno=304 #define EDVSBADPROC  	(_SIGN 304)  /* Bad Process Number */
DEBUG 701:dvk_put2lcl:495: ioctl ret=-304
 lz4tcp_proxy_bat.c:pr_start_serving:540:RPROXY: Message processing failure [-304]

EL KERNEL DEL NODE2 LOGGEA 
Aug 14 19:34:20 node2 kernel: [  234.582243] DEBUG 701:dvk_ioctl:349: cmd=4004E314 arg=BFBD9F74
Aug 14 19:34:20 node2 kernel: [  234.582244] DEBUG 701:dvk_ioctl:369: DVK_CALL=20 (io_put2lcl) 
Aug 14 19:34:20 node2 kernel: [  234.582245] DEBUG 701:io_put2lcl:217: 
Aug 14 19:34:20 node2 kernel: [  234.582246] DEBUG 701:check_caller:616: caller_pid=701 caller_tgid=701
Aug 14 19:34:20 node2 kernel: [  234.582247] DEBUG 701:check_caller:657: WLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582248] DEBUG 701:check_caller:662: nr=0 endp=27342 dcid=-1 flags=0 misc=3 lpid=701 vpid=-1 nodeid=2 name=lz4tcp_proxy_ba 
Aug 14 19:34:20 node2 kernel: [  234.582249] DEBUG 701:check_caller:722: WUNLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582249] DEBUG 701:check_caller:739: caller_pid=701 
Aug 14 19:34:20 node2 kernel: [  234.582250] DEBUG 701:check_caller:742: RLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582250] DEBUG 701:check_caller:746: RUNLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582251] DEBUG 701:new_put2lcl:606: WLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582252] DEBUG 701:new_put2lcl:632: WUNLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582253] DEBUG 701:new_put2lcl:633: cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
Aug 14 19:34:20 node2 kernel: [  234.582254] DEBUG 701:new_put2lcl:639: dcid=0
Aug 14 19:34:20 node2 kernel: [  234.582255] DEBUG 701:new_put2lcl:655: WLOCK_DC dc=0 count=0
Aug 14 19:34:20 node2 kernel: [  234.582256] DEBUG 701:new_put2lcl:687: TIMESTAMP sec=1628980460 nsec=194340437
Aug 14 19:34:20 node2 kernel: [  234.582256] DEBUG 701:new_put2lcl:706: WLOCK_PROC ep=20 count=0
Aug 14 19:34:20 node2 kernel: [  234.582257] DEBUG 701:new_put2lcl:706: WLOCK_PROC ep=60 count=0
Aug 14 19:34:20 node2 kernel: [  234.582258] DEBUG 701:new_put2lcl:770: REMOTE source OK endpoint=20
Aug 14 19:34:20 node2 kernel: [  234.582258] DEBUG 701:new_put2lcl:778: LOCAL destination OK endpoint=60
Aug 14 19:34:20 node2 kernel: [  234.582259] DEBUG 701:new_put2lcl:852: WLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582259] DEBUG 701:new_put2lcl:854: WUNLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582260] DEBUG 701:new_put2lcl:855: RUNLOCK_DC dc=0 count=0
Aug 14 19:34:20 node2 kernel: [  234.582261] DEBUG 701:new_put2lcl:919: CMD_COPYOUT_RQST dcid=0 rmt_ep=20 rmt_nr=20 lcl_ep=60 lcl_nr=60
Aug 14 19:34:20 node2 kernel: [  234.582263] DEBUG 701:copyout_rqst_rmt2lcl:465: cmd=0x6 dcid=0 src=20 dst=60 snode=0 dnode=2 rcode=0 len=0
Aug 14 19:34:20 node2 kernel: [  234.582264] DEBUG 701:copyout_rqst_rmt2lcl:466: src=20 dst=20 rqtr=20 saddr=bfdb9e2e daddr=004541a0 bytes=11 <<<<<<<<< ERRORR !!!!
Aug 14 19:34:20 node2 kernel: [  234.582264] ERROR: 701:copyout_rqst_rmt2lcl:485: rcode=-304
Aug 14 19:34:20 node2 kernel: [  234.582265] DEBUG 701:new_put2lcl:996: WUNLOCK_PROC ep=60 count=0
Aug 14 19:34:20 node2 kernel: [  234.582266] DEBUG 701:new_put2lcl:996: WUNLOCK_PROC ep=20 count=0
Aug 14 19:34:20 node2 kernel: [  234.582266] DEBUG 701:new_put2lcl:1014: WLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582267] DEBUG 701:new_put2lcl:1016: WUNLOCK_PROC ep=27342 count=0
Aug 14 19:34:20 node2 kernel: [  234.582267] ERROR: 701:new_put2lcl:1019: rcode=-304
Aug 14 19:34:20 node2 kernel: [  234.582268] ERROR: 701:dvk_ioctl:373: rcode=-304

AQUI SALTA EL ERROR EN copyout_rqst_rmt2lcl
		if( lcl_ptr->p_usr.p_endpoint !=  h_ptr->c_u.cu_vcopy.v_src)	{ret = EDVSBADPROC; break;}
		
src=20 dst=20 rqtr=20 saddr=bfdb9e2e daddr=004541a0 bytes=11 <<<<<<<<< ERRORR TODOS IGUALES !!! 
QUIERE DECIR QUE EL LB ESTA CAMBIANDO INCORRECTAMENTE LOS VALORES

=====================================================================================
20210815:
	Cuando se hace el triple FTP los endpoints deberian ser:
	20-60
	21-61
	22-62 
	Pero las sesiones quedaron
	20-60
	21-62 
	y el server manda src=22 dst=62 
	
 1290:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=50 se_clt_PID=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
 1290:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x6 dcid=0 src=22 dst=62 snode=1 dnode=0 rcode=0 len=0
 1290:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=742 c_dst_pid=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=60 se_clt_PID=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=20 se_svr_PID=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=20 se_lbsvr_ep=60 
 1290:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x6 dcid=0 src=22 dst=62 snode=1 dnode=0 rcode=0 len=0
 1290:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=742 c_dst_pid=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:194:XXX_SERVER_RPROXY(node1):se_dcid=0 se_clt_nodeid=2 se_clt_ep=62 se_clt_PID=753
 1290:lb_svrpxy.c:svr_Rproxy_2server:196:XXX_SERVER_RPROXY(node1):se_dcid=0 se_svr_nodeid=1 se_svr_ep=21 se_svr_PID=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:198:XXX_SERVER_RPROXY(node1):se_dcid=0 se_lbclt_ep=22 se_lbsvr_ep=62 
 1290:lb_svrpxy.c:svr_Rproxy_2server:201:XXX_SERVER_RPROXY(node1):cmd=0x6 dcid=0 src=22 dst=62 snode=1 dnode=0 rcode=0 len=0
 1290:lb_svrpxy.c:svr_Rproxy_2server:203:XXX_SERVER_RPROXY(node1):c_flags=0x0 c_src_pid=742 c_dst_pid=-1
 1290:lb_svrpxy.c:svr_Rproxy_2server:298:MTX_UNLOCK sess_table[dcid].st_mutex 
ERROR: lb_svrpxy.c:svr_Rproxy_2server:299: rcode=-59

--------------------------------------------------------------------------
PRUEBA SIN DEBUG!!! 
El ls_dvs en su debug solo muestra 

ERROR: lb_svrpxy.c:svr_Rproxy_2server:166: rcode=-302
ERROR: lb_svrpxy.c:svr_Rproxy_loop:129: rcode=-11

# ./latency_client 0 50 10 100  QUEDA SIN AVANZAR (NO SE CUELGA, PERO NO HACE NADA)

En NODE1 NI SE ENTERAN
root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats 
DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
 0  10    10   727/727    1        0        0        0        0 latency_server 
 0  11    11   728/728    1        0        0        0        0 latency_server 
 0  12    12   729/729    1        0        0        0        0 latency_server 
 0  20    20   730/730    1        0        0        0        0 m3ftpd         
 0  21    21   731/731    1        0        0        0        0 m3ftpd         
 0  22    22   732/732    1        0        0        0        0 m3ftpd         
 0  50    50    -1/-1     0        0        0        0        0 latclient0     
 0  51    51    -1/-1     0        0        0        0        0 latclient1     
 0  52    52    -1/-1     0        0        0        0        0 latclient2     
 0  60    60    -1/-1     0        0        0        0        0 m3ftp0         
 0  61    61    -1/-1     0        0        0        0        0 m3ftp1         
 0  62    62    -1/-1     0        0        0        0        0 m3ftp2  
 
en NODE2 SE CAE UN PROXY 
tcp        0      0 192.168.0.102:22        192.168.0.196:63577     ESTABLISHED
tcp        0      0 192.168.0.102:3001      192.168.0.101:58878     ESTABLISHED
tcp        0     52 192.168.0.102:22        192.168.0.196:60980     ESTABLISHED
tcp        0      0 192.168.0.102:3000      192.168.0.100:57318     CLOSE_WAIT 
tcp        0      0 192.168.0.102:38550     192.168.0.101:3002      ESTABLISHED

EL LB TAMBIEN MARCA ERRORES 
ERROR: lb_svrpxy.c:svr_Rproxy_2server:166: rcode=-302
ERROR: lb_svrpxy.c:svr_Rproxy_loop:129: rcode=-11
ERROR: lb_svrpxy.c:svr_Sproxy_sndpay:789: rcode=-14
Y TERMINA CAYENDO 


PROBLEMA CON EL TAMAÑO DEL MENSAJE DE IPC.H 
 649:lb_svrpxy.c:svr_Rproxy_rcvhdr:420:SERVER_RPROXY (node1): n:92 | received:92 | HEADER_SIZE:132
 649:lb_svrpxy.c:svr_Rproxy_rcvhdr:429:SERVER_RPROXY (node1): Header partially received. There are 40 bytes still to get
 651:lb_cltpxy.c:clt_Rproxy_rcvhdr:540:CLIENT_RPROXY (node2): n:92 | received:92 | HEADER_SIZE:132
 651:lb_cltpxy.c:clt_Rproxy_rcvhdr:549:CLIENT_RPROXY (node2): Header partially received. There are 40 bytes still to get
 
==========================================================================================
==========================================================================================
==========================================================================================
==========================================================================================
				TODO TODO TODO 
				
- Incluir dynamic binding, batched y compresion (no se necesitaria incluir clients en config)
- Incorporar TIPC en lugar de TCP 
- Probar como levanta y mata servers en autoscaling 
- Agregar como parametro al LB un min_servers que indica la cantidad minima de servers a conservar y max_servers
- EL LB se deberia lanzar a ejecutar con el SPREAD y el modulo DVK arrancados Para luego hacer estas verificaciones contra los parametros del DC y no contra las constantes.				
- Arrancar una VM por autoscaling 
- SPREAD unicast en lugar de multicast desde LBA->LBM 
- Agregar un contador de sesiones por SERVER o por SERVICE que puede usarse como parametro de carga
- Los test a realizar son?
			- latencia  con server persistente
			- throughput con server persistente 
			- startup    con server efimero 
- Las sesiones podrian tener un contador que se setea en MAXRETRIES, 
   cada vez que se utiliza una sesion, se setea en MAXRETRIES, pero una ALARM cada 30 segundos 
   toma cada sesion y le descuenta 1. Cuando llega a cero, la sesion se termina.
 - Incorporar un archivo de LOG.
  
 ALB uses the least outstanding request routing algorithm. 
 If the requests to the backend vary in complexity where one request 
 might need a lot more CPU time than another, then the least outstanding 
 request algorithm is more appropriate. It’s also the right routing algorithm 
 to use if the targets vary in processing capabilities. 
 An outstanding request is when a request is sent to the backend 
 server and a response hasn’t been received yet.

For example, if the EC2 instances in a target group aren’t the same size, 
one server’s CPU utilization will be higher than the other if the same 
number of requests are sent to each server using the round-robin routing algorithm. 
That same server will have more outstanding requests as well. 
Using the least outstanding request routing algorithm would ensure an equal usage across targets.

ALB uses sticky sessions. If requests must be sent to the same backend server because 
the application is stateful, use the sticky session feature. This feature uses an HTTP 
cookie to remember across connections which server to send the traffic to.

MANEJAR DESTINOS FIJOS en funcion de algun filtro.
route fileserver {
	src_ep	 	ANY
	dst_ep   	22
	dst_node	node1 
}
 
ATENCION MANEJAR LOS DIFERENTES ERRORES 
- CLIENT SE CAE DESPUES DE HABAR INICIADO 
- SERVER SE CAE 
- NODO CLIENTE SE CAE
- NODO SERVER SE CAE 
- SERVER MIGRA 
- SERVER SE REPLICA 

- AGREGAR UN INTERPRETE DE COMANDOS 
- COMANDO QUE SE DE DE BAJA TEMPORARIAMENTE UN SERVER NODE Y YA NO SE LE PASAN NUEVAS SESIONES
- IDEM PERO DE ALTA 



==========================================================================================
20220103:
		Se modificaron el latency_client y latency_server para que soporten como parametro
		el tiempo de uso de CPU 
		
		Usage: latency_client <dcid> <client_nr> <server_nr> <loops> [<cpu_secs>]	
		  if cpu_secs > 0, it is the fixed time CPU usage in server
		  if cpu_secs < 0, it is a random time from 0 < t < abs(cpu_secs) of CPU usage in server
		  
		De esta forma se puede medir la latencia (cpu_secs=0) o cargar durante determinados 
		segundos la CPU del server.

==========================================================================================
20220104:		
		Se creo una imagen de VM liviana para cliente.
		Para ello se utiliza otra red VMnet2 tipo HOST en el VMWARE.
		En tanto que se agregó a NODE0 (Load Balancer) una segunda interface eth2 conectada a VMnet2.
		Las direcciones IP de la red de servidores o nodos es la 192.168.0.XXX tipo Bridge
		Las direcciones IP de la red de clientes es 192.168.137.XXX en la VMnet2 tipo Host
		El windows tiene direcciones IP 192.168.0.196 (otorgado por DHCP) y 192.168.137.1 fijado en VMnet2
		El NODE0 tiene direcciones IP 192.168.0.100 y 192.168.137.100 
		Los nombres de los hosts en la red de servidores es "nodeXX", en tanto que en la red de Clients es "clientXX"
	

En el CLIENT1  se modifico el archivo que configura los proxies /usr/src/dvs/dvk-proxies
Ahora apunta al LB que se llama "client0". 
proxy client0 {
	proxyid		0;
	proto		tcp; 
	rport		3000;
	sport		3001; 
	compress	YES; 
	batch		YES;
	autobind	YES;
};

ATENCION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Aparentemente tanto NODE0, NODE1, NODE2 no estan actualizados en el /dvk-tests y /dvk-proxies 

ATENCION: Podria haber un conflicto en el LB cuando escucha en el puerto XXXX porque lo hace tanto para 
los clients como para los nodes.
por ejemplo 
root@client1:/etc# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN  <<<< escucha en todas las direcciones en el mismo puerto.

==========================================================================================
20220105: Se armo la siguiente topologia
			NODE0 <-> CLIENT11 
		Se probo de ejecutar latency_server y latency_client OK!!

LISTO:	Modificar el dvs_lb para adaptarlo a la nueva estructura del header que se preparo para el LB DISTRIBUIDO.

		EN NODE0 
		root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
		Active Internet connections (servers and established)
		Proto Recv-Q Send-Q Local Address           Foreign Address         State      
		tcp        0      0 0.0.0.0:3001            0.0.0.0:*               LISTEN     
		tcp        0      0 0.0.0.0:3011            0.0.0.0:*               LISTEN     
		tcp        0      0 192.168.137.100:3011    192.168.137.111:59300   ESTABLISHED

		EN NODE11
		root@client11:/usr/src/dvs/dvk-tests# netstat -nat
		Active Internet connections (servers and established)
		Proto Recv-Q Send-Q Local Address           Foreign Address         State      
		tcp        0      0 192.168.137.111:3000    0.0.0.0:*               LISTEN     
		tcp        0      0 192.168.137.111:59300   192.168.137.100:3011    ESTABLISHED



===================================================================================
20220106:
		EN NODE0 
		root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
		Active Internet connections (servers and established)
		Proto Recv-Q Send-Q Local Address           Foreign Address         State      
		tcp        0      0 192.168.0.100:3001      0.0.0.0:*               LISTEN     
		tcp        0      0 192.168.137.100:3011    0.0.0.0:*               LISTEN     
		tcp        0      0 192.168.137.100:3011    192.168.137.111:54176   ESTABLISHED <<< CLIENT RECEIVER PROXY OK! 
		tcp        0      1 192.168.137.100:56660   192.168.0.111:3000      SYN_SENT   <<<< no es 192.168.0.111:3000 !! CLIENT SENDER PROXY 
		tcp        0      1 192.168.0.100:37240     192.168.0.101:3000      SYN_SENT   

		HABIA ERROR EN EL ARCHIVO DE CONFIGURACION!! decia node11 en lugar de client11 
		
		root@client11:/usr/src/dvs/dvk-tests# netstat -nat
	Active Internet connections (servers and established)
	Proto Recv-Q Send-Q Local Address           Foreign Address         State      
	tcp        0      0 192.168.137.111:3000    0.0.0.0:*               LISTEN     
	tcp        0    388 192.168.137.111:22      192.168.137.1:50307     ESTABLISHED
	tcp        0      0 192.168.137.111:35906   192.168.137.100:3011    ESTABLISHED    >>> CLIENT->LB 
	tcp        0      0 192.168.137.111:3000    192.168.137.100:34744   ESTABLISHED    >>> LB->CLIENT  

	
	root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
	Active Internet connections (servers and established)
	Proto Recv-Q Send-Q Local Address           Foreign Address         State      
	tcp        0      0 192.168.0.100:3001      0.0.0.0:*               LISTEN     
	tcp        0      0 192.168.137.100:3011    0.0.0.0:*               LISTEN     
	tcp        0      0 192.168.137.100:3011    192.168.137.111:35906   ESTABLISHED   >>>> CLIENT-> LB 
	tcp        0      0 192.168.137.100:34744   192.168.137.111:3000    ESTABLISHED   >>>> LB->CLIENT 
	tcp        0      1 192.168.0.100:37262     192.168.0.101:3000      SYN_SENT   
		
 

LISTO:   INCOPORAR PARAMETROS DE PORTS AL LB 
		client client11 {
			nodeid 		11;
			lbRport		3011; // LB Receiver port  
			cltRport	3000; // Client Receiver port 
		};

		server node1 {
			nodeid 		1;
			lbRport		3001; // LB Receiver port  
			svrRport	3000; // Client Receiver port 
		};

#define	TKN_SVR_NODEID	0
#define	TKN_SVR_LBRPORT	1
#define	TKN_SVR_SVRRPORT 2
#define TKN_SVR_MAX 	3 // MUST be the last

#define TKN_CLT_NODEID	0
#define TKN_CLT_LBRPORT	1
#define TKN_CLT_CLTRPORT 2
#define TKN_CLT_MAX 	3	// MUST be the last

LISTO:  Se incorporó la seleccion de interfaces para redes clientes y servidores
	https://www.gta.ufrj.br/ensino/eel878/sockets/setsockoptman.html

LISTO:	INCORPORAR EL CONTROL DE ARGMENTOS CON BITMAP 


===================================================================================
20220110:		SE CONFIGURO NODE1(server)----NODE0(lb)----CLIENT11(client)

	Se tomaron medidas con simpleproxy instalado en NODE0 para llegar desde CLIENT11 a NODE1

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
LISTO	PONER UN PROXY TCP EN NODE0 PARA MEDIR PERFORMANCE
		https://manpages.debian.org/testing/simpleproxy/simpleproxy.1.en.html
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	
	Se hicieron pruebas CON DEBUG de latency_client y latency_server
	Se hicieron pruebas CON DEBUG de m3ftp y m3ftgd
	
	Se quitó el DEBUG de
		•	DEBUG DEL DVK
		•	DEBUG DE tests.sh !!! ATENCION, EL NIVEL DE DEBUG SE CAMBIA EN 2 LUGARES al arrancar del DVK y al iniciar el DVS 
		•	DEBUG DE LA LIB
		•	DEBUG DE LOS PROXIES
		•	DEBUG DEL LB
		•	DEBUG DE LATENCY_CLIENT Y LATENCY_SERVER
		•	DEBUG DE FTPD Y FTP

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
LISTO:		INCOPORAR BATCH Y COMPRESION A LOAD BALANCER 
ATENCION: 	NO SE INCORPORA BATCH NI COMPRESION AL LB YA 
			QUE LA IDEA ES QUE EL PAYLOAD PASE DE LARGO
			COMPRIMIDO O NO, BATCHEADO O NO
			LO QUE SI HACE EL LB ES COMPROBAR QUE LA 
			CONFIGURACION DEL CLIENT Y SERVER DE UNA SESION
			POSEAN IDENTICOS VALORES PARA COMPRESS Y BATCH 
	
SE CONTROLA 

	if( svr_ptr->svr_compress != clt_ptr->clt_compress 
	||  svr_ptr->svr_batch    != clt_ptr->clt_batch) {
		rcode = clt_Rproxy_error(clt_ptr, EDVSBADPROXY);
		if( rcode < 0) 	ERROR_PRINT(EDVSBADPROXY);
		MTX_UNLOCK(sess_table[dcid].st_mutex);		
		return(NULL);		
	}
	
 802:lb_cltpxy.c:select_server:472:CLIENT_RPROXY(client11): server name=node1 new_ep=10
 802:lb_cltpxy.c:clt_Rproxy_2server:364:CLIENT_RPROXY(client11): svr_name=node1 svr_nodeid=1 svr_compress=1 svr_batch=1
 802:lb_cltpxy.c:clt_Rproxy_2server:366:CLIENT_RPROXY(client11): clt_name=client11 clt_nodeid=11 clt_lbRport=3011 clt_cltRport=3011 clt_compress=0 clt_batch=0
 802:lb_cltpxy.c:clt_Rproxy_error:219:CLIENT_RPROXY(client11): Replying -327
 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


===================================================================================
20220111:

TODO:		PROBAR TRANSFERENCIAS CON COMPRESS Y BATCH HABILITADOS
			2 NODOS DIRECTOS
			2 NODOS CON LB 

===================================================================================
20220112:   MODIFICAR EL LB PARA QUE COPIE EL HEADER Y LUEGO MODIFIQUE

 SERVER 
  multi_proxy.c:pr_process_message:252:RPROXY(0): About to receive header
 multi_proxy.c:pr_receive_header:218:RPROXY(0): socket=5
 multi_proxy.c:pr_receive_header:224:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
 multi_proxy.c:pr_receive_header:227:RPROXY(0): cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=82 PID=713
 multi_proxy.c:pr_receive_header:229:RPROXY(0):c_batch_nr=1 c_flags=0x3 c_snd_seq=-1263525888 c_ack_seq=20
 multi_proxy.c:pr_process_message:258:RPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=82 PID=713
 multi_proxy.c:pr_process_message:260:RPROXY(0):c_batch_nr=1 c_flags=0x3 c_snd_seq=-1263525888 c_ack_seq=20
 multi_proxy.c:pr_process_message:262:RPROXY(0): 681 c_timestamp=1641949680.528003281
 multi_proxy.c:pr_process_message:264:RPROXY(0): 681 c_flags=0x3 c_src_pid=-1263525888 c_dst_pid=20
 multi_proxy.c:pr_receive_payload:198:RPROXY(0): pl_size=82
 multi_proxy.c:pr_receive_payload:201:RPROXY(0): recv=82
 multi_proxy.c:pr_receive_payload:203:RPROXY(0): n:82 | received:82
 multi_proxy.c:decompress_payload:77:RPROXY(0): INPUT maxRsize=65536 maxCsize=65559 
 multi_proxy.c:decompress_payload:82:RPROXY(0): INPUT comp_len=82  MAX raw_len=65536 
 multi_proxy.c:decompress_payload:94:RPROXY(0): OUTPUT raw_len=120 comp_len=82
 multi_proxy.c:pr_process_message:317:RPROXY(0): put2lcl
 multi_proxy.c:pr_process_message:432:RPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=713
 multi_proxy.c:pr_process_message:434:RPROXY(0):c_batch_nr=1 c_flags=0x3 c_snd_seq=-1263525888 c_ack_seq=20
 multi_proxy.c:pr_process_message:439:RPROXY(0): px_ptr->px_rdesc.td_batch_nr=1
 multi_proxy.c:pr_process_message:454:RPROXY(0): bat_cmd[0]:cmd=0x3 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=713
 multi_proxy.c:pr_process_message:456:RPROXY(0):cmd=0x3 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=713
 multi_proxy.c:pr_process_message:458:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 multi_proxy.c:pr_process_message:460:RPROXY(0): 681 c_timestamp=1641949680.528003281
 multi_proxy.c:pr_process_message:462:RPROXY(0): 681 c_flags=0x0 c_src_pid=0 c_dst_pid=0
 multi_proxy.c:pr_process_message:464:RPROXY(0): source=50 type=10 m1i1=20 m1i2=0 m1i3=3 m1p1=0x4 m1p2=(nil) m1p3=(nil) 
 multi_proxy.c:pr_start_serving:522:RPROXY(0): Message processing failure [-319] <<<< EDVSNODCNODE

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EN ESTA CASO PARTICULAR, ES UN SENDACK+SENDREC Y POR ESO ESTA BATCHEADO 
Y TAMBIEN ESTA COMPRIMIDO raw_len=120 comp_len=82
POR ESO c_flags=0x3
HAY QUE VER PORQUE c_snd_seq=-1263525888
EL ERROR 319 EDVSNODCNODE VIENE PORQUE snode=11 !!! en lugar de snode=0
EL PROBLEMA SE DA EN EL COMANDO BATCHEADO QUE SE PUSO TODO EL ENCABEZADO SIN
NINGUN TIPO DE MODIFICACION
Y COMO ESTE ESTA COMPRIMIDO, PRIMERO HAY QUE DESCOMPRIMIR Y 
LUEGO VOLVER A BATCHEAR !!!!!!!
PROBAR COMPRIMIENDO SIN BATCHEAR 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

===================================================================================
20220113: REALIZAR PRUEBAS CON TRANSFERENCIA CON  MAXCOPYLEN	(16 * MAXCOPYBUF)
			LOCAL
			ENTRE 2 NODOS DIRECTO 
			ENTRE 2 NODOS CON EL LB 
			
	MULTI_PROXY: Agregar un parametro para no comprimir los BATCH 
					Habilitar DEBUG

root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/info | grep copy
max_copy_buf=65536
max_copy_len=1048576

root@client11:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/info | grep copy
max_copy_buf=65536
max_copy_len=1048576

					
ERROR EN NODE1(SERVER)
root@node1:/usr/src/dvs/dvs-apps/m3ftp# *** Error in `.': double free or corruption (!prev): 0x01765008 ***
======= Backtrace: =========
/lib/i386-linux-gnu/libc.so.6(+0x6738a)[0xb759d38a]
/lib/i386-linux-gnu/libc.so.6(+0x6dfc7)[0xb75a3fc7]
/lib/i386-linux-gnu/libc.so.6(+0x6e806)[0xb75a4806]
/lib/i386-linux-gnu/libc.so.6(fclose+0x16f)[0xb759367f]
.(main+0x11b5)[0x4279a3]
/lib/i386-linux-gnu/libc.so.6(__libc_start_main+0xf6)[0xb754e286]
.(+0xe71)[0x425e71]
======= Memory map: ========
00425000-00428000 r-xp 00000000 08:01 155995     /usr/src/dvs/dvs-apps/m3ftp/m3ftpd
00428000-00429000 r--p 00002000 08:01 155995     /usr/src/dvs/dvs-apps/m3ftp/m3ftpd
00429000-0042a000 rw-p 00003000 08:01 155995     /usr/src/dvs/dvs-apps/m3ftp/m3ftpd
0042a000-0043c000 rw-p 00000000 00:00 0 
01765000-01786000 rw-p 00000000 00:00 0          [heap]
b7400000-b7421000 rw-p 00000000 00:00 0 
b7421000-b7500000 ---p 00000000 00:00 0 
b750e000-b752a000 r-xp 00000000 08:01 917508     /lib/i386-linux-gnu/libgcc_s.so.1
b752a000-b752b000 r--p 0001b000 08:01 917508     /lib/i386-linux-gnu/libgcc_s.so.1
b752b000-b752c000 rw-p 0001c000 08:01 917508     /lib/i386-linux-gnu/libgcc_s.so.1
b7534000-b7536000 rw-p 00000000 00:00 0 
b7536000-b76e7000 r-xp 00000000 08:01 921598     /lib/i386-linux-gnu/libc-2.24.so
b76e7000-b76e8000 ---p 001b1000 08:01 921598     /lib/i386-linux-gnu/libc-2.24.so
b76e8000-b76ea000 r--p 001b1000 08:01 921598     /lib/i386-linux-gnu/libc-2.24.so
b76ea000-b76eb000 rw-p 001b3000 08:01 921598     /lib/i386-linux-gnu/libc-2.24.so
b76eb000-b76ee000 rw-p 00000000 00:00 0 
b76ee000-b7707000 r-xp 00000000 08:01 921862     /lib/i386-linux-gnu/libpthread-2.24.so
b7707000-b7708000 r--p 00018000 08:01 921862     /lib/i386-linux-gnu/libpthread-2.24.so
b7708000-b7709000 rw-p 00019000 08:01 921862     /lib/i386-linux-gnu/libpthread-2.24.so
b7709000-b770b000 rw-p 00000000 00:00 0 
b770b000-b770f000 r-xp 00000000 08:01 1049323    /usr/src/dvs/dvk-lib/stub_dvkcall.o
b770f000-b7710000 r--p 00003000 08:01 1049323    /usr/src/dvs/dvk-lib/stub_dvkcall.o
b7710000-b7711000 rw-p 00004000 08:01 1049323    /usr/src/dvs/dvk-lib/stub_dvkcall.o
b7711000-b7715000 r-xp 00000000 08:01 1064643    /usr/src/dvs/vos/mol/lib/mollib/libmollib.so
b7715000-b7716000 ---p 00004000 08:01 1064643    /usr/src/dvs/vos/mol/lib/mollib/libmollib.so
b7716000-b7717000 r--p 00004000 08:01 1064643    /usr/src/dvs/vos/mol/lib/mollib/libmollib.so
b7717000-b7718000 rw-p 00005000 08:01 1064643    /usr/src/dvs/vos/mol/lib/mollib/libmollib.so
b7718000-b7719000 r-xp 00000000 08:01 1064656    /usr/src/dvs/vos/mol/lib/timers/libtimers.so
b7719000-b771a000 r--p 00000000 08:01 1064656    /usr/src/dvs/vos/mol/lib/timers/libtimers.so
b771a000-b771b000 rw-p 00001000 08:01 1064656    /usr/src/dvs/vos/mol/lib/timers/libtimers.so
b771b000-b771d000 r-xp 00000000 08:01 1064364    /usr/src/dvs/vos/mol/lib/syslib/libsyslib.so
b771d000-b771e000 r--p 00001000 08:01 1064364    /usr/src/dvs/vos/mol/lib/syslib/libsyslib.so
b771e000-b771f000 rw-p 00002000 08:01 1064364    /usr/src/dvs/vos/mol/lib/syslib/libsyslib.so
b771f000-b7735000 r-xp 00000000 08:01 921854     /lib/i386-linux-gnu/libnsl-2.24.so
b7735000-b7736000 r--p 00016000 08:01 921854     /lib/i386-linux-gnu/libnsl-2.24.so
b7736000-b7737000 rw-p 00017000 08:01 921854     /lib/i386-linux-gnu/libnsl-2.24.so
b7737000-b7739000 rw-p 00000000 00:00 0 
b7739000-b7740000 r-xp 00000000 08:01 921864     /lib/i386-linux-gnu/librt-2.24.so
b7740000-b7741000 r--p 00006000 08:01 921864     /lib/i386-linux-gnu/librt-2.24.so
b7741000-b7742000 rw-p 00007000 08:01 921864     /lib/i386-linux-gnu/librt-2.24.so
b7742000-b7795000 r-xp 00000000 08:01 921852     /lib/i386-linux-gnu/libm-2.24.so
b7795000-b7796000 r--p 00052000 08:01 921852     /lib/i386-linux-gnu/libm-2.24.so

EL PROBLEMA ERA QUE EL BUFFER ESTABA EN MAXCOPYBUF EN LUGAR DE MAXCOPYLEN

===================================================================================
20220113:	Hubo fallos en pruebas de CLIENT-LB-SERVER  SIN COMPRESION CON BATCH!!!
				FALLO EL LATENCY TEST 
				FALLO LOS M3FTP PUT (LOS GET FUNCIONARON)
				

			
Cuando se hace en CLIENT11 
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 1000 > lat0.out 
en NODE1
	root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats 
	DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
	 0  10    10   710/710    1        0       13        0        0 latency_server 

EN CLIENT11
	 multi_proxy.c:ps_start_serving:931:SPROXY(0): 704 cmd=0x2001 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
	 multi_proxy.c:ps_start_serving:933:SPROXY(0): 704 c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:ps_start_serving:935:SPROXY(0): 704 c_timestamp=1642070090.432006106
	 multi_proxy.c:ps_start_serving:937:SPROXY(0): 704 c_flags=0x0 c_src_pid=0 c_dst_pid=0
	 multi_proxy.c:ps_start_serving:956:SPROXY(0): 704 Getting more messages px_ptr->px_sdesc.td_batch_nr=0 
	 multi_proxy.c:ps_start_serving:985:SPROXY(0): bat_vect[0]:cmd=0x3 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
	 multi_proxy.c:ps_start_serving:987:SPROXY(0): new BATCHED COMMAND px_ptr->px_sdesc.td_batch_nr=0
	 multi_proxy.c:ps_start_serving:956:SPROXY(0): 704 Getting more messages px_ptr->px_sdesc.td_batch_nr=1 
	 multi_proxy.c:ps_start_serving:991:SPROXY(0): px_ptr->px_sdesc.td_batch_nr=1 ret=-61
	 multi_proxy.c:ps_start_serving:1000:SPROXY(0): sending BATCHED COMMANDS px_ptr->px_sdesc.td_batch_nr=1
	 multi_proxy.c:ps_send_remote:830:SPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=120 PID=734
	 multi_proxy.c:ps_send_remote:831:SPROXY(0):c_batch_nr=1 c_flags=0x1 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:ps_send_header:706:SPROXY(0): send header=120 
	 multi_proxy.c:ps_send_header:709:SPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=120 PID=734
	 multi_proxy.c:ps_send_header:710:SPROXY(0):c_batch_nr=1 c_flags=0x1 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:ps_send_header:731:SPROXY(0): socket=6 sent header=120 
	 multi_proxy.c:ps_send_remote:861:SPROXY(0): send payload len=120
	 multi_proxy.c:ps_send_payload:747:SPROXY(0): ps_send_payload bytesleft=120 
	 multi_proxy.c:ps_send_payload:752:SPROXY(0): sent=120 
	 multi_proxy.c:ps_send_payload:767:SPROXY(0): socket=6 sent payload=120 
	 multi_proxy.c:ps_start_serving:898:SPROXY(0): Waiting a message
	 multi_proxy.c:ps_start_serving:908:SPROXY(0): Sending HELLO 
	 multi_proxy.c:ps_send_remote:830:SPROXY(0):cmd=0x0 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
	 multi_proxy.c:ps_send_remote:831:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:ps_send_header:706:SPROXY(0): send header=120 
	 multi_proxy.c:ps_send_header:709:SPROXY(0):cmd=0x0 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
	 multi_proxy.c:ps_send_header:710:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:ps_send_header:731:SPROXY(0): socket=6 sent header=120 
	 multi_proxy.c:ps_start_serving:898:SPROXY(0): Waiting a message
 
 EN NODE1
		ERROR: multi_proxy.c:ps_start_serving:962: rcode=-61
		ERROR: 679:dvk_put2lcl:494: rcode=-319
		ERROR: 679:dvk_put2lcl:499: rcode=-319

		 multi_proxy.c:pr_process_message:252:RPROXY(0): About to receive header
		 multi_proxy.c:pr_receive_header:218:RPROXY(0): socket=5
		 multi_proxy.c:pr_receive_header:224:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
		 multi_proxy.c:pr_receive_header:227:RPROXY(0): cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 multi_proxy.c:pr_receive_header:229:RPROXY(0):c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 multi_proxy.c:pr_process_message:258:RPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 multi_proxy.c:pr_process_message:260:RPROXY(0):c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 multi_proxy.c:pr_process_message:262:RPROXY(0): 681 c_timestamp=1642070090.432006106
		 multi_proxy.c:pr_process_message:264:RPROXY(0): 681 c_flags=0x1 c_src_pid=-1263525888 c_dst_pid=13
		 multi_proxy.c:pr_receive_payload:198:RPROXY(0): pl_size=120
		 multi_proxy.c:pr_receive_payload:201:RPROXY(0): recv=120
		 multi_proxy.c:pr_receive_payload:203:RPROXY(0): n:120 | received:120
		 multi_proxy.c:pr_process_message:329:RPROXY(0): put2lcl
		 multi_proxy.c:pr_process_message:432:RPROXY(0):cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 multi_proxy.c:pr_process_message:434:RPROXY(0):c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 multi_proxy.c:pr_process_message:439:RPROXY(0): px_ptr->px_rdesc.td_batch_nr=1
		 multi_proxy.c:pr_process_message:454:RPROXY(0): bat_cmd[0]:cmd=0x3 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
		 multi_proxy.c:pr_process_message:456:RPROXY(0):cmd=0x3 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=0 PID=734
		 multi_proxy.c:pr_process_message:458:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:pr_process_message:460:RPROXY(0): 681 c_timestamp=1642070090.432006106
		 multi_proxy.c:pr_process_message:462:RPROXY(0): 681 c_flags=0x0 c_src_pid=0 c_dst_pid=0
		 multi_proxy.c:pr_process_message:464:RPROXY(0): source=50 type=10 m1i1=13 m1i2=0 m1i3=3 m1p1=0x4 m1p2=(nil) m1p3=(nil) 
		 multi_proxy.c:pr_start_serving:522:RPROXY(0): Message processing failure [-319]
		 multi_proxy.c:pr_process_message:252:RPROXY(0): About to receive header
		 multi_proxy.c:pr_receive_header:218:RPROXY(0): socket=5
 
 EN NODE0(LB)
		 777:lb_cltpxy.c:clt_Rproxy_rcvhdr:570:CLIENT_RPROXY (client11): n:120 | received:120 | HEADER_SIZE:120
		 777:lb_cltpxy.c:clt_Rproxy_rcvhdr:573:CLIENT_RPROXY (client11): cmd=0x2001 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=120 PID=734
		 777:lb_cltpxy.c:clt_Rproxy_rcvhdr:575:CLIENT_RPROXY (client11): c_batch_nr=1 c_flags=0x1 c_snd_seq=28 c_ack_seq=13
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:491:CLIENT_RPROXY(client11): header bytes=120
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:499:CLIENT_RPROXY(client11): cmd=0x2001 dcid=0 src=50 dst=10 snode=11 dnode=0 rcode=0 len=120 PID=734
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:501:CLIENT_RPROXY(client11): c_batch_nr=1 c_flags=0x1 c_snd_seq=28 c_ack_seq=13
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:503:CLIENT_RPROXY(client11): c_timestamp=1642070090.432006106
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:505:CLIENT_RPROXY(client11): c_flags=0x1 c_src_pid=28 c_dst_pid=13
		 777:lb_cltpxy.c:clt_Rproxy_rcvpay:595:CLIENT_RPROXY (client11): pl_size=120
		 777:lb_cltpxy.c:clt_Rproxy_rcvpay:600:CLIENT_RPROXY (client11): n:120 | received:120
		 777:lb_cltpxy.c:clt_Rproxy_getcmd:548:CLIENT_RPROXY(client11): payload bytes=120
		 777:lb_cltpxy.c:clt_Rproxy_loop:150:CLIENT_RPROXY(client11):Message succesfully processed.
		 777:lb_cltpxy.c:clt_Rproxy_loop:156:CLIENT_RPROXY(client11): svc_name=latency svc_dcid=0 svc_extep=10 svc_minep=10 svc_maxep=19 svc_bind=5 svc_prog=none
		 777:lb_cltpxy.c:clt_Rproxy_loop:166:CLIENT_RPROXY(client11): minep=10  maxep=19
		 777:lb_cltpxy.c:clt_Rproxy_loop:168:CLIENT_RPROXY(client11): ep=10
		 777:lb_cltpxy.c:clt_Rproxy_loop:172:CLIENT_RPROXY(client11): service endpoint found ep=10
		 777:lb_cltpxy.c:clt_Rproxy_2server:254:CLIENT_RPROXY(client11): 
		 777:lb_cltpxy.c:clt_Rproxy_2server:256:MTX_LOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_2server:272:MTX_LOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_2server:276:CLIENT_RPROXY(client11): Expired Session Found se_clt_PID=27 c_src_pid=28
		 777:lb_cltpxy.c:clt_Rproxy_2server:301:MTX_UNLOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_2server:327:MTX_UNLOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_2server:342:MTX_LOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:select_server:416:MTX_LOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:select_server:465:MTX_UNLOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:select_server:416:MTX_LOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:select_server:460:MTX_UNLOCK svr_ptr->svr_mutex 
		 777:lb_cltpxy.c:select_server:472:CLIENT_RPROXY(client11): server name=node1 new_ep=10
		 777:lb_cltpxy.c:clt_Rproxy_2server:364:CLIENT_RPROXY(client11): svr_name=node1 svr_nodeid=1 svr_compress=0 svr_batch=1
		 777:lb_cltpxy.c:clt_Rproxy_2server:366:CLIENT_RPROXY(client11): clt_name=client11 clt_nodeid=11 clt_lbRport=3011 clt_cltRport=3011 clt_compress=0 clt_batch=1
		 777:lb_cltpxy.c:clt_Rproxy_2server:392:CLIENT_RPROXY(client11): NEW Session with server node1
		 777:lb_cltpxy.c:clt_Rproxy_2server:394:CLIENT_RPROXY(client11):se_dcid=0 se_clt_nodeid=11 se_clt_ep=50 se_clt_PID=28
		 777:lb_cltpxy.c:clt_Rproxy_2server:396:CLIENT_RPROXY(client11):se_dcid=0 se_svr_nodeid=1 se_svr_ep=10 se_svr_PID=-1
		 777:lb_cltpxy.c:clt_Rproxy_2server:398:CLIENT_RPROXY(client11):se_dcid=0 se_lbclt_ep=10 se_lbsvr_ep=50 
		 777:lb_cltpxy.c:clt_Rproxy_2server:400:MTX_UNLOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_loop:191:MTX_LOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_loop:193:MTX_UNLOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:626:CLIENT_RPROXY(client11) BEFORE: cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=0 rcode=0 len=120 PID=734
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:627:CLIENT_RPROXY(client11) BEFORE: c_batch_nr=1 c_flags=0x1 c_snd_seq=28 c_ack_seq=13
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:628:MTX_LOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:665:MTX_UNLOCK sess_table[dcid].st_mutex 
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:668:CLIENT_RPROXY(client11) AFTER: cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:669:CLIENT_RPROXY(client11) AFTER: c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 777:lb_cltpxy.c:clt_Rproxy_svrmq:672:CLIENT_RPROXY(client11): sending HEADER 120 bytes to server node1 by mqid=0
		 
		 774:lb_svrpxy.c:svr_Sproxy_mqrcv:840:SERVER_SPROXY(node1): 120 bytes received
		 774:lb_svrpxy.c:svr_Sproxy_serving:695:SERVER_SPROXY(node1): cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 774:lb_svrpxy.c:svr_Sproxy_serving:697:SERVER_SPROXY(node1): c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 774:lb_svrpxy.c:svr_Sproxy_send:720:SERVER_SPROXY(node1): 
		 774:lb_svrpxy.c:svr_Sproxy_sndhdr:756:SERVER_SPROXY(node1): send header=120 
		 774:lb_svrpxy.c:svr_Sproxy_sndhdr:760:SERVER_SPROXY(node1): cmd=0x2001 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=120 PID=734
		 774:lb_svrpxy.c:svr_Sproxy_sndhdr:762:SERVER_SPROXY(node1): c_batch_nr=1 c_flags=0x1 c_snd_seq=-1263525888 c_ack_seq=13
		 774:lb_svrpxy.c:svr_Sproxy_sndhdr:780:SERVER_SPROXY(node1): sent header=120 
		 774:lb_svrpxy.c:svr_Sproxy_send:733:SERVER_SPROXY(node1): send payload len=120
		 774:lb_svrpxy.c:svr_Sproxy_sndpay:802:SERVER_SPROXY(node1): svr_Sproxy_sndpay bytesleft=120 
		 774:lb_svrpxy.c:svr_Sproxy_sndpay:806:SERVER_SPROXY(node1): src=27342 dst=27342 rqtr=27342 saddr=(nil) daddr=(nil) bytes=0 
		 774:lb_svrpxy.c:svr_Sproxy_sndpay:811:SERVER_SPROXY(node1): payload sent=120 
		 774:lb_svrpxy.c:svr_Sproxy_sndpay:825:SERVER_SPROXY(node1): sent payload=120 
		 774:lb_svrpxy.c:svr_Sproxy_send:736:FREE 0xb4b02000
		 774:lb_svrpxy.c:svr_Sproxy_serving:688:SERVER_SPROXY(node1): Reading message queue..
		 774:lb_svrpxy.c:svr_Sproxy_mqrcv:836:SERVER_SPROXY(node1): reading from mqid=0
 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
			EL PROBLEMA ESTA CON LOS BATCH EN GENERAL 
			EL LOAD BALANCER NO ABRE EL CONTENIDO DE LOS COMANDOS BATCHEADOS
			Y ALLI SIGUEN LOS DATOS DEL COMANDO ORIGINAL POR LO QUE 
			SE MANTIENEN LOS ENDPOINTS Y NODOS ORIGINALES Y ESTO
			HACE QUE FALLEN EN EL PROXY DEL SERVER 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

===================================================================================
20220114:
		PROBAR EL AGENT 
		EN PRINCIPIO PROBAR 2 SERVER CON 4 CLIENTES Y MEDIR BW/CPU  

EN NODE0: EL  MONITOR 
 872:lb_monitor.c:lbm_SP_receive:143:ret=36
 872:lb_monitor.c:lbm_SP_receive:165:node0: sender=#LBA.01#node1 Private_group=#LBM.00#node0 service_type=4
 872:lb_monitor.c:lbm_SP_receive:172:node0: FIFO message from #LBA.01#node1, of type 2222, (endian 0) to 1 groups (36 bytes)
 872:lb_monitor.c:get_nodeid:252:mbr_string=#LBA.01#node1
 872:lb_monitor.c:get_nodeid:264:mbr_string=#LBA.01#node1 nid=1
 872:lb_monitor.c:lbm_reg_msg:284:sender_ptr=#LBA.01#node1 agent_id=1 msg_type=2222
 872:lb_monitor.c:lbm_reg_msg:306:msg_type=2222
 872:lb_monitor.c:lbm_reg_msg:322:source=1 type=2222 m9i1=1 m9l1=50 m9t1.tv_sec=1642165698 m9t1.tv_nsec=921158368
 872:lb_monitor.c:lbm_lvlchg_msg:360:source=1 type=2222 m9i1=1 m9l1=50 m9t1.tv_sec=1642165698 m9t1.tv_nsec=921158368
 872:lb_monitor.c:lbm_lvlchg_msg:370:MTX_LOCK svr_ptr->svr_mutex 
 872:lb_monitor.c:lbm_lvlchg_msg:373:MTX_UNLOCK svr_ptr->svr_mutex 
 872:lb_monitor.c:lbm_lvlchg_msg:374:svr_name=node1 svr_nodeid=1 svr_lbRport=3001 svr_svrRport=3001 svr_level=1 svr_load=50 svr_bm_svc=0
 872:lb_monitor.c:lb_monitor:33:rcode=0
 872:lb_monitor.c:lb_monitor:29:lb_name=node0 lb_nodeid=0 lb_lowwater=30 lb_highwater=70 lb_period=30
 872:lb_monitor.c:lb_monitor:30:lb_name=node0 lb_nodeid=0 lb_nr_nodes=2 lb_nr_init=2 lb_bm_nodes=3 lb_bm_init=3
 872:lb_monitor.c:lbm_SP_receive:134:node0


EN NODE1: El agent 
 965:lb_agent.c:get_metrics:654:
 965:lb_agent.c:get_metrics:655:MTX_LOCK lba_ptr->lba_mutex 
 965:lb_agent.c:get_metrics:660:MTX_UNLOCK lba_ptr->lba_mutex 
 965:lb_agent.c:get_CPU_usage:701:
 965:lb_agent.c:get_CPU_usage:723:cpu  62638 0 78820 64106 68 0 63 0
 965:lb_agent.c:get_CPU_usage:741:didl=1467 Div=2899
 965:lb_agent.c:get_CPU_usage:749:cpu_usage=50 cpu_idle=50
 965:lb_agent.c:get_metrics:675:MTX_LOCK lba_ptr->lba_mutex 
 965:lb_agent.c:get_metrics:679:lba_mbr_name=LBA.01 lba_monitor=0 lba_load_lvl=1 lba_cpu_usage=50 lba_bm_eps=0
 965:lb_agent.c:mcast_load_level:601:source=1 type=2222 m9i1=1 m9l1=50 m9t1.tv_sec=1642165698 m9t1.tv_nsec=921158368
 965:lb_agent.c:get_metrics:682:MTX_UNLOCK lba_ptr->lba_mutex 
 964:lb_agent.c:lba_SP_receive:224:ret=36
 964:lb_agent.c:lba_SP_receive:246:LBA.01: sender=#LBA.01#node1 Private_group=#LBA.01#node1 service_type=4
 964:lb_agent.c:lba_SP_receive:253:LBA.01: FIFO message from #LBA.01#node1, of type 2222, (endian 0) to 1 groups (36 bytes)
 964:lb_agent.c:get_nodeid:333:mbr_string=#LBA.01#node1
 964:lb_agent.c:get_nodeid:345:mbr_string=#LBA.01#node1 nid=1
 964:lb_agent.c:lba_reg_msg:363:sender_ptr=#LBA.01#node1 node_id=1
 964:lb_agent.c:main:102:rcode=0
 964:lb_agent.c:main:98:lba_mbr_name=LBA.01 lba_monitor=0 lba_lowwater=30 lba_highwater=70 lba_period=30
 964:lb_agent.c:main:99:lba_mbr_name=LBA.01 lba_monitor=0 lba_nr_nodes=2 lba_nr_init=1 lba_bm_nodes=3 lba_bm_init=1
 964:lb_agent.c:lba_SP_receive:216:LBA.01

TODO:	Modificar el tests.sh como test_lb.sh 
		de tal modo que:
			1- setee el $dcid en 0  directamente
			2- que tome el $lcl del nombre del host. Es decir
					Si es node2 	que tome $lcl=2
					si es client13 	que tome $lcl=13 
			3- que cree automaticamente su archivo de configuracion de proxy
					o que modifique un template.
				Server:
					proxy node0 { <<<<<<<<<<<<<< modificar este 
						proxyid		0;
						proto		tcp; 
						rport		3000;
						sport		3001; 	<<<< modificar este 
						compress	NO; 
						batch		NO;
						autobind	NO;
						rname		node1;  <<<< modificar este 
					};
				Client:
					proxy client0 { <<<<<<<<<<<<<< modificar este 
						proxyid		0;
						proto		tcp; 
						rport		3000;
						sport		3011;  <<<< modificar este 
						compress	NO; 
						batch		NO;
						autobind	NO;
						rname       client11; <<<< modificar este 
					}; 
				
TODO:	Hacer las copias de las VMs NODE1 => NODE2
		Y de CLIENT11 => {CLIENT12, CLIENT13, CLIENT14} 
	

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 192.168.0.100:3001      0.0.0.0:*               LISTEN     
tcp        0      0 192.168.0.100:3002      0.0.0.0:*               LISTEN     
tcp        0      0 192.168.137.100:3011    0.0.0.0:*               LISTEN     
tcp        0      0 192.168.137.100:3012    0.0.0.0:*               LISTEN     
tcp        0      0 192.168.137.100:3013    0.0.0.0:*               LISTEN     
tcp        0      1 192.168.0.100:55092     192.168.0.101:3000      SYN_SENT   
tcp        0      1 192.168.137.100:45380   192.168.137.111:3000    SYN_SENT   
tcp        0      1 192.168.0.100:44378     192.168.0.102:3000      SYN_SENT   
tcp        0      1 192.168.137.100:48466   192.168.137.112:3000    SYN_SENT   
tcp        0      1 192.168.137.100:55036   192.168.137.113:3000    SYN_SENT   
	
===================================================================================
20220115:
		Se creo un script test_lb.sh para arrancar en los 3 tipos de nodos
		que segun el nombre realiza la inicializacion correspondiente
		- Load Balancer
		- Server
		- Client
		Tambien se hicieron dos scripts run_latency.sh y run_m3ftpd.sh para que 
		sean invocados desde sshpass, pero esto producen un fallo en NODE1
		En NODE0 da el siguiente error /tmp/node1.err 
			ssh: connect to host node1 port 22: Connection refused
	
Se implemento ucast_cmd para enviar un comando remoto al agente del server 
		via spread 

PROBLEMAS:
	Supongamos que se arranca un cliente, se arranca automaticamente el server y es atendido por el server
	una vez finalizado el cliente, el server permanece vivo.
	Si arranca nuevamente el cliente con los mismos endpoints en la sesion no coincide el clt_PID
	entonces, antes de arrancar un nuevo server, hay que matar al existente.

EL AGENTE RECIBE 	
 647:lb_agent.c:lba_reg_msg:393:MT_RUN_COMMAND= kill -s SIGKILL -1 >> /tmp/node1.out 2>> /tmp/node1.err
	RESUELTO, El problema es que el LB no usaba el campo c_pid del header y el PID del server estaba mal  
	EL PROBLEMA ESTA EN MULTIPROXY QUE ESTA PONIENDO NROS DE SECUENCIA EN LUGAR DE LOS PIDS

Cuando se reitera el mismo comando con los mismos endpoints, deberia matar al server existente y crear uno nuevo
EL LB ENVIA AL AGENTE EL COMANDO CORRECTO 
 675:lb_cltpxy.c:clt_Rproxy_2server:275:CLIENT_RPROXY(client11): Expired Session Found se_clt_PID=626 c_pid=627
 675:lb_cltpxy.c:clt_Rproxy_2server:305:CLIENT_RPROXY(client11): se_rmtcmd= kill -s SIGKILL 641 >> /tmp/node1.out 2>> /tmp/node1.err
EN EL SERVER 
	root       641     1  0 19:15 pts/0    00:00:00 ./latency_server 0 10
EN EL AGENTE
	632:lb_agent.c:lba_reg_msg:393:MT_RUN_COMMAND= kill -s SIGKILL 641 >> /tmp/node1.out 2>> /tmp/node1.err
	sh: 1: kill: invalid signal number or name: SIGKILL

RESUELTO
	root@node1:/usr/src/dvs/dvk-tests# ps -ef | grep latency
	root       652     1  0 19:48 pts/0    00:00:00 ./latency_server 0 10 <<<<<<<<<<<  652
	root       655   597  0 19:48 pts/0    00:00:00 grep latency
	root@node1:/usr/src/dvs/dvk-tests# ps -ef | grep latency
	root       662     1  0 19:49 pts/0    00:00:00 ./latency_server 0 10 <<<<<<<<<<< 662
	root       665   597  0 19:49 pts/0    00:00:00 grep latency

===================================================================================
20220117:
		Se creo startup script que arranca el test_lb 
		
		Se creo el archivo /etc/init.d/dvs 
				#! /bin/bash
				### BEGIN INIT INFO
				# Provides:       dvs 
				# Required-Start:    \$local_fs \$syslog
				# Required-Stop:     \$local_fs \$syslog
				# Default-Start:     2 3 4 5
				# Default-Stop:      0 1 6
				# Short-Description: starts dvs
				# Description:       starts dvs using start-stop-daemon
				### END INIT INFO

				# put your script here
				/usr/src/dvs/dvk-tests/test_lb.sh   
				exit 0
				
				chmod 755 /etc/init.d/dvs 
				update-rc.d dvs defaults

		RESULTADO EN NODE0 /var/log/syslog
		syslog:Jan 17 12:24:49 node0 systemd[1]: Starting LSB: starts dvs...
		syslog:Jan 17 12:24:49 node0 dvs[396]: eth0=xx
		syslog:Jan 17 12:24:50 node0 dvs[396]: node_name=node0 svr=0 clt=node0 dcid=0 lb=0 base_port=3000
		syslog:Jan 17 12:24:50 node0 dvs[396]: eth1=xx
		syslog:Jan 17 12:24:51 node0 dvs[396]: Running Load Balancer in Background
		syslog:Jan 17 12:24:52 node0 kernel: [    8.219436] Hello, DVS! dbglvl=FFFFFFFF dvs.d_dbglvl=FFFFFFFF
		syslog:Jan 17 12:24:52 node0 dvs[396]: dvk                   167936  0
		syslog:Jan 17 12:24:52 node0 dvs[396]: root       662   413  0 12:24 ?        00:00:00 ./lb_dvs lb_dvs.cfg
		syslog:Jan 17 12:24:52 node0 dvs[396]: root       664   413  0 12:24 ?        00:00:00 grep lb_dvs
		syslog:Jan 17 12:24:52 node0 systemd[1]: Started LSB: starts dvs.

		RESULTADO EN NODE1 
		Jan 17 12:36:05 node1 kernel: [    7.351314] Hello, DVS! dbglvl=FFFFFFFF dvs.d_dbglvl=FFFFFFFF
		Jan 17 12:36:05 node1 dvs[305]: dvk                   167936  0
		Jan 17 12:36:05 node1 dvs[305]: partition 5
		Jan 17 12:36:05 node1 dvs[305]: Initializing DVS. Local node ID 1...
		Jan 17 12:36:05 node1 dvs[305]: d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64
		Jan 17 12:36:05 node1 dvs[305]: d_max_copybuf=65536 d_max_copylen=1048576
		Jan 17 12:36:05 node1 dvs[305]: d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
		Jan 17 12:36:05 node1 dvs[305]: Get DVS info
		Jan 17 12:36:05 node1 dvs[305]: local node ID 1...
		Jan 17 12:36:05 node1 dvs[305]: d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64
		Jan 17 12:36:05 node1 dvs[305]: d_max_copybuf=65536 d_max_copylen=1048576
		Jan 17 12:36:05 node1 dvs[305]: d_dbglvl=FFFFFF version=5 flags=101 sizeof(proc)=512
		Jan 17 12:36:10 node1 dvs[305]: Active Internet connections (servers and established)
		Jan 17 12:36:10 node1 dvs[305]: Proto Recv-Q Send-Q Local Address           Foreign Address         State
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 0.0.0.0:16385           0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 0.0.0.0:4803            0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 192.168.0.101:3000      0.0.0.0:*               LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 192.168.0.101:22        192.168.0.196:55236     ESTABLISHED
		Jan 17 12:36:10 node1 dvs[305]: tcp        0      0 192.168.0.101:41488     192.168.0.100:3001      ESTABLISHED
		Jan 17 12:36:10 node1 dvs[305]: tcp6       0      0 :::873                  :::*                    LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp6       0      0 :::80                   :::*                    LISTEN
		Jan 17 12:36:10 node1 dvs[305]: tcp6       0      0 :::22                   :::*                    LISTEN
		Jan 17 12:36:10 node1 dvs[305]: ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		Jan 17 12:36:10 node1 dvs[305]:  0     6       0        0        0 -------------------------------- node0
		Jan 17 12:36:10 node1 dvs[305]:  1     6      -1        0        0 -------------------------------X node1
		Jan 17 12:36:10 node1 dvs[305]: Proxies Flags Sender Receiver --Proxies_Name- 10987654321098765432109876543210
		Jan 17 12:36:10 node1 dvs[305]:       0     3    587      586           node0 -------------------------------X
		Jan 17 12:36:10 node1 dvs[305]: ID Type -lpid- -flag- -misc- -pxsent- -pxrcvd- -getf- -sendt -wmig- name
		Jan 17 12:36:10 node1 dvs[305]:  0 send    587      8      3        0        0  31438  27342  27342 multi_proxy
		Jan 17 12:36:10 node1 dvs[305]:  0 recv    586      0      1        0        0  27342  27342  27342 multi_proxy
		Jan 17 12:36:10 node1 dvs[305]: dcid=0
		Jan 17 12:36:10 node1 dvs[305]: flags=0
		Jan 17 12:36:10 node1 dvs[305]: nr_procs=221
		Jan 17 12:36:10 node1 dvs[305]: nr_tasks=34
		Jan 17 12:36:10 node1 dvs[305]: nr_sysprocs=64
		Jan 17 12:36:10 node1 dvs[305]: nr_nodes=32
		Jan 17 12:36:10 node1 dvs[305]: dc_nodes=2
		Jan 17 12:36:10 node1 dvs[305]: dc_pid=582
		Jan 17 12:36:10 node1 dvs[305]: warn2proc=27342
		Jan 17 12:36:10 node1 dvs[305]: warnmsg=-1
		Jan 17 12:36:10 node1 dvs[305]: dc_name=DC0
		Jan 17 12:36:10 node1 dvs[305]: nodes 33222222222211111111110000000000
		Jan 17 12:36:10 node1 dvs[305]:       10987654321098765432109876543210
		Jan 17 12:36:10 node1 dvs[305]:       ------------------------------X-
		Jan 17 12:36:10 node1 dvs[305]: cpumask=1
		Jan 17 12:36:10 node1 dvs[305]: Adding node 0 to DC 0...
		Jan 17 12:36:11 node1 dvs[305]: ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		Jan 17 12:36:11 node1 dvs[305]:  0     E       0        0        0 -------------------------------X node0
		Jan 17 12:36:11 node1 dvs[305]:  1     6      -1        0        0 -------------------------------X node1
		Jan 17 12:36:11 node1 dvs[305]: dcid=0
		Jan 17 12:36:11 node1 dvs[305]: flags=0
		Jan 17 12:36:11 node1 dvs[305]: nr_procs=221
		Jan 17 12:36:11 node1 dvs[305]: nr_tasks=34
		Jan 17 12:36:11 node1 dvs[305]: nr_sysprocs=64
		Jan 17 12:36:11 node1 dvs[305]: nr_nodes=32
		Jan 17 12:36:11 node1 dvs[305]: dc_nodes=3
		Jan 17 12:36:11 node1 dvs[305]: dc_pid=582
		Jan 17 12:36:11 node1 dvs[305]: warn2proc=27342
		Jan 17 12:36:11 node1 dvs[305]: warnmsg=-1
		Jan 17 12:36:11 node1 dvs[305]: dc_name=DC0
		Jan 17 12:36:11 node1 dvs[305]: nodes 33222222222211111111110000000000
		Jan 17 12:36:11 node1 dvs[305]:       10987654321098765432109876543210
		Jan 17 12:36:11 node1 dvs[305]:       ------------------------------XX
		Jan 17 12:36:11 node1 dvs[305]: cpumask=1
		Jan 17 12:36:11 node1 dvs[305]: DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		Jan 17 12:36:11 node1 dvs[305]: run # . /dev/shm/DC0.sh
		Jan 17 12:36:11 node1 dvs[305]: Running lb_agent on server node1
		Jan 17 12:36:11 node1 systemd[1]: Started LSB: starts dvs.

		RESULTADO EN CLIENT11 
		root@client11:~# grep dvs /var/log/syslog
		Jan 17 12:40:28 client11 kernel: [    7.327284] Hello, DVS! dbglvl=FFFFFFFF dvs.d_dbglvl=FFFFFFFF
		Jan 17 12:40:28 client11 dvs[346]: dvk                   167936  0
		Jan 17 12:40:28 client11 dvs[346]: partition 5
		Jan 17 12:40:28 client11 dvs[346]: Initializing DVS. Local node ID 11...
		Jan 17 12:40:28 client11 dvs[346]: d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64
		Jan 17 12:40:28 client11 dvs[346]: d_max_copybuf=65536 d_max_copylen=1048576
		Jan 17 12:40:28 client11 dvs[346]: d_dbglvl=FFFFFF version=5 flags=0 sizeof(proc)=0
		Jan 17 12:40:28 client11 dvs[346]: Get DVS info
		Jan 17 12:40:28 client11 dvs[346]: local node ID 11...
		Jan 17 12:40:28 client11 dvs[346]: d_name=TEST_CLUSTER d_nr_dcs=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64
		Jan 17 12:40:28 client11 dvs[346]: d_max_copybuf=65536 d_max_copylen=1048576
		Jan 17 12:40:28 client11 dvs[346]: d_dbglvl=FFFFFF version=5 flags=101 sizeof(proc)=512
		Jan 17 12:40:33 client11 dvs[346]: Active Internet connections (servers and established)
		Jan 17 12:40:33 client11 dvs[346]: Proto Recv-Q Send-Q Local Address           Foreign Address         State
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 0.0.0.0:16385           0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 0.0.0.0:4803            0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 192.168.137.111:3000    0.0.0.0:*               LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp        0      0 192.168.137.111:43266   192.168.137.100:3011    ESTABLISHED
		Jan 17 12:40:33 client11 dvs[346]: tcp6       0      0 :::873                  :::*                    LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp6       0      0 :::80                   :::*                    LISTEN
		Jan 17 12:40:33 client11 dvs[346]: tcp6       0      0 :::22                   :::*                    LISTEN
		Jan 17 12:40:33 client11 dvs[346]: ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		Jan 17 12:40:33 client11 dvs[346]:  0     6       0        0        0 -------------------------------- client0
		Jan 17 12:40:33 client11 dvs[346]: 11     6      -1        0        0 -------------------------------X node11
		Jan 17 12:40:33 client11 dvs[346]: Proxies Flags Sender Receiver --Proxies_Name- 10987654321098765432109876543210
		Jan 17 12:40:33 client11 dvs[346]:       0     3    597      596         client0 -------------------------------X
		Jan 17 12:40:33 client11 dvs[346]: ID Type -lpid- -flag- -misc- -pxsent- -pxrcvd- -getf- -sendt -wmig- name
		Jan 17 12:40:33 client11 dvs[346]:  0 send    597      8      3        0        0  31438  27342  27342 multi_proxy
		Jan 17 12:40:33 client11 dvs[346]:  0 recv    596      0      1        0        0  27342  27342  27342 multi_proxy
		Jan 17 12:40:33 client11 dvs[346]: dcid=0
		Jan 17 12:40:33 client11 dvs[346]: flags=0
		Jan 17 12:40:33 client11 dvs[346]: nr_procs=221
		Jan 17 12:40:33 client11 dvs[346]: nr_tasks=34
		Jan 17 12:40:33 client11 dvs[346]: nr_sysprocs=64
		Jan 17 12:40:33 client11 dvs[346]: nr_nodes=32
		Jan 17 12:40:33 client11 dvs[346]: dc_nodes=800
		Jan 17 12:40:33 client11 dvs[346]: dc_pid=592
		Jan 17 12:40:33 client11 dvs[346]: warn2proc=27342
		Jan 17 12:40:33 client11 dvs[346]: warnmsg=-1
		Jan 17 12:40:33 client11 dvs[346]: dc_name=DC0
		Jan 17 12:40:33 client11 dvs[346]: nodes 33222222222211111111110000000000
		Jan 17 12:40:33 client11 dvs[346]:       10987654321098765432109876543210
		Jan 17 12:40:33 client11 dvs[346]:       --------------------X-----------
		Jan 17 12:40:33 client11 dvs[346]: cpumask=1
		Jan 17 12:40:33 client11 dvs[346]: Adding node 0 to DC 0...
		Jan 17 12:40:34 client11 dvs[346]: ID Flags Proxies -pxsent- -pxrcvd- 10987654321098765432109876543210 Name
		Jan 17 12:40:34 client11 dvs[346]:  0     6       0        0        0 -------------------------------X client0
		Jan 17 12:40:34 client11 dvs[346]: 11     6      -1        0        0 -------------------------------X node11
		Jan 17 12:40:34 client11 dvs[346]: dcid=0
		Jan 17 12:40:34 client11 dvs[346]: flags=0
		Jan 17 12:40:34 client11 dvs[346]: nr_procs=221
		Jan 17 12:40:34 client11 dvs[346]: nr_tasks=34
		Jan 17 12:40:34 client11 dvs[346]: nr_sysprocs=64
		Jan 17 12:40:34 client11 dvs[346]: nr_nodes=32
		Jan 17 12:40:34 client11 dvs[346]: dc_nodes=801
		Jan 17 12:40:34 client11 dvs[346]: dc_pid=592
		Jan 17 12:40:34 client11 dvs[346]: warn2proc=27342
		Jan 17 12:40:34 client11 dvs[346]: warnmsg=-1
		Jan 17 12:40:34 client11 dvs[346]: dc_name=DC0
		Jan 17 12:40:34 client11 dvs[346]: nodes 33222222222211111111110000000000
		Jan 17 12:40:34 client11 dvs[346]:       10987654321098765432109876543210
		Jan 17 12:40:34 client11 dvs[346]:       --------------------X----------X
		Jan 17 12:40:34 client11 dvs[346]: cpumask=1
		Jan 17 12:40:34 client11 dvs[346]: DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
		Jan 17 12:40:34 client11 dvs[346]: run # . /dev/shm/DC0.sh
		Jan 17 12:40:34 client11 systemd[1]: Started LSB: starts dvs.
		
		HASTA QUE NO ESTE ARRANCADA LA INTERFACE CORRESPONDIENTE NO DEBERIA ARRANCAR NI SPREAD, LB, AGENT 
		

PROBLEMA: MULTIPROXY 
			Al enviar el CLIENT el 1er mensaje al LB, este se lo pasa al SERVER 
			como todavia no se arranco el programa server ni hizo el bind, entonces 
			da -310 EDVSNOTBIND 	(_SIGN 310)  /* The process has not BINDed */
			Luego, no le envia el resultado al LB y este al CLIENT
			
		 multi_proxy.c:pr_receive_header:224:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
		 multi_proxy.c:pr_receive_header:227:RPROXY(0): cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0 PID=647
		 multi_proxy.c:pr_receive_header:229:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:pr_process_message:258:RPROXY(0):cmd=0x3 dcid=0 src=50 dst=10 snode=0 dnode=1 rcode=0 len=0 PID=647
		 multi_proxy.c:pr_process_message:260:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:pr_process_message:262:RPROXY(0): 622 c_timestamp=1642385962.420001185
		 multi_proxy.c:pr_process_message:264:RPROXY(0): 622 c_flags=0x0 c_src_pid=0 c_dst_pid=0
		 multi_proxy.c:pr_process_message:277:RPROXY(0): source=50 type=10 m1i1=0 m1i2=0 m1i3=3 m1p1=0x4 m1p2=(nil) m1p3=(nil) 
		 multi_proxy.c:pr_process_message:329:RPROXY(0): put2lcl
		 multi_proxy.c:pr_start_serving:523:RPROXY(0): Message processing failure [-310]
		 multi_proxy.c:pr_process_message:252:RPROXY(0): About to receive header
		 multi_proxy.c:pr_receive_header:218:RPROXY(0): socket=5
		 multi_proxy.c:ps_start_serving:909:SPROXY(0): Sending HELLO 
		 multi_proxy.c:ps_send_remote:831:SPROXY(0):cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0 PID=0
		 multi_proxy.c:ps_send_remote:832:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:ps_send_header:707:SPROXY(0): send header=120 
		 multi_proxy.c:ps_send_header:710:SPROXY(0):cmd=0x0 dcid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0 PID=0
		 multi_proxy.c:ps_send_header:711:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:ps_send_header:732:SPROXY(0): socket=6 sent header=120 	


PROBLEMA:
		Cuando se arranca el comando remoto, el LB le envia al proxy del SERVER el primer SENDREC 
		Como todavia no fue arrancado y bindeado, este puede que lo rechace 
			- verificar que no vuelve ningun codigo de error
			- como hacer para que el paquete del SENDREC no se envie al SERVER hasta tanto este bindeado ??
		ADEMAS EL KILL TAMBIEN RETRASA entonces el primer mensaje que se envia desde el LB al PROXY 
			Lo atiende el SERVER VIEJO 
			root@node1:/usr/src/dvs/dvk-tests# ps -ef | grep latency
			root       706     1  0 19:51 pts/0    00:00:00 ./latency_server 0 10
			Los mensajes siguientes los atiende el SERVER NUEVO 
			root@node1:/usr/src/dvs/dvk-tests# ps -ef | grep latency
			root       729     1  0 19:52 pts/0    00:00:00 ./latency_server 0 10

			ES POR ESO QUE EL CLIENTE NO EXPERIMENTA LATENCIA EN EL PRIMER MENSAJE
			PERO SI EN EL SEGUNDO Y LUEGO EL RESTO FUNCIONAN BIEN 
			root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 4
			Time total elapsed in loop 0: 0.0040431023 
			ERROR: 639:dvk_sendrec_T:948: rcode=-61
			ERROR: 639:dvk_sendrec_T:957: rcode=-61
			ERROR: latency_client.c:run_client:54: rcode=-61
			Time total elapsed in loop 2: 0.0027468204 
			Time total elapsed in loop 3: 0.0092267990 
			*********************************************************************
			Total average latency in 4 loops: 0.0040041804 [s] 
			Message throughput in 4 loops: 499.48 [msg/s]
			*********************************************************************		
			
	SOLUCION 1: Que el thread del SERVER_SENDER no envie el mensaje al CLIENT 
				hasta que el AGENT no le avise mediante un mensaje spread 
				El problema es que el AGENT no sabe ni el endpoint ni el DCID
				como para hacer un wait4bind
				Hay que modificar el mensaje de tal forma de que envie un mensaje minix 
				en el que indica MT_RUN_COMMAND y a continuacion va el comando 
				En los campos del mensaje minix, va el endpoint y DCID y el comando WAIT4BIND 
				Luego el SERVER_SENDER se queda bloqueado en una variable de condicion+mutex 
				por un timeout dado.
				De igual forma, para el KILL, va el endpoint y DCID y el comando WAIT4UNBIND 
	SOLUCION 2 (IMPLEMENTADA)
				Idem anterior pero en lugar de enviar un solo unicast, envia 2 unicast
				uno con el comando (string)
				y otro con la operacion a realizar WAIT4BIND O WAIT4UNBIND

ARRANCAR UNA VM EN VMWARE PLAYER 
	Se deberia incluir en el archivo de configuracion:
		node_start  "C:\Program Files (x86)\VMware\VMware Player\vmplayer.exe start"
		node_stop   "C:\Program Files (x86)\VMware\VMware Player\vmplayer.exe stop "
		node_img    "D:\PAP\Virtual Machines\Debian 9.4\Debian 9.4.vmx"
	Como verificar si ya arrancó una VM?
		Cuando el AGENT hizo el JOIN!! 
ATENCION: Para LISTA DOBLEMENTE ENLAZADA (TAILQ) se utiliza 
	https://linux.die.net/man/3/queue

===================================================================================
20220118:
	El agente del NODE1 recibe del LB la orden de ejecutar un comando 
		 613:lb_agent.c:lba_SP_receive:184:ret=91
		 613:lb_agent.c:lba_SP_receive:206:LBA.01: sender=#LBM.00#node0 Private_group=#LBA.01#node1 service_type=4
		 613:lb_agent.c:lba_SP_receive:213:LBA.01: FIFO message from #LBM.00#node0, of type 1004, (endian 0) to 1 groups (91 bytes)
		 613:lb_agent.c:get_nodeid:293:mbr_string=#LBM.00#node0
		 613:lb_agent.c:get_nodeid:305:mbr_string=#LBM.00#node0 nid=0
		 613:lb_agent.c:lba_reg_msg:323:sender_ptr=#LBM.00#node0 node_id=0 msg_type=1004
		 613:lb_agent.c:lba_reg_msg:392:MT_RUN_COMMAND=/usr/src/dvs/dvs-apps/dvs_lb/run_latency.sh 0 10 0 50 >> /tmp/node1.out 2>> /tmp/node1.err
	Luego recibe la orden de MT_CLT_WAIT_BIND=1000
		 613:lb_agent.c:lba_SP_receive:176:LBA.01
		 613:lb_agent.c:lba_SP_receive:184:ret=36
		 613:lb_agent.c:lba_SP_receive:206:LBA.01: sender=#LBM.00#node0 Private_group=#LBA.01#node1 service_type=4
		 613:lb_agent.c:lba_SP_receive:213:LBA.01: FIFO message from #LBM.00#node0, of type 1000, (endian 0) to 1 groups (36 bytes)
		 613:lb_agent.c:get_nodeid:293:mbr_string=#LBM.00#node0
		 613:lb_agent.c:get_nodeid:305:mbr_string=#LBM.00#node0 nid=0
		 613:lb_agent.c:lba_reg_msg:323:sender_ptr=#LBM.00#node0 node_id=0 msg_type=1000	
	
TODO:   Falta modificar el AGENT para que reciba los MT_CLT_WAIT_BIND y MT_CLT_WAIT_UNBIND
		Y envie una notificación al MONITOR 
		LB						AGENT
		------>MT_RUN_COMMAND---->
		------>MT_CLT_WAIT_BIND------>
		<------MT_CLT_WAIT_BIND+ACK---
		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	ERROR EN EL DVK WAIT4BIND
	ENTRA EN LOOP
	[   96.128765] DEBUG 638:new_wait4bind:2616: Other process bind waiting
	[   96.128766] DEBUG 638:new_wait4bind:2627: RLOCK_PROC ep=-2 count=0
	[   96.128766] DEBUG 638:new_wait4bind:2629: RUNLOCK_PROC ep=-2 count=0
	[   96.128766] DEBUG 638:new_wait4bind:2632: ret=-2
	[   96.128767] DEBUG 638:new_wait4bind:2634: pending: sig[0]:0x00000000, sig[1]:0x00000000
	[   96.128768] DEBUG 638:new_wait4bind:2637: shared_pending sig[0]:0x00000000, sig[1]:0x00000000
SOLUCIONADO: ERROR en new_wait4bind
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

EL PROXY CLIENT PREPARA EL COMANDO REMOTO A EJECUTAR EN EL SERVER 
 661:lb_cltpxy.c:select_server:520:CLIENT_RPROXY(client11): se_rmtcmd=/usr/src/dvs/dvs-apps/dvs_lb/run_latency.sh 0 10 0 50 >> /tmp/node1.out 2>> /tmp/node1.err
ENVIA EL COMANDO AL SERVER MEDIANTE UN UNICAST AL AGENT  
 661:lb_monitor.c:ucast_cmd:665:priv_name=#LBA.01#node1
ENVIA UN WAIT4BIND REMOTO CON  m1p1=0x56fdf8 DONDE ESTA clt_ptr
 661:lb_monitor.c:ucast_wait4bind:643:source=0 type=1 m1i1=0 m1i2=10 m1i3=1701080942 m1p1=0x56fdf8 m1p2=0xb75ba2f9 m1p3=0xb7704960 
 661:lb_monitor.c:ucast_wait4bind:646:priv_name=#LBA.01#node1
SE BLOQUEA CON TIMEOUT ESPERANDO LA NOTIFICACION DEL AGENT  
 661:lb_cltpxy.c:select_server:534:MTX_LOCK clt_ptr->clt_agent_mtx 
 661:lb_cltpxy.c:select_server:538:COND_WAIT_T clt_ptr->clt_agent_cond clt_ptr->clt_agent_mtx
 
EL AGENT RECIBE LA PETICION DESDE EL LB DE WAIT4BIND
 639:lb_agent.c:lba_SP_receive:226:LBA.01: sender=#LBM.00#node0 Private_group=#LBA.01#node1 service_type=4
 639:lb_agent.c:lba_SP_receive:233:LBA.01: FIFO message from #LBM.00#node0, of type 1, (endian 0) to 1 groups (36 bytes)
 639:lb_agent.c:get_nodeid:313:mbr_string=#LBM.00#node0
 639:lb_agent.c:get_nodeid:325:mbr_string=#LBM.00#node0 nid=0
 639:lb_agent.c:lba_reg_msg:343:sender_ptr=#LBM.00#node0 node_id=0 msg_type=1
EL AGENT HACE SU BIND, WAIT4BIND Y UNBIND. EN m1p1=0x56fdf8 ESTA clt_ptr
 639:lb_agent.c:lba_reg_msg:429:source=0 type=1 m1i1=0 m1i2=10 m1i3=1701080942 m1p1=0x56fdf8 m1p2=0xb75ba2f9 m1p3=0xb7704960 
 639:lb_agent.c:lba_reg_msg:458:dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
 639:lb_agent.c:lba_reg_msg:461:dvk_bind rcode=-2
 639:lb_agent.c:lba_reg_msg:467:msg_type=1
 639:lb_agent.c:lba_reg_msg:473:dvk_wait4bindep_T ret=10
 639:lb_agent.c:lba_reg_msg:481:dvk_unbind rcode=0
ENVIA UN UNICAST AL LB 
 639:lb_agent.c:ucast_lb_ack:31:source=1 type=4097 m1i1=0 m1i2=10 m1i3=10 m1p1=0x56fdf8 m1p2=0xb75ba2f9 m1p3=0xb7704960 
 
LUEGO, EL LB RECIBE EL RESULTADO DEL WAIT4BIND + ACKNOWLEDGE
 666:lb_monitor.c:lbm_SP_receive:142:ret=36
 666:lb_monitor.c:lbm_SP_receive:164:node0: sender=#LBA.01#node1 Private_group=#LBM.00#node0 service_type=4
 666:lb_monitor.c:lbm_SP_receive:171:node0: FIFO message from #LBA.01#node1, of type 4097, (endian 0) to 1 groups (36 bytes)
 666:lb_monitor.c:get_nodeid:251:mbr_string=#LBA.01#node1
 666:lb_monitor.c:get_nodeid:263:mbr_string=#LBA.01#node1 nid=1
 666:lb_monitor.c:lbm_reg_msg:284:sender_ptr=#LBA.01#node1 agent_id=1 msg_type=4097
 666:lb_monitor.c:lbm_reg_msg:303:lb_name=node0 lb_nodeid=0 lb_nr_nodes=1 lb_nr_init=1 lb_bm_nodes=1 lb_bm_init=2
 666:lb_monitor.c:lbm_reg_msg:306:msg_type=1001
 666:lb_monitor.c:lbm_reg_msg:359:source=1 type=4097 m1i1=0 m1i2=10 m1i3=10 m1p1=0x56fdf8 m1p2=0xb75ba2f9 m1p3=0xb7704960 
COMO EL RESULTADO ES > CERO, EN ESTE CASO m1i3=10 OBTIENE EL clt_ptr QUE LO ENVIO EN m1p1
 666:lb_monitor.c:lbm_reg_msg:376:MTX_LOCK clt_ptr->clt_agent_mtx 
 666:lb_monitor.c:lbm_reg_msg:377:COND_SIGNAL clt_ptr->clt_agent_cond
 666:lb_monitor.c:lbm_reg_msg:378:MTX_UNLOCK clt_ptr->clt_agent_mtx 

===================================================================================
20220120:
		Cual es la razon para poner en colas las peticiones de KILL ??
		Por ahora las remuevo dejandolas como comentarios.
	
ANTE 2 PRUEBAS SUCESIVAS EL PID DEL SERVER CAMBIA 
	root@node1:~# ps -ef | grep latency
	root       636     1  0 11:42 ?        00:00:00 ./latency_server 0 10
	root@node1:~# ps -ef | grep latency
	root       663     1  0 11:43 ?        00:00:00 ./latency_server 0 10

	EL LB DETECTA UNA SESION EXPIRADA Y ENVIA UNCAST DE KILL AL AGENT DEL NODE1   
	 669:lb_cltpxy.c:clt_Rproxy_2server:277:CLIENT_RPROXY(client11): Expired Session Found se_clt_PID=642 c_pid=645
	 669:lb_cltpxy.c:clt_Rproxy_2server:307:CLIENT_RPROXY(client11): se_rmtcmd= kill -9 636 >> /tmp/node1.out 2>> /tmp/node1.err
	 669:lb_monitor.c:ucast_cmd:665:priv_name=#LBA.01#node1
	LUEGO ENVIA UN WAIT4UNBIND
	 669:lb_monitor.c:ucast_wait4bind:643:source=0 type=2 m1i1=0 m1i2=10 m1i3=1701080942 m1p1=0x4f6df8 m1p2=0xb74e92f9 m1p3=0xb7633960 
	 669:lb_monitor.c:ucast_wait4bind:646:priv_name=#LBA.01#node1
	EL AGENT DEL NODE1 HACE EL KILL
	 624:lb_agent.c:lba_reg_msg:343:sender_ptr=#LBM.00#node0 node_id=0 msg_type=6
	 624:lb_agent.c:lba_reg_msg:412:MT_RUN_COMMAND= kill -9 636 >> /tmp/node1.out 2>> /tmp/node1.err
	 624:lb_agent.c:lba_reg_msg:418:system rcode=0
	LUEGO HACE UN WAIT4UNBIND
	624:lb_agent.c:lba_reg_msg:343:sender_ptr=#LBM.00#node0 node_id=0 msg_type=2
	 624:lb_agent.c:lba_reg_msg:429:source=0 type=2 m1i1=0 m1i2=10 m1i3=1701080942 m1p1=0x4f6df8 m1p2=0xb74e92f9 m1p3=0xb7633960 
	 624:lb_agent.c:lba_reg_msg:458:dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
	 624:lb_agent.c:lba_reg_msg:461:dvk_bind rcode=-337
	 624:lb_agent.c:lba_reg_msg:463:SYSTEM=-2
	ERROR: lb_agent.c:lba_reg_msg:464: rcode=-337

	AQUI EL AGENT HACE EL UNBIND 
	Jan 20 12:09:07 node1 kernel: [   49.879725] DEBUG 618:dvk_ioctl:349: cmd=4004E30A arg=BF97FDD0
	Jan 20 12:09:07 node1 kernel: [   49.879726] DEBUG 618:dvk_ioctl:369: DVK_CALL=10 (io_unbind) 
	Jan 20 12:09:07 node1 kernel: [   49.879727] DEBUG 618:io_unbind:109: 
	Jan 20 12:09:07 node1 kernel: [   49.879727] DEBUG 618:new_unbind:1910: dcid=0 proc_ep=0
	Jan 20 12:09:07 node1 kernel: [   49.879728] DEBUG 618:check_caller:616: caller_pid=618 caller_tgid=618
	Jan 20 12:09:07 node1 kernel: [   49.879729] DEBUG 618:check_caller:657: WLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879730] DEBUG 618:check_caller:662: nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=618 vpid=618 nodeid=1 name=lb_agent 
	Jan 20 12:09:07 node1 kernel: [   49.879731] DEBUG 618:check_caller:722: WUNLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879731] DEBUG 618:check_caller:725: dcid=0
	Jan 20 12:09:07 node1 kernel: [   49.879732] DEBUG 618:check_caller:729: RLOCK_DC dc=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879732] DEBUG 618:check_caller:733: RUNLOCK_DC dc=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879733] DEBUG 618:check_caller:739: caller_pid=618 
	Jan 20 12:09:07 node1 kernel: [   49.879733] DEBUG 618:check_caller:742: RLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879734] DEBUG 618:check_caller:746: RUNLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879734] DEBUG 618:new_unbind:1926: RLOCK_DC dc=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879735] DEBUG 618:new_unbind:1938: RUNLOCK_DC dc=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879735] DEBUG 618:new_unbind:1940: RLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879736] DEBUG 618:new_unbind:1957: RUNLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879736] DEBUG 618:new_unbind:1960: RLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879737] DEBUG 618:new_unbind:1965: RUNLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879738] DEBUG 618:new_unbind:1967: WLOCK_TASK pid=618 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879738] DEBUG 618:new_unbind:1984: WLOCK_DC dc=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879739] DEBUG 618:new_unbind:1988: WLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.879740] DEBUG 618:do_unbind:733: nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=618 vpid=618 nodeid=1 name=lb_agent 
	Jan 20 12:09:07 node1 kernel: [   49.879741] DEBUG 618:do_unbind:762: Caller nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=618 vpid=618 nodeid=1 name=lb_agent 
	Jan 20 12:09:07 node1 kernel: [   49.879742] DEBUG 618:do_unbind:826: wakeup with error those processes trying to send a message to the proc
	Jan 20 12:09:07 node1 kernel: [   49.879742] DEBUG 618:do_unbind:864: delete notify messages bits sent by the proc
	Jan 20 12:09:07 node1 kernel: [   49.880130] DEBUG 618:do_unbind:968: nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=618 vpid=618 nodeid=1 name=lb_agent 
	Jan 20 12:09:07 node1 kernel: [   49.880131] DEBUG 618:do_unbind:1027: wakeup with error those processes waiting this process MIGRATION
	Jan 20 12:09:07 node1 kernel: [   49.880132] DEBUG 618:do_unbind:1078: wakeup those processes waiting this process UNBINDING
	Jan 20 12:09:07 node1 kernel: [   49.880133] DEBUG 618:do_unbind:1112: nr=0 endp=0 dcid=0 flags=0 misc=20 lpid=618 vpid=618 nodeid=1 name=lb_agent 
	Jan 20 12:09:07 node1 kernel: [   49.880134] DEBUG 618:init_proc_desc:16: p_name=lb_agent dcid=0
	Jan 20 12:09:07 node1 kernel: [   49.880134] DEBUG 618:init_proc_desc:27: Clearing Privileges
	Jan 20 12:09:07 node1 kernel: [   49.880134] DEBUG 618:init_proc_desc:35: Setting Default DVK calls privileges
	Jan 20 12:09:07 node1 kernel: [   49.880135] DEBUG 618:init_proc_desc:41: Clearing Process fields
	Jan 20 12:09:07 node1 kernel: [   49.880135] DEBUG 618:do_unbind:1114: initialized 
	Jan 20 12:09:07 node1 kernel: [   49.880136] DEBUG 618:do_unbind:1115: DC_DECREF counter=3
	Jan 20 12:09:07 node1 kernel: [   49.880137] DEBUG 618:new_unbind:2021: WUNLOCK_TASK pid=618 count=0
	Jan 20 12:09:07 node1 kernel: [   49.880137] DEBUG 618:new_unbind:2047: WUNLOCK_PROC ep=0 count=0
	Jan 20 12:09:07 node1 kernel: [   49.880138] DEBUG 618:new_unbind:2050: WUNLOCK_DC dc=0 count=0

	AQUI HACE EL BIND Y DA ERROR : QUIERE DECIR QUE EL UNBIND DEJO ALGO MAL?
	Luego intente hacer un bind en el mismo DCID y ENDPOINT con el comando test_bind y funciono sin problemas.
		Jan 20 12:22:21 node1 kernel: [  843.639479] DEBUG 618:dvk_ioctl:349: cmd=4004E309 arg=BF97FDB8
		Jan 20 12:22:21 node1 kernel: [  843.639480] DEBUG 618:dvk_ioctl:369: DVK_CALL=9 (io_bind) 
		Jan 20 12:22:21 node1 kernel: [  843.639480] DEBUG 618:io_bind:97: 
		Jan 20 12:22:21 node1 kernel: [  843.639482] DEBUG 618:new_bind:1605: oper=0 dcid=0 param_pid=-1 endpoint=0 nodeid=-1
		Jan 20 12:22:21 node1 kernel: [  843.639483] DEBUG 618:new_bind:1628: dc_ptr=f8ea4000
		Jan 20 12:22:21 node1 kernel: [  843.639483] DEBUG 618:new_bind:1630: RLOCK_DC dc=0 count=0
		Jan 20 12:22:21 node1 kernel: [  843.639484] DEBUG 618:new_bind:1643: proc_ptr=edf24400
		Jan 20 12:22:21 node1 kernel: [  843.639484] DEBUG 618:new_bind:1644: WLOCK_PROC ep=0 count=0
		Jan 20 12:22:21 node1 kernel: [  843.639485] DEBUG 618:new_bind:1674: WUNLOCK_PROC ep=0 count=0
		Jan 20 12:22:21 node1 kernel: [  843.639486] DEBUG 618:new_bind:1675: RUNLOCK_DC dc=0 count=0
		Jan 20 12:22:21 node1 kernel: [  843.639486] ERROR: 618:new_bind:1675: rcode=-337
		Jan 20 12:22:21 node1 kernel: [  843.639487] ERROR: 618:dvk_ioctl:373: rcode=-337 

	AQUI ESTA DANDO EL ERROR  
			// IS the task already bound ?
			if (task_ptr->task_proc != NULL){
					UNLOCK_TASK_LIST; //read_unlock(&tasklist_ptr);
					WUNLOCK_PROC(proc_ptr);
					ERROR_RUNLOCK_DC(dc_ptr, EDVSSLOTUSED);
			}	
RESUELTO!!!! modificacion del DVK 

....................................................
ESTO NO ME CIERRA 
			
			// Same destination endpoint from the same source node ??
			if( (sess_ptr->se_clt_nodeid == hdr_ptr->c_snode)
			&&  (sess_ptr->se_lbclt_ep	 == hdr_ptr->c_dst) ){
				rcode = clt_Rproxy_error(clt_ptr, EDVSRSCBUSY);
				MTX_UNLOCK(sess_table[dcid].st_mutex);
			}
SI SE HACE LA PRIMER SESION DESDE CLIENT(50)->LB(10) FUNCIONA 
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
	Time total elapsed in loop 0: 0.0389900208 
	Time total elapsed in loop 1: 0.0027911663 
	Time total elapsed in loop 2: 0.0031378269 
	Time total elapsed in loop 3: 0.0024330616 
	Time total elapsed in loop 4: 0.0030939579 
	*********************************************************************
	Total average latency in 5 loops: 0.0100892067 [s] 
	Message throughput in 5 loops: 198.23 [msg/s]
	*********************************************************************

SI SE HACE OTRA SESION DESDE CLIENT(51)->LB(10) NO ANDA 
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 51 10 5
Y EL ERROR SE PRESENTA -342 EDVSRSCBUSY
 657:lb_cltpxy.c:clt_Rproxy_loop:151:CLIENT_RPROXY(client11):Message succesfully processed.
 657:lb_cltpxy.c:clt_Rproxy_loop:157:CLIENT_RPROXY(client11): svc_name=latency svc_dcid=0 svc_extep=10 svc_minep=10 svc_maxep=19 svc_bind=4 svc_prog=/usr/src/dvs/dvs-apps/dvs_lb/run_latency.sh
 657:lb_cltpxy.c:clt_Rproxy_loop:167:CLIENT_RPROXY(client11): minep=10  maxep=19
 657:lb_cltpxy.c:clt_Rproxy_loop:169:CLIENT_RPROXY(client11): ep=10
 657:lb_cltpxy.c:clt_Rproxy_loop:173:CLIENT_RPROXY(client11): service endpoint found ep=10
 657:lb_cltpxy.c:clt_Rproxy_2server:257:CLIENT_RPROXY(client11): 
 657:lb_cltpxy.c:clt_Rproxy_2server:259:MTX_LOCK sess_table[dcid].st_mutex 
 657:lb_cltpxy.c:clt_Rproxy_error:220:CLIENT_RPROXY(client11): Replying -342	
LUEGO
 657:lb_cltpxy.c:clt_Rproxy_2server:371:MTX_UNLOCK sess_table[dcid].st_mutex 
 657:lb_cltpxy.c:clt_Rproxy_2server:375:MTX_UNLOCK sess_table[dcid].st_mutex 
 657:lb_cltpxy.c:clt_Rproxy_error:220:CLIENT_RPROXY(client11): Replying -306
ERROR: lb_cltpxy.c:clt_Rproxy_loop:188: rcode=-11	


 multi_proxy.c:pr_receive_header:224:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
 multi_proxy.c:pr_receive_header:227:RPROXY(0): cmd=0x2003 dcid=0 src=10 dst=51 snode=0 dnode=11 rcode=-342 len=0 PID=646
 multi_proxy.c:pr_receive_header:229:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=21 c_ack_seq=10
 multi_proxy.c:pr_process_message:258:RPROXY(0):cmd=0x2003 dcid=0 src=10 dst=51 snode=0 dnode=11 rcode=-342 len=0 PID=646
 multi_proxy.c:pr_process_message:260:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=21 c_ack_seq=10
 multi_proxy.c:pr_process_message:262:RPROXY(0): 585 c_timestamp=1642703974.608853132
 multi_proxy.c:pr_process_message:264:RPROXY(0): 585 c_flags=0x0 c_src_pid=21 c_dst_pid=10
 multi_proxy.c:pr_process_message:329:RPROXY(0): put2lcl
 multi_proxy.c:pr_start_serving:523:RPROXY(0): Message processing failure [-313] EDVSACKSRC 	(_SIGN 313)  /* The IPC proxy sent local process a BAD source ACK  */
 
 SE MODIFICO EL DVK Y AHORA FUNCIONA 
		 root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
		Time total elapsed in loop 0: 0.0259270668 
		Time total elapsed in loop 1: 0.0039730072 
		Time total elapsed in loop 2: 0.0024201870 
		Time total elapsed in loop 3: 0.0019888878 
		Time total elapsed in loop 4: 0.0031499863 
		*********************************************************************
		Total average latency in 5 loops: 0.0074918270 [s] 
		Message throughput in 5 loops: 266.96 [msg/s]
		*********************************************************************
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
		Time total elapsed in loop 0: 0.0308279991 
		Time total elapsed in loop 1: 0.0023150444 
		Time total elapsed in loop 2: 0.0028979778 
		Time total elapsed in loop 3: 0.0020070076 
		Time total elapsed in loop 4: 0.0028760433 
		*********************************************************************
		Total average latency in 5 loops: 0.0081848145 [s] 
		Message throughput in 5 loops: 244.35 [msg/s]
		*********************************************************************
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 51 10 5
		ERROR: 634:dvk_sendrec_T:948: rcode=-342
		ERROR: 634:dvk_sendrec_T:957: rcode=-342
		ERROR: latency_client.c:run_client:57: rcode=-342

DE TODOS MODOS HAY QUE ANALIZAR PORQUE NO CREAR OTRA SESION 
EN REALIDAD EL PROBLEMA ESTA EN LA APLICACION SERVER 
CUANDO RECIBE UN PEDIDO, LE RESPONDE AL CLIENTE QUE LO PIDIO Y LISTO.
EL TEMA ES COMO MANEJAR MULTIPLES SESIONES. POR EJEMPLO MULTIPLES FTP 
POSIBLE SOLUCION 
		EL SERVER ATIENDE EN UN ENDPOINT CONOCIDO, PERO TIENE UN RANGO DE ENDPOINTS PARA USAR-
		ESCUCHA EN EL ENDPOINT CONOCIDO 
		CUANDO RECIBE EL PRIMER PUT O GET SE FIJA SI TIENEN ENDPOINS AUXILIARES LIBRE
		SI TIENE, TOMA UNO Y LO DEVUELVE EN LA RESPUESTA
		EL CLIENTE, 
			SI ES REMOTO HACE EL BIND DE ESE ENDPOINT
			Y A PARTIR DE ALLI COMIENZA A DIALOGAR CON EL ENDPOINT AUXILIAR
		EL SERVER
			CREA UN HILO PARA ATENDER AL CLIENTE 
			HACE EL BIND LOCAL DEL ENDPOINT AUXILIAR
			SI EN UN TIEMPO DADO NO HAY TRAFICO
				HACE EL UNBIND DEL ENDPOINT
				LIBERAR REGISTRO DEL ENDPOINT AUXILIAR 
PARA RESOLVER ESTO 				
			// Same destination endpoint from the same source node ??
			if( (sess_ptr->se_clt_nodeid == hdr_ptr->c_snode)
			&&  (sess_ptr->se_lbclt_ep	 == hdr_ptr->c_dst) ){
				rcode = clt_Rproxy_error(clt_ptr, EDVSRSCBUSY);
				MTX_UNLOCK(sess_table[dcid].st_mutex);
			}				
NO DEBERIA CREAR UN SERVER EN UNO DE LOS ENDPOINTS DEL RANGO?
RESUELTO !!!!! SE ANULO ESE CODIGO 
	PRIMER SESSION 50->10
		root@client11:/usr/src/dvs/dvk-tests# ./test_rmtbind 0 10 0 latency_server
		 dvk_rmtbind latency_server with p_nr=10 to DC0 on node=0
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
		Time total elapsed in loop 0: 0.0338058472 
		Time total elapsed in loop 1: 0.0056469440 
		Time total elapsed in loop 2: 0.0027279854 
		Time total elapsed in loop 3: 0.0018939972 
		Time total elapsed in loop 4: 0.0035340786 
		*********************************************************************
		Total average latency in 5 loops: 0.0095217705 [s] 
		Message throughput in 5 loops: 210.04 [msg/s]
		*********************************************************************
	SEGUNDA SESSION 50->10
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
		Time total elapsed in loop 0: 0.0292849541 
		Time total elapsed in loop 1: 0.0020060539 
		Time total elapsed in loop 2: 0.0054659843 
		Time total elapsed in loop 3: 0.0018899441 
		Time total elapsed in loop 4: 0.0020580292 
		*********************************************************************
		Total average latency in 5 loops: 0.0081409931 [s] 
		Message throughput in 5 loops: 245.67 [msg/s]
		*********************************************************************
	PRIMER SESSION AL 51->10
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 51 10 5
		Time total elapsed in loop 0: 0.0139069557 
		Time total elapsed in loop 1: 0.0060021877 
		Time total elapsed in loop 2: 0.0029740334 
		Time total elapsed in loop 3: 0.0034370422 
		Time total elapsed in loop 4: 0.0023839474 
		*********************************************************************
		Total average latency in 5 loops: 0.0057408333 [s] 
		Message throughput in 5 loops: 348.38 [msg/s]
		*********************************************************************
	PRIMER SESSION AL 51->2(FUERA DE RANGO)
		root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 51 2 5
		ERROR: 641:dvk_sendrec_T:948: rcode=-105
		ERROR: 641:dvk_sendrec_T:957: rcode=-105
		ERROR: latency_client.c:run_client:57: rcode=-105

LUEGO SE CREARON MULTIPLES INSTANCIAS AUTOMATICAMENTE CAMBIANDO EL ENDPOINT DEL CLIENTE
	EN EL CLIENTE
	./latency_client 0 50 10 5
	./latency_client 0 51 10 5
	./latency_client 0 52 10 5
	./latency_client 0 53 10 5
	./latency_client 0 54 10 5
	./latency_client 0 55 10 5
	./latency_client 0 56 10 5
	./latency_client 0 57 10 5
	./latency_client 0 58 10 5

	EN EL SERVER 
	root       642     1  0 16:27 ?        00:00:00 ./latency_server 0 10
	root       648     1  0 16:27 ?        00:00:00 ./latency_server 0 11
	root       695     1  0 16:33 ?        00:00:00 ./latency_server 0 12
	root       700     1  0 16:33 ?        00:00:00 ./latency_server 0 13
	root       706     1  0 16:33 ?        00:00:00 ./latency_server 0 14
	root       713     1  0 16:33 ?        00:00:00 ./latency_server 0 15
	root       719     1  0 16:33 ?        00:00:00 ./latency_server 0 16
	root       724     1  0 16:33 ?        00:00:00 ./latency_server 0 17
	root       730     1  0 16:33 ?        00:00:00 ./latency_server 0 18

===================================================================================
20220128:	Se configuro SSH CLIENT FOR WINDOWS para conectarse con KEY PAIR 
			contra NODE0, NODE1, NODE2 Y CLIENT11
			El instructivo esta en pdf en BOOKS
				ARCHIVED: Using SSH Secure Shell for Windows,
				how do I set up public key authentication?
			https://kb.iu.edu/d/amzx
			NO SE REALIZARON CAMBIOS en el archivo de configuracion de los HOSTS
			/etc/ssh/sshd_config << SIN CAMBIOS 
			
			
===================================================================================
TODO: FTP CLIENT/SERVER CONCURRENTE 
		En este caso hay que configurar el LB para que no arranque automaticamente el proceso SERVER 
		CLIENT(>50) ---->PUT/GET ------>> SERVER(10) 
											Crea un thread en un endpoint libre (ej: 11)
											le pasa como parametros 
												- el mensaje (para que sepa el endpoint client)
											y hace el bind de su propio thread 
											lock
											marca el endpoint 11 como ocupado 
											unlock
											el cual al arrancar hace un wait4bind
											de donde obtiene su propio endpoint 
											luego vuelve a seguir escuchando
	
		CLIENT(>50)	<---- TRY IN 11 ---- SERVER(10) El server le envia al client el endpoint del cliente
		
		CLIENT(>50) ---->PUT/GET ------>> SERVER(11) El server compara ambos mensajes y si son identicos responde
													Cuando la transferencia termina 
													el server hace 
													lock
													marca el endpoint 11 como libre  
													unlock
													finaliza 
													
											
											
TODO: CUANDO SE QUEDA SIN PODER LEVANTAR MAS SESIONES EN EL SERVER NODE1 
		DEBERIA PODER LEVANTAR SERVER EN NODE2 PREVIAMENTE AL ARRANCAR LA VM
		LA PREGUNTA ES: 
			Donde definir los limites de NODOS a arrancar? en el LB o en el Servicio?
		Deberia ser en el servicio.
		La cantidad de servers se define en el archivo de configuracion al mencionar "server"
		En el servicio deberia haber un parametro "max_nodes" que limitaria la cantidad de nodos que darian soporte al servicio
		Si la cantidad de nodos definida en el archivo de configuracion es menor que max_nodes, ese sera obviamente el limite 
		Por otro lado hay que definir en cada nodo que programa ejecutar y el nombre del archivo para arrancar una VM.
		En el archivo de configuracion 
			server node1 {
				nodeid 		1;
				compress	NO;
				batch		NO;	
				node_start  "C:\Program Files (x86)\VMware\VMware Player\vmplayer.exe start";
				node_stop   "C:\Program Files (x86)\VMware\VMware Player\vmplayer.exe stop ";
				node_image  "E:\NODE1\Debian 9.4.vmx";
			};
		PROBLEMA: Estos comandos se DEBEN ejecutar en el prompt de WINDOWS, no en LINUX!!!
			Como hacer para ejecutarlos desde Linux ?????
					
		SOLUCION UTILIZAR WINEXE
		BAJE winexe_4.13.0.1-1_i386.deb de 
		https://software.opensuse.org/package/winexe
		https://software.opensuse.org/download/package?package=winexe&project=home%3Auibmz%3Awinexe
		
			root@node0:/usr/src# dpkg --install winexe_4.13.0.1-1_i386.deb 
			Seleccionando el paquete winexe previamente no seleccionado.
			(Leyendo la base de datos ... 319020 ficheros o directorios instalados actualmente.)
			Preparando para desempaquetar winexe_4.13.0.1-1_i386.deb ...
			Desempaquetando winexe (4.13.0.1-1) ...
			dpkg: problemas de dependencias impiden la configuraciÃ³n de winexe:
			 winexe depende de libjansson4 (>= 2.0.1); sin embargo:
			  El paquete `libjansson4' no estÃ¡ instalado.

			dpkg: error al procesar el paquete winexe (--install):
			 problemas de dependencias - se deja sin configurar
			Se encontraron errores al procesar:
			 winexe
			root@node0:/usr/src# apt-get install libjansson4
			Leyendo lista de paquetes... Hecho
			Creando Ã¡rbol de dependencias       
			Leyendo la informaciÃ³n de estado... Hecho
			Se instalarÃ¡n los siguientes paquetes NUEVOS:
			  libjansson4
			0 actualizados, 1 nuevos se instalarÃ¡n, 0 para eliminar y 67 no actualizados.
			1 no instalados del todo o eliminados.
			Se necesita descargar 30,3 kB de archivos.
			Se utilizarÃ¡n 79,9 kB de espacio de disco adicional despuÃ©s de esta operaciÃ³n.
			Des:1 http://ftp.us.debian.org/debian stretch/main i386 libjansson4 i386 2.9-1 [30,3 kB]
			Descargados 30,3 kB en 10s (2.850 B/s)
			Seleccionando el paquete libjansson4:i386 previamente no seleccionado.
			(Leyendo la base de datos ... 319023 ficheros o directorios instalados actualmente.)
			Preparando para desempaquetar .../libjansson4_2.9-1_i386.deb ...
			Desempaquetando libjansson4:i386 (2.9-1) ...
			Configurando libjansson4:i386 (2.9-1) ...
			Configurando winexe (4.13.0.1-1) ...
			Procesando disparadores para libc-bin (2.24-11+deb9u4) ...
			root@node0:/usr/src# dpkg --install winexe_4.13.0.1-1_i386.deb 
			(Leyendo la base de datos ... 319032 ficheros o directorios instalados actualmente.)
			Preparando para desempaquetar winexe_4.13.0.1-1_i386.deb ...
			Desempaquetando winexe (4.13.0.1-1) sobre (4.13.0.1-1) ...
			Configurando winexe (4.13.0.1-1) ...
	RESULTADO: HAY QUE TENER INSTALADO UN WINEXESVC.EXE EN WINDOWS QUE PUEDE RESULTAR PELIGROSO 
	USAR SSH 	Se instalo segun 
		https://virtualizationreview.com/articles/2020/05/21/ssh-server-on-windows-10.aspx
	    luego active el servicio OpenSSH Server 
		desde el linux 
				root@node0:~# ssh Admin@192.168.0.196
				Admin@192.168.0.196's password: 
				

		
		
		
		
TODO:  Esta todo mal. Se esta fijando si el agent esta levantado y por default el agent esta levantado
		Eso se usaria solo si se levanta la VM.
	ADEMAS el wait4bind generico no contiene el DCID, con lo cual no lo puedo usar 
	long dvk_wait4bindep_X(int cmd, int endpoint, unsigned long timeout);
	SOLUCION TRUCHA: Hacer un bind del agent con endpoint=-2 
	Cuando sale hacer unbind()
	
	CREE EL BATCH EN WINDOWS C:\Users\Usuario>startvm.bat
		cd C:\Program Files (x86)\VMware\VMware Workstation\
		vmrun -T ws start  "E:\NODE2\Debian 9.4.vmx" nogui 
	PERO SUCEDE LO SIGUIENTE DESDE LINUX 
	
		root@node0:~# sshpass -p 'mendieta' ssh Admin@192.168.0.196 "startvm.bat"
		admin@USUARIO-PC C:\Users\Usuario>cd C:\Program Files (x86)\VMware\VMware Workstation\ 
		admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun -T ws  start "E:\NODE1\Debian 9.4.vmx" nogui 
	DICE QUE EJECUTAN 3 VMS 
		admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun list  
		Total running VMs: 3
		J:\Virtual Machines\Debian 9.4 NODE0\Debian 9.4.vmx
		E:\CLIENT11\Debian 9.4.vmx
		E:\NODE1\Debian 9.4.vmx
	PERO LUEGO DICE QUE EJECUTAN 2 VMS 
		root@node0:~# sshpass -p 'mendieta' ssh Admin@192.168.0.196 "vmlist.bat"
		admin@USUARIO-PC C:\Users\Usuario>cd C:\Program Files (x86)\VMware\VMware Workstation\ 
		admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun list  
		Total running VMs: 2
		J:\Virtual Machines\Debian 9.4 NODE0\Debian 9.4.vmx
		E:\CLIENT11\Debian 9.4.vmx

EN SESION DE Administrador 
Microsoft Windows [Versión 10.0.19044.1466]
(c) Microsoft Corporation. Todos los derechos reservados.
C:\WINDOWS\system32>ssh-keygen -A
C:\WINDOWS\system32>C:\WINDOWS\System32\OpenSSH\sshd.exe -D

C:\WINDOWS\System32\OpenSSH\sshd.exe -D 
DEPENDIENDO DEL USUARIO DE WINDOWS UTILIZADO Admin o Administrador, da diferentes errores
	root@node0:~# sshpass -p 'mendieta' ssh Admin@192.168.0.196 "vmlist.bat"
	Authentication failed.
	root@node0:~# sshpass -p 'mendieta' ssh Administrador@192.168.0.196 "vmlist.bat"
	packet_write_wait: Connection to 192.168.0.196 port 22: Broken pipe

https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement
https://stackoverflow.com/questions/16212816/setting-up-openssh-for-windows-using-public-key-authentication

================================================================================================

SIGUIENDO EL INSTRUCTIVO 
https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement

C:\Users\Usuario>ssh-add .ssh\id_ed25519
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Permissions for '.ssh\\id_ed25519' are too open.
It is required that your private key files are NOT accessible by others.
This private key will be ignored.

C:\Users\Usuario>icacls .\private.key /inheritance:r
.\private.key: El sistema no puede encontrar el archivo especificado.
Se procesaron correctamente 0 archivos; error al procesar 1 archivos

C:\Users\Usuario>icacls .ssh\id_ed25519 /inheritance:r
archivo procesado: .ssh\id_ed25519
Se procesaron correctamente 1 archivos; error al procesar 0 archivos

C:\Users\Usuario>icacls .ssh\id_ed25519 /grant:r "%username%":"(R)"
archivo procesado: .ssh\id_ed25519
Se procesaron correctamente 1 archivos; error al procesar 0 archivos

C:\Users\Usuario>ssh-add .ssh\id_ed25519
Identity added: .ssh\id_ed25519 (Admin@Usuario-PC)

============================================================================
RESUELTO : NO SE UTILIZO LOGIN POR INTERCAMBIO DE CLAVES 
Hacer en Windows en el directorio siguiente  
	C:\Users\Usuario>dir *.bat
	 El volumen de la unidad C es DISCO C
	 El número de serie del volumen es: A295-0210
	 Directorio de C:\Users\Usuario
	31/01/2022  23:00               222 startvm.bat <<<<<<<<<<
	20/01/2022  19:10                65 vmlist.bat
				   2 archivos            287 bytes
el siguiente script			   
	cd C:\Program Files (x86)\VMware\VMware Workstation\
	rem vmrun -T ws  start "E:\\NODE2\\Debian 9.4.vmx" 
	vmrun -T ws -u Admin -p mendieta start "E:\NODE2\Debian 9.4.vmx" nogui
	:loop1
		vmrun list 
		sleep 60
	goto loop1
LA VM DURA MIENTRAS ESTE SCRIPT ESTE VIVO!!

Para invocarlo desde LINUX a través de SSH
	root@node0:~#  sshpass -p 'mendieta' ssh Admin@192.168.0.196 "startvm.bat" > /tmp/node2.out 2> /tmp/node2.err &
============================================================================
20220201: 

		LISTA la incorporacion en el archivo de configuracion :
			When the load level of all active servers is LVL_SATURATED during 
			a specified START_VM_PERIOD, the LBM commands the hypervisor to start 
			a new node (scale up). If a server node has no active sessions during 
			a specified SHUTDOWN_PERIOD, it will be shut down (scale down).
			IMPLEMENTAR EN CONFIGURACION START_PERIOD SHUTDOWN_PERIOD como parametro del LB 
			ARCHIVO DE CONFIGURACION DEL LB 
			lb node0 {
				nodeid 		0;
				lowwater	30;
				highwater	70;
				period		30;
				start		60; <<=== START_VM_PERIOD
				shutdown 	60; <<=== SHUTDOWN_VM_PERIOD
				ssh_user	Admin;
				ssh_pass	mendienta;
				cltname		client0;
				svrname     node0;	
				cltdev		eth1;
				svrdev   	eth0;
			};


PARA DLB:   Pensar si conviene automatizar que al levantar los SERVERS se registran automaticamente en el LB 
		O dejarlos fijos segun archivo de configuracion y levantarlos a demanda???
		POR AHORA NO CONVIENE AUTOMATIZAR !! SERVERS CONFIGURADOS EN CONFIG  

		Tanto SERVER como CLIENTs pueden activarse via SPREAD 
		SERVER:
			Hacen un Multicast 
			Actualment deberia ser 
			sprintf( lba_ptr->lba_mbr_name, "LBA.%s", hostname);
			Despues del JOIN le envia al LBM los LBA_CONFIG_NEW parametros de configuracion 
			cada LBA tiene su propio archivo de configuracion 
			server node1 {
				nodeid 		1;
				compress	NO;
				batch		NO;	
				vm_start  "C:\Users\Usuario\start_vm.bat";
				vm_stop   "C:\Users\Usuario\stop_vm.bat"
				node_image  "E:\NODE1\Debian 9.4.vmx";
			};
			El LBM verifica todos los parámetros y arranca el par de PROXIES correspondientes
			Dando de alta en la base de datos y enviando un UNICAST LBA_CONFIG_ACK 
		CLIENT 
			client client11 {
				nodeid 		11;
				compress	NO;
				batch		NO;	
			};
		El LBM verifica todos los parámetros y arranca el par de PROXIES correspondientes
			Dando de alta en la base de datos y enviando un UNICAST LBA_CONFIG_ACK
	

ATENCION!!!! 
	NO SOPORTA ESPACIOS EN EL NOMBRE DEL ARCHIVO VMX
	NO SOPORTA EL NOMBRE DE WINDOWS SINO LA IP 	
	Asi que se renombro a "node1.vmx"
nohup sshpass -p mendieta ssh Admin@192.168.0.196 startvm.bat Admin mendieta 'E:\\NODE1\\node1.vmx' > /tmp/vm_node1.out 2> /tmp/vm_node1.err &
nohup sshpass -p mendieta ssh Admin@192.168.0.196 stopvm.bat Admin mendieta 'E:\\NODE1\\node1.vmx' > /tmp/vm_node1.out 2> /tmp/vm_node1.err &
sshpass -p 'mendieta' ssh Admin@192.168.0.196 "vmlist.bat"

EN WINDOWS 
	admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun -T ws start "E:\\NODE1\\node1.vmx" nogui 
	admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun list  
	Total running VMs: 3
	J:\Virtual Machines\Debian 9.4 NODE0\Debian 9.4.vmx
	E:\CLIENT11\Debian 9.4.vmx
	E:\NODE1\node1.vmx
	admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>sleep 60 

EN CLIENT11 
	PRIMERO EL SERVER NODE1 NO ESTA ARRANCADO Y SE FUERZA EL ARRANQUE 
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
	ERROR: 650:dvk_sendrec_T:948: rcode=-109
	ERROR: 650:dvk_sendrec_T:957: rcode=-109
	ERROR: latency_client.c:run_client:57: rcode=-109

	YA ARRANCO EL NODO, PERO TODAVIA NO ARRACO EL SERVER 
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
	ERROR: 652:dvk_sendrec_T:948: rcode=-61
	ERROR: 652:dvk_sendrec_T:957: rcode=-61
	ERROR: latency_client.c:run_client:54: rcode=-61
	Time total elapsed in loop 1: 25.2820310593 
	Time total elapsed in loop 2: 31.1201808453 
	Time total elapsed in loop 3: 0.0199480057  <<<<<<<<<<< AQUI ARRANCO 
	Time total elapsed in loop 4: 0.0019891262 
	*********************************************************************
	Total average latency in 5 loops: 11.2848298073 [s] 
	Message throughput in 5 loops: 0.18 [msg/s]
	*********************************************************************

	AQUI YA ESTA ARRANCADO
	root@client11:/usr/src/dvs/dvk-tests# ./latency_client 0 50 10 5
	Time total elapsed in loop 0: 0.0404770374 
	Time total elapsed in loop 1: 0.0020840168 
	Time total elapsed in loop 2: 0.0031449795 
	Time total elapsed in loop 3: 0.0023260117 
	Time total elapsed in loop 4: 0.0037779808 
	*********************************************************************
	Total average latency in 5 loops: 0.0103620052 [s] 
	Message throughput in 5 loops: 193.01 [msg/s]
	
EN NODE1: SE VE EL AGENT Y EL SERVER 
	root       625     1  0 18:34 ?        00:00:00 ./lb_agent
	root       636     1  0 18:34 ?        00:00:00 ./latency_server 0 10
	

LISTO:	
		Cuando levantar una VM?
			Cuando todos los activos servers ya estan saturados 		LISTO! 
					o bien porque estan LVL_SATURATED
					o bien porque tienen ENDPOINTS LIBRES 
			Solo se levantan si quedan servidores configurados pero no arrancados LISTO 


LISTO:	Cuando el cliente hace un sendrec() fuerza al arranque de la VM con el NODE1
		Cuando vuelve a intentar, otra vez vuelve a hacer lo mismo y asi lo hace 100 veces

		1) Para ello, hay que marcar al server como que está arrancando. LISTO 
		2) Como hacer para que se ignore al cliente que ya forzó el arranque de otro server?
				si hay un server arrancando, no arrancar otro.			LISTO


EL CLIENTE ENVIO EL 2DO MENSAJE (SEPARADOS 1 SEGUNDO) 
	latency_client.c:run_client:49:CLIENT SEND MSG AT: 1643804396.15 
EL CLIENTE RECIBE LA RESPUESTA EN EL MENSAJE NRO 30
	latency_client.c:run_client:58:CLIENT RECEIVE server MSG AT: 1643804425.28 
TIEMPO DE ARRANQUE = 29 SEGUNDOS !!!!!!

EN EL SERVER 
	root       712     1  0 09:20 ?        00:00:00 ./lb_agent
	root       720     1  0 09:20 ?        00:00:00 ./latency_server 0 10
	
	root@node1:~# cat /proc/dvs/DC0/procs 
	DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
	 0  10    10   720/720    1    8   20 31438 27342 27342 27342 latency_server 
	 0  50    50    -1/-1     0 1000    0 27342 27342 27342 27342 clt_0_0_50  
 
	 root@node1:~# cat /proc/dvs/DC0/stats 
	DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
	 0  10    10   720/720    1        0       10        0        0 latency_server 
	 0  50    50    -1/-1     0        0        0        0        0 clt_0_0_50 
		
	admin@USUARIO-PC C:\Program Files (x86)\VMware\VMware Workstation>vmrun list  
	Total running VMs: 3
	J:\Virtual Machines\Debian 9.4 NODE0\Debian 9.4.vmx
	E:\CLIENT11\Debian 9.4.vmx
	E:\NODE1\node1.vmx

PARA SABER SI UN SERVER NO SE USA SE PUEDE UTILIZA EL SERVER_RPROXY 	
 1395:lb_svrpxy.c:svr_Rproxy_rcvhdr:486:SERVER_RPROXY (node1): n:120 | received:120 | HEADER_SIZE:120
 1395:lb_svrpxy.c:svr_Rproxy_rcvhdr:489:SERVER_RPROXY (node1): cmd=0x0 dcid=0 src=10 dst=50 snode=1 dnode=0 rcode=0 len=0 PID=720
 1395:lb_svrpxy.c:svr_Rproxy_rcvhdr:491:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=10 c_ack_seq=20
 1395:lb_svrpxy.c:svr_Rproxy_getcmd:428:SERVER_RPROXY(node1): NONE
 1395:lb_svrpxy.c:svr_Rproxy_getcmd:433:SERVER_RPROXY(node1): Replying NONE
 1395:lb_svrpxy.c:svr_Rproxy_getcmd:414:SERVER_RPROXY(node1): About to receive header
	UN SERVER NO SE USA SI:
		- Cuando el SERVER hace el JOIN se guarda su timestamp en un campo del server svr_idle_ts
		- El SERVER_RPROXY cuando recibe un header != NONE actualiza su timestamp en un campo del server svr_idle_ts
		- El SERVER_RPROXY cuando recibe un header == NONE y su estado es !UNLOAD actualiza su timestamp en un campo del server svr_idle_ts		
		- El SERVER_RPROXY cuando recibe un header == NONE y su estado es UNLOAD resta el diff = timestamp-svr_idle_ts
			- Si diff > lb.lb_stop entonces puede hacer SHUTDOWN de la VM 
	
FUNCIONO!!!	
		root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep IDLE lb_dvs.out 
		 1776:lb_svrpxy.c:svr_Rproxy_getcmd:446:SERVER_RPROXY(node1): Server IDLE diff_secs=27
		 1776:lb_svrpxy.c:svr_Rproxy_getcmd:446:SERVER_RPROXY(node1): Server IDLE diff_secs=31
		 1776:lb_svrpxy.c:svr_Rproxy_getcmd:446:SERVER_RPROXY(node1): Server IDLE diff_secs=31
		 1776:lb_svrpxy.c:svr_Rproxy_getcmd:446:SERVER_RPROXY(node1): Server IDLE diff_secs=62 <<<<< DISPARA EL STOP 
		root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "STOP THE VM" lb_dvs.out
		 1776:lb_svrpxy.c:svr_Rproxy_getcmd:448:SERVER_RPROXY(node1): STOP THE VM!!!


root@node0:/usr/src/dvs/dvs-apps/dvs_lb# sshpass -p mendieta ssh Admin@192.168.0.196 stopvm.bat Admin mendieta 'E:\\NODE1\\node1.vmx' 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# killall sshpass
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# killall ssh
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# killall lb_dvs
REVISAR LOS ARCHIVO LCK DE LA VM 
sshpass -p 'mendieta' ssh Admin@192.168.0.196 "vmlist.bat"
LISTAR VMS 	

============================================================================
20220203: 
LISTO:	ATENCION CON LA MANIPULACION DE SERVERS!! HAY QUE PROTEGER CON MUTEXES
		ATENCION PUEDE SER NECESARIO MANIPULAR LOS DATOS DEL LB CON MUTEXES 


LISTO:	Agregar max_servers y min_servers a la estructura LB para limitar la cantidad de servers
		esto se contrasta con lb_nr_init.
		
LISTO:	Antes de levantar una nueva VM se tiene que fijar que no llego al max_servers
		
LISTO:  En el monitor, al hacer el JOIN de si mismo 
		- ver cuantos agentes hay conectados 
		- si lb_nr_init < min_servers, entonces arrancar las VMs de los nuevos servers.
		(es BEST EFFORT)

ERROR: PORQUE TRATA DE LEVANTAR 2 VECES DESDE EL MONITOR 
root     29476   977  0 13:09 pts/0    00:00:00 ./lb_dvs lb_dvs.cfg
root     29489     1  0 13:09 pts/0    00:00:00 sshpass -p zzzzzzzz ssh Admin@192.168.0.196 
root     29490 29489  0 13:09 pts/1    00:00:00 ssh Admin@192.168.0.196 startvm.bat Admin me
root     29499     1  0 13:10 pts/0    00:00:00 sshpass -p zzzzzzzz ssh Admin@192.168.0.196 
root     29500 29499  0 13:10 pts/2    00:00:00 ssh Admin@192.168.0.196 startvm.bat Admin me

LISTO : 	Antes de TERMINAR un NODO o VM fijarse si lb_nr_init > min_servers

LISTO:	El chequeo del servidor IDLE lo hace el MONITOR al recibir el LOAD_LEVEL del server 
	ERROR !!!!!! El server SOLO envia mensajes si cambio el LEVEL!! por lo tanto
			si el level no cambia, no va a chequear nada !!
			
LISTO: El chequeo si ya termino de STARTING o STOPPING se hace start_new_node.

LISTO: En el main thread, hacer un loop the 30 segundos  para verificar 
	1) Ver si los servers estan IDLE 
	2) Ver si los servers terminaron de STARTING o STOPPING 
FUNCIONO !!!	
	root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep IDLE lb_dvs.out
	 30114:lb_dvs.c:main:280:Server node1: Server IDLE diff=30
	 30114:lb_dvs.c:main:280:Server node1: Server IDLE diff=60
	 30114:lb_dvs.c:main:280:Server node1: Server IDLE diff=90
	root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep STOP lb_dvs.out
	 30114:lb_dvs.c:main:282:Server node1: STOP THE VM!!!
	
Que controle:
LISTO	Si falla el arranque de la VM y el server queda marcado como arrancando!!
LISTO 	Tomar un tiempo para que arranque. Si se vence ese tiempo, borrar el flag de STARTING 	
LISTO 	Se puede utilizar el timestamp para saber cuanto tiempo hace q se arranco.
LISTO	Si hace el join se borra el STARTING 
LISTO 	Si hay un cliente que pide acceder a un server y esta en STARTING hace demasiado tiempo
		lo borra.

==========================================================================================
20220207: 		
		LISTO, SIN PROBAR:	Reemplazar dvs_ptr->d_nr_nodes por el valor del DVS dvs_ptr->d_nr_nodes.  		

TODO:	INCORPORAR parametro vm_status y cambiar el path continuo por solo el nombre del archivo
		que estará en el directorio HOME del usuario.
		
	vm_start 	"start_vm.bat";
	vm_stop   	"stop_vm.bat";
	vm_status  	"list_vm.bat";
	
LISTO:	Como controlar el estado de una VM desde el punto de vista del HYPERVISOR
		Crear un script que permita saber si la VM está arrancada o finalizada 
		Poner en ECHO OFF los scripts 
FALTA PROBAR !!!!!!!
		
		SCRIPTS EN WINDOWS PARA START, STOP Y LIST DE VMS 
		startvm.bat <pass> <user> <file.vmx>
		stopvm.bat 	<pass> <user> <file.vmx>
		listvm.bat <vmname>
		
		EJEMPLOS 
		startvm.bat Admin mendieta E:\\NODE1\\node1.vmx
		stopvm.bat  Admin mendieta E:\\NODE1\\node1.vmx
		listvm.bat node1.vmx <<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEJA UN ARCHIVO EN C:\Users\Usuario\%1.lb
							CON LA SALIDA DE UN FIND 
		
		sshpass -p mendieta scp Admin@192.168.0.196:C\:\\Users\\Usuario\\%s.out /tmp

LISTO:	OPTIMIZAR EL CODIGO DEL SWITCH 
			ESTO ESTA EN CASI TODOS LOS PARAMETROS EXCEPTO EL nodeid;
			case TKN_LB_STATUS:
				if( lb.lb_nodeid == LB_INVALID){
					fprintf(stderr, "lb nodeid must be defined first: line %d\n", cfg->line);
					return(EXIT_CODE);
				}
	
		
LISTO: 
	Hacer una  funcion de INIT para cada una de las estructuras de datos 
	De esa forma cuando se cae un SERVER por ejemplo, hay que inicializar sus datos 

LISTO:	
	Cuando bajar una VM 
	LISTO	Cuando la sesion no se usa hace mucho tiempo (agregar un campo a la session de timestamp)
					cuando el LB recibe del server un mensaje, pega el timestamp del cmd_t en la sesion
					siempre y cuando el header se CMD_NODE
	LISTO	Cuando el uso de CPU tiene varios ciclos LVL_IDLE

LISTO:		Hacer un shutdown del OS antes de STOPEAR la VM 

==========================================================================================
20220214:

	LISTO:   Ver si hay problemas de COMPRESION+BATCH al anularlo en el Proxy
		SIN PROBAR 
	
ATENCION:	Cuando llegan a un proxy receiver multiples comandos BATCH , estos podrian ir a 
		diferentes endpoints destinos que pueden estar redirigidos a diferentes nodos 
		por lo que hay que desbatchearlos y volverlos a batchear individualmente. 
		POR LO TANTO, POR AHORA, CONVIENE TRABAJAR
				COMPRESS	YES 
				BATCH		NO 	
		
		
root@client11:/usr/src/dvs/dvs-apps/m3ftp# ./m3ftp -g 0 61 20 file-100M.dat file1-100M.dat > gftp1.out &[1] 649
root@client11:/usr/src/dvs/dvs-apps/m3ftp# ./m3ftp -g 0 62 20 file-100M.dat file2-100M.dat > gftp2.out &[2] 650
root@client11:/usr/src/dvs/dvs-apps/m3ftp# ERROR: 650:dvk_sendrec_T:948: rcode=-109
ERROR: 650:dvk_sendrec_T:957: rcode=-109
ERROR: m3ftp.c:ftp_request:37: rcode=-109
ERROR: m3ftp.c:main:257: rcode=-109

EL PROBLEMA ES EL SIGUIENTE
	Cuando se modifico el m3ftp para ser utilizado con el mm3ftpd, en el 1er mensaje que envia
	le dice a que endpoint apuntar los siguientes mensajes
	Esto no es visto por el PROXY por lo que falla 
		CLIENT(61->20)======LB========SERVER(20)			FUNCIONA OK 
		CLIENT(62->20)======LB========SERVER(21)			FALLA, PORQUE EL SERVER RETORNA 21 EN EL MENSAJE DE PRIMER REPLY
	
SI EN LUGAR DEL m3ftpd uso el mm3ftpd el problema es otro
	El mm3ftpd utiliza para recibir solo el endpoint 20 quiere decir que:
		CLIENT(61->20)======LB========SERVER(20)			FUNCIONA OK 
		CLIENT(62->20)======LB========SERVER(21)			FALLA PORQUE AQUI NO ESTA ESCUCHANDO EL MM3FTPD !!

EN CLIENT 	
	./m3ftp -g 0 61 20 file-1M.dat file1-1M.dat 		FUNCIONA OK
	./m3ftp -g 0 61 20 file-10M.dat file1-10M.dat		FUNCIONA OK
	./m3ftp -g 0 61 21 file-10M.dat file1-10M.dat		FALLA
	
==========================================================================================
20220215:

EN CLIENT 	

root@client11:/usr/src/dvs/dvs-apps/m3ftp# ./m3ftp -g 0 61 21 file-1M.dat file1-1M.

EL LB 
 662:lb_cltpxy.c:clt_Rproxy_rcvhdr:721:CLIENT_RPROXY (client11): n:120 | received:120 | HEADER_SIZE:120
 662:lb_cltpxy.c:clt_Rproxy_rcvhdr:724:CLIENT_RPROXY (client11): cmd=0x3 dcid=0 src=61 dst=21 snode=11 dnode=0 rcode=0 len=0 PID=655
 662:lb_cltpxy.c:clt_Rproxy_rcvhdr:726:CLIENT_RPROXY (client11): c_batch_nr=0 c_flags=0x0 c_snd_seq=27 c_ack_seq=22
 662:lb_cltpxy.c:clt_Rproxy_getcmd:642:CLIENT_RPROXY(client11): header bytes=120
 662:lb_cltpxy.c:clt_Rproxy_getcmd:650:CLIENT_RPROXY(client11): cmd=0x3 dcid=0 src=61 dst=21 snode=11 dnode=0 rcode=0 len=0 PID=655
 662:lb_cltpxy.c:clt_Rproxy_getcmd:652:CLIENT_RPROXY(client11): c_batch_nr=0 c_flags=0x0 c_snd_seq=27 c_ack_seq=22
 662:lb_cltpxy.c:clt_Rproxy_getcmd:654:CLIENT_RPROXY(client11): c_timestamp=1644943385.932002416
 662:lb_cltpxy.c:clt_Rproxy_getcmd:656:CLIENT_RPROXY(client11): c_flags=0x0 c_pid=655
 662:lb_cltpxy.c:clt_Rproxy_loop:153:CLIENT_RPROXY(client11):Message succesfully processed.
 662:lb_cltpxy.c:clt_Rproxy_loop:160:CLIENT_RPROXY(client11): svc_name=latency svc_dcid=0 svc_extep=10 svc_minep=10 svc_maxep=19 svc_bind=5 svc_prog=none
 662:lb_cltpxy.c:clt_Rproxy_loop:169:CLIENT_RPROXY(client11): extep=10 minep=10  maxep=19 c_dst=21
 662:lb_cltpxy.c:clt_Rproxy_loop:160:CLIENT_RPROXY(client11): svc_name=m3ftp0 svc_dcid=0 svc_extep=20 svc_minep=20 svc_maxep=20 svc_bind=5 svc_prog=none
 662:lb_cltpxy.c:clt_Rproxy_loop:169:CLIENT_RPROXY(client11): extep=20 minep=20  maxep=20 c_dst=21
 662:lb_cltpxy.c:clt_Rproxy_loop:160:CLIENT_RPROXY(client11): svc_name=m3ftp1 svc_dcid=0 svc_extep=21 svc_minep=21 svc_maxep=21 svc_bind=5 svc_prog=none
 662:lb_cltpxy.c:clt_Rproxy_loop:169:CLIENT_RPROXY(client11): extep=21 minep=21  maxep=21 c_dst=21
 662:lb_cltpxy.c:clt_Rproxy_2server:268:CLIENT_RPROXY(client11): 
 662:lb_cltpxy.c:clt_Rproxy_2server:270:MTX_LOCK sess_table[dcid].st_mutex 
 662:lb_cltpxy.c:clt_Rproxy_2server:278:CLIENT_RPROXY(client11) se_clt_ep=60 c_src=61
 662:lb_cltpxy.c:clt_Rproxy_2server:397:MTX_UNLOCK sess_table[dcid].st_mutex 
 662:lb_cltpxy.c:clt_Rproxy_2server:412:MTX_LOCK sess_table[dcid].st_mutex 
 662:lb_cltpxy.c:select_server:488:MTX_LOCK lb_ptr->lb_mtx 
 662:lb_cltpxy.c:select_server:495:MTX_LOCK svr_ptr->svr_mutex 
 662:lb_cltpxy.c:select_server:519:CLIENT_RPROXY(client11): svr_bm_svc=1 bit=0
 662:lb_cltpxy.c:select_server:596:MTX_UNLOCK svr_ptr->svr_mutex 
 662:lb_cltpxy.c:select_server:600:CLIENT_RPROXY(client11): i=32 d_nr_nodes=32 new_ep=22 <<<<<<<<<<<<
 662:lb_cltpxy.c:select_server:607:CLIENT_RPROXY(client11): lb_nr_init=2 lb_nr_svrpxy=1
ERROR: lb_cltpxy.c:select_server:615: rcode=-28
 662:lb_cltpxy.c:select_server:617:MTX_UNLOCK lb_ptr->lb_mtx 
 662:lb_cltpxy.c:clt_Rproxy_error:231:CLIENT_RPROXY(client11): Replying -11
 662:lb_cltpxy.c:clt_Rproxy_2server:428:MTX_UNLOCK sess_table[dcid].st_mutex 
ERROR: lb_cltpxy.c:clt_Rproxy_loop:199: rcode=-11

EL PROBLEMA ESTA AQUI:
	Busca un endpoint libre pero si no tiene  
			// ALLOCATE FREE SERVER ENDPOINT
			for ( new_ep = svc_ptr->svc_minep; 
				  new_ep <= svc_ptr->svc_maxep; new_ep++){
				if( TEST_BIT(svr_ptr->svr_bm_svc, new_ep-svc_ptr->svc_minep) == 0) {
					SET_BIT(svr_ptr->svr_bm_svc, new_ep-svc_ptr->svc_minep);
		
 668:lb_cltpxy.c:select_server:513:CLIENT_RPROXY(client11): svr_bm_svc=1 bit=0
				EFECTIVAMENT EL TEST_BIT(svr_ptr->svr_bm_svc, new_ep-svc_ptr->svc_minep) da FALSO 
				Por lo que va a buscar otro endpoint!! 
				EL PUNTO ES QUE ESE 


LISTO:		SI LOS svc son PROG_BIND entonces hay que marcar en el bitmap de los servers 
			que esos servicios estan habilitados 
			Se hizo en el monitor al momento del JOIN del agente 
			
==========================================================================================
20220216:
			3 transferencias simultaneas 
			cd /usr/src/dvs/dvs-apps/m3ftp
			./m3ftp -g 0 60 10 file-100M.dat file0-100M.dat > gftp0.out &
			./m3ftp -g 0 61 10 file-100M.dat file1-100M.dat > gftp1.out &
			./m3ftp -g 0 62 10 file-100M.dat file2-100M.dat > gftp2.out &
	
			EN LB 
			# service started by hand !!
			service mm3ftpd10 {
				dcid	0;
				ext_ep	10;
				min_ep	11;
				max_ep	29;	
				bind	external;
				bind	replica;
			};

			EN SERVER 
			nsenter -p -t$DC0 ./mm3ftpd 0 10 11 29 > mm3ftpd0.out 2> mm3ftpd0.err &

			DURANTE LAS TRANSFERENCIAS EN SERVER 
			root@node1:/usr/src/dvs/dvs-apps/m3ftp# cat /proc/dvs/DC0/stats
			DC pnr -endp -lpid/vpid- nd --lsnt-- --rsnt-- -lcopy-- -rcopy-- name
			 0  10    10  1268/12     1        0        7        0        0 mm3ftpd  << MAIN        
			 0  11    11  1286/17     1        0       20        0       20 mm3ftpd  << THREAD 1   
			 0  12    12  1287/18     1        0       20        0       20 mm3ftpd  << THREAD 2      
			 0  13    13  1288/19     1        0       19        0       19 mm3ftpd  << THREAD 3
 
 LISTO:		FUNCIONO EL ARRANQUE AUTOMATICO DE SERVERS POR PARTE DEL LB 
			# service started by hand !!
			service m3ftpd10 {
				dcid	0;
				ext_ep	10;
				min_ep	10;
				max_ep  29;
				bind	replica;
				prog	"/usr/src/dvs/dvs-apps/dvs_lb/run_m3ftpd.sh";
			};
			
			EN CLIENTE
			cd /usr/src/dvs/dvs-apps/m3ftp
			./m3ftp -g 0 60 10 file-100M.dat file0-100M.dat > gftp0.out &
			./m3ftp -g 0 61 10 file-100M.dat file1-100M.dat > gftp1.out &
			./m3ftp -g 0 62 10 file-100M.dat file2-100M.dat > gftp2.out &
			
LISTO: PROBAR MMFTP3 
		LOS CLIENTES SIEMPRE VAN AL MISMO ENDPOINT EXTEP
		LUEGO Q RECIBEN LA RESPUESTA AL 1ER MENSAJE 
		ENVIAN SUS MENSAJES AL ENDPOINT QUE SE LE INDICO
		
==========================================================================================
20220217:
		
LISTO:		 MODIFICAR PARA QUE REFERENCIE A NR_PROCS, NR_TASKS, NR_SYS_PROCS, NR_DCS
	



		
	EN EL LB SE ENVIAN MT_LOAD_THRESHOLDS en el mensaje NONE 
	688:lb_dvs.c:send_load_threadholds:653:Server node1: source=0 type=4 m1i1=30 m1i2=70 m1i3=30 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
	 688:lb_dvs.c:send_hello_msg:667:Server node1: send a HELLO message to server
	 688:lb_dvs.c:send_hello_msg:684:Server node1: cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=685
	 688:lb_dvs.c:send_hello_msg:685:Server node1: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1645127517
	 688:lb_svrpxy.c:svr_Sproxy_serving:752:SERVER_SPROXY(node1): Reading message queue..
	 688:lb_svrpxy.c:svr_Sproxy_mqrcv:905:SERVER_SPROXY(node1): reading from mqid=0
	 688:lb_svrpxy.c:svr_Sproxy_mqrcv:909:SERVER_SPROXY(node1): 120 bytes received
	 688:lb_svrpxy.c:svr_Sproxy_serving:759:SERVER_SPROXY(node1): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=685
	 688:lb_svrpxy.c:svr_Sproxy_serving:761:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1645127517
	 688:lb_svrpxy.c:svr_Sproxy_send:784:SERVER_SPROXY(node1): 
	 688:lb_svrpxy.c:svr_Sproxy_sndhdr:825:SERVER_SPROXY(node1): send header=120 
	 688:lb_svrpxy.c:svr_Sproxy_sndhdr:829:SERVER_SPROXY(node1): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=685
	 688:lb_svrpxy.c:svr_Sproxy_sndhdr:831:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1645127517
	 688:lb_svrpxy.c:svr_Sproxy_sndhdr:849:SERVER_SPROXY(node1): sent header=120 
 
 
	EL MULTIPROXY LO RECIBE 
	 multi_proxy.c:pr_start_serving:526:RPROXY(0): Remote sender [192.168.0.100] connected on sd [5]. Getting remote command.
	 multi_proxy.c:pr_process_message:254:RPROXY(0): About to receive header
	 multi_proxy.c:pr_receive_header:220:RPROXY(0): socket=5
	 multi_proxy.c:pr_receive_header:226:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
	 multi_proxy.c:pr_receive_header:229:RPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=685
	 multi_proxy.c:pr_receive_header:231:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:pr_process_message:268:RPROXY(0): 585 NONE
	 multi_proxy.c:pr_process_message:270:RPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=685
	 multi_proxy.c:pr_process_message:273:RPROXY(0): source=0 type=4 m1i1=30 m1i2=70 m1i3=30 m1p1=(nil) m1p2=(nil) m1p3=(nil) 

==========================================================================================
20220218:
		Como forzar el envio de HELLO por parte del multiproxy cuando el LB le pide algo?
		
		El sender del multiproxy esta en dvk_get2rmt() 
		habria q probar enviandole una signal
		y ver que reporta, y en funcion de eso deberia enviar un paquete prearmado
		por el receiver proxy
		
		1- RPROXY: recibe un HELLO con un comando 
		2- RPROXY: Ejecuta el comando y el resultado lo arma un comando de respuesta
		3- RPROXY: Envia un signal al SPROXY 
		4- SPROXY: En funcion de la salida del dvk_get2rmt, envia la respuesta como HELLO al LB 
		5- SPROXY: Si es TIMEOUT entonces envia un HELLO con MT_LOAD_LEVEL
	
		SERVER 
			RECIBE EL MENSAJE MT_LOAD_THRESHOLDS m1i1=30 m1i2=70 m1i3=30
		multi_proxy.c:pr_process_message:263:RPROXY(0): About to receive header
		 multi_proxy.c:pr_receive_header:229:RPROXY(0): socket=5
		 multi_proxy.c:pr_receive_header:235:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
		 multi_proxy.c:pr_receive_header:238:RPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=648
		 multi_proxy.c:pr_receive_header:240:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
		 multi_proxy.c:pr_process_message:277:RPROXY(0): 576 NONE
		 multi_proxy.c:pr_process_message:279:RPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=648
		 multi_proxy.c:pr_process_message:282:RPROXY(0): source=0 type=4 m1i1=30 m1i2=70 m1i3=30 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		 multi_proxy.c:pr_process_message:284:RPROXY(0): MT_LOAD_THRESHOLDS
			ENVIA EL MENSAJE  MT_LOAD_LEVEL m1i1=0 m1i2=0 m1i3=30
		 multi_proxy.c:pr_process_message:285:MTX_LOCK mpa_ptr->mpa_mutex 
		 multi_proxy.c:pr_process_message:290:MTX_UNLOCK mpa_ptr->mpa_mutex 
		 multi_proxy.c:pr_process_message:291:MTX_LOCK px_ptr->px_sdesc.td_mtx 
		 multi_proxy.c:build_load_level:1743:SPROXY(0):
		 multi_proxy.c:build_load_level:1754:SPROXY(0): source=1 type=5 m1i1=0 m1i2=0 m1i3=30 m1p1=0x2 m1p2=0x1 m1p3=(nil) 
		 multi_proxy.c:pr_process_message:294:MTX_UNLOCK px_ptr->px_sdesc.td_mtx 
		 multi_proxy.c:pr_process_message:263:RPROXY(0): About to receive header
		 multi_proxy.c:pr_receive_header:229:RPROXY(0): socket=5
 
SE CUELGA MULTIPROXY PORQUE AL HACER EL SIGNAL MUERE !!

OTRA POSIBILIDAD
SENDER 
	UNLOCK 
		DVK_GET2RMT
	LOCK 
	
RECEIVER
	LOCK 
		SEND TCP 
	UNLOCK 

ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR 
ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR 
ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR 

[   72.671868] multi_proxy[578]: segfault at 3c ip 0040b219 sp b6c76250 error 4 in multi_proxy[404000+30000]

 
  // SPROXY ENVIA UN HELLO NORMAL 
 multi_proxy.c:ps_start_serving:956:MTX_LOCK px_ptr->px_sdesc.td_mtx 
 multi_proxy.c:ps_start_serving:966:SPROXY(0): Sending HELLO 
 multi_proxy.c:build_load_level:1755:SPROXY(0):
 multi_proxy.c:build_load_level:1766:SPROXY(0): source=1 type=5 m1i1=1 m1i2=0 m1i3=30 m1p1=0x2 m1p2=0x1 m1p3=(nil) 
 multi_proxy.c:send_hello_msg:1781:SPROXY(0): send a HELLO message
 multi_proxy.c:send_hello_msg:1797:SPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=1 dnode=0 rcode=0 len=0 PID=578
 multi_proxy.c:send_hello_msg:1798:SPROXY(0): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 multi_proxy.c:ps_send_remote:881:SPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=1 dnode=0 rcode=0 len=0 PID=578
 multi_proxy.c:ps_send_remote:882:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 multi_proxy.c:ps_send_header:757:SPROXY(0): send bytesleft=120 px_sproxy_sd=6
 multi_proxy.c:ps_send_header:760:SPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=1 dnode=0 rcode=0 len=0 PID=578
 multi_proxy.c:ps_send_header:761:SPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 multi_proxy.c:ps_send_header:782:SPROXY(0): socket=6 sent header=120 
 multi_proxy.c:ps_start_serving:953:SPROXY(0): Waiting a message
 multi_proxy.c:ps_start_serving:954:MTX_UNLOCK px_ptr->px_sdesc.td_mtx
 
 // ESTO LO HACE MAIN LOOP 
 multi_proxy.c:get_metrics:1640:mpa_lowwater=30 mpa_highwater=70 mpa_period=30
 multi_proxy.c:get_metrics:1643:Check initialization... 
 multi_proxy.c:get_metrics:1644:MTX_LOCK mpa_ptr->mpa_mutex 
 multi_proxy.c:get_metrics:1654:MTX_UNLOCK mpa_ptr->mpa_mutex 
 multi_proxy.c:get_CPU_usage:1685:
 multi_proxy.c:get_CPU_usage:1707:cpu  129 0 439 6083 19 0 3 0
 multi_proxy.c:get_CPU_usage:1725:didl=2987 Div=2993
 multi_proxy.c:get_CPU_usage:1733:cpu_usage=1 cpu_idle=99
 multi_proxy.c:get_metrics:1660:MTX_LOCK mpa_ptr->mpa_mutex 
 multi_proxy.c:get_metrics:1670:mpa_load_lvl=0 mpa_cpu_usage=1
 multi_proxy.c:get_metrics:1672:MTX_UNLOCK mpa_ptr->mpa_mutex 
 
  // SPROXY ENVIA UN HELLO NORMAL 
 multi_proxy.c:ps_start_serving:956:MTX_LOCK px_ptr->px_sdesc.td_mtx 
 multi_proxy.c:ps_start_serving:966:SPROXY(0): Sending HELLO 

==========================================================================================
20220219:

 multi_proxy.c:pr_receive_header:235:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
 multi_proxy.c:pr_receive_header:238:RPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=651
 multi_proxy.c:pr_receive_header:240:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 multi_proxy.c:pr_process_message:277:RPROXY(0): 588 NONE
 multi_proxy.c:pr_process_message:279:RPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=651
 multi_proxy.c:pr_process_message:281:RPROXY(0): m_type=7
 multi_proxy.c:pr_process_message:303:RPROXY(0): MT_RMTBIND

root       583   335  0 19:55 ?        00:00:00 /usr/src/dvs/dvk-proxies/multi_proxy /dev/shm/multi_proxy
root       591   477  0 19:55 ?        00:00:00 -bash
root       597     1  0 19:55 ?        00:00:00 /usr/bin/perl /usr/share/webmin/miniserv.pl /etc/webmin/m
root       625   335  0 19:55 ?        00:00:00 cat /proc/dvs/proxies/procs
root       633   474  0 19:57 ?        00:00:00 sshd: root@pts/0
root       639   633  0 19:57 pts/0    00:00:00 -bash
root       645   639  0 19:57 pts/0    00:00:00 ps -ef
root@node1:~# dmesg | grep fault
[   12.016754] multi_proxy[588]: segfault at 0 ip b75bdbc4 sp b754ec20 error 6 in libc-2.24.so[b7552000+1b1000]
 		

==========================================================================================
20220219:
	LB:
	 668:lb_dvs.c:send_rmtbind:680:Server node1: source=0 type=7 m3i1=0 m3i2=1 m3p1=(nil) m3ca1=[FS]
	668:lb_dvs.c:send_hello_msg:714:Server node1: send a HELLO message to server
	668:lb_dvs.c:send_hello_msg:731:Server node1: cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=665
	668:lb_dvs.c:send_hello_msg:732:Server node1: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1645398779

	SERVER1:
	multi_proxy.c:pr_process_message:263:RPROXY(0): About to receive header
	 multi_proxy.c:pr_receive_header:229:RPROXY(0): socket=5
	 multi_proxy.c:pr_receive_header:235:RPROXY(0): n:120 | received:120 | HEADER_SIZE:120
	 multi_proxy.c:pr_receive_header:238:RPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=665
	 multi_proxy.c:pr_receive_header:240:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:pr_process_message:268:RPROXY(0):cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=665
	 multi_proxy.c:pr_process_message:270:RPROXY(0):c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
	 multi_proxy.c:pr_process_message:272:RPROXY(0): 585 c_timestamp=1645398779.807741910
	 multi_proxy.c:pr_process_message:274:RPROXY(0): 585 c_flags=0x0 c_src_pid=0 c_dst_pid=0
	 multi_proxy.c:pr_process_message:277:RPROXY(0): 585 NONE
	 multi_proxy.c:pr_process_message:279:RPROXY(0): m_type=7
	 multi_proxy.c:pr_process_message:301:RPROXY(0): MT_RMTBIND
	 multi_proxy.c:pr_process_message:308:RPROXY(0): dc_dcid=0 dc_nr_procs=221 dc_nr_tasks=34 dc_nr_sysprocs=64 dc_nr_nodes=32
	 multi_proxy.c:pr_process_message:309:RPROXY(0): flags=0 dc_nodes=3 dc_pid=581 dc_name=DC0
	 multi_proxy.c:pr_process_message:313:RPROXY(0):source=0 type=7 m3i1=0 m3i2=1 m3p1=(nil) m3ca1=[FS]
	 multi_proxy.c:pr_process_message:315:RPROXY(0): MT_RMTBIND rcode=1
	 multi_proxy.c:pr_process_message:319:MTX_LOCK px_ptr->px_send_mtx 
	 multi_proxy.c:build_reply_msg:1812:SPROXY(0): mtype=1007
	 multi_proxy.c:build_reply_msg:1821:SPROXY(0): source=1 type=4103 m1i1=1 m1i2=0 m1i3=30 m1p1=0x2 m1p2=0x1 m1p3=(nil) 
	 multi_proxy.c:send_hello_msg:1858:SPROXY(0): send a HELLO message. m_type=4103
	 multi_proxy.c:send_hello_msg:1864:SPROXY(0):source=1 type=4103 m3i1=1 m3i2=0 m3p1=0x1e m3ca1=[]
	 multi_proxy.c:send_hello_msg:1885:SPROXY(0): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=1 dnode=0 rcode=0 len=0 PID=586
	 multi_proxy.c:send_hello_msg:1886:SPROXY(0): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0
 
	LB:
	 669:lb_svrpxy.c:svr_Rproxy_rcvhdr:506:SERVER_RPROXY (node1): n:120 | received:120 | HEADER_SIZE:120
	 669:lb_svrpxy.c:svr_Rproxy_rcvhdr:509:SERVER_RPROXY (node1): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=1 dnode=0 rcode=0 len=0 PID=586
	 669:lb_svrpxy.c:svr_Rproxy_rcvhdr:511:SERVER_RPROXY (node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1645398807
	 669:lb_svrpxy.c:svr_Rproxy_getcmd:446:SERVER_RPROXY(node1): NONE
	 669:lb_svrpxy.c:svr_Rproxy_getcmd:449:SERVER_RPROXY(node1):source=1 type=4103 m1i1=1 m1i2=0 m1i3=30 m1p1=0x2 m1p2=0x1 m1p3=(nil) 
	 669:lb_svrpxy.c:svr_Rproxy_getcmd:429:SERVER_RPROXY(node1): About to receive header
 
==========================================================================================
20220225:
	HAY ALGUN PROBLEMA AL HACER LOS recv_packet_FD  
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "recvfrom" lb_dvs.out
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=84
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=84
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=84
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "oper" lb_dvs.out
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=1 oper=0
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=2 oper=0
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=3 oper=0
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=4 oper=0
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "lb_bm_suspect" lb_dvs.out
 1784:lb_fd.c:recv_packet_FD:180:Server node1(1): INPUT lb_bm_init=0 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:246:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:180:Server node1(1): INPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:246:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:180:Server node1(1): INPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:246:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:180:Server node1(1): INPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:246:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:180:Server node1(1): INPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1784:lb_fd.c:recv_packet_FD:246:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
AHORA LAS SEQUENCIAS SE LEEN CORRECTAMENTE 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "recv_packet_FD:197:Server node1:" lb_dvs.out
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=1 svr_icmp_seq=1
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=2 svr_icmp_seq=2
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=3 svr_icmp_seq=3
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=4 svr_icmp_seq=4
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=5 svr_icmp_seq=5
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=6 svr_icmp_seq=6
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=7 svr_icmp_seq=7
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=8 svr_icmp_seq=8
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=9 svr_icmp_seq=9
 1784:lb_fd.c:recv_packet_FD:197:Server node1: seq=10 svr_icmp_seq=10

LUEGO BAJO NODE1 DESCONECTANDO SU ETH0 

 1784:lb_fd.c:unpack:106:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1779
ERROR: lb_fd.c:unpack:118: rcode=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:unpack:93:Server node1
 1784:lb_fd.c:unpack:106:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1779
ERROR: lb_fd.c:unpack:118: rcode=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:unpack:93:Server node1
 1784:lb_fd.c:unpack:106:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1779
ERROR: lb_fd.c:unpack:118: rcode=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 1784:lb_fd.c:recv_packet_FD:188:Server node1: recvfrom n=88
 1784:lb_fd.c:unpack:93:Server node1
 1784:lb_fd.c:unpack:106:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1779
ERROR: lb_fd.c:unpack:118: rcode=-2
 1784:lb_fd.c:recv_packet_FD:192:Server node1: seq=-2 oper=-2
 

==========================================================================================
20220225: 
LISTO:		UNA POSIBILIDAD ES HACER UN THREAD EMISOR DE PING Y OTRO RECEPTOR 
			sincronizados con VARIABLES DE CONDICION 

ESTA FUNCIONANDO RELATIVAMENTE BIEN PERO NO ES CONFIABLE 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT lb_bm_init" lb_dvs.out
NODO1 CAIDO => SOSPECHOSO 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=0 lb_bm_suspect=2 lb_bm_suspect2=0 
NODO1 CONECTADO 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=0 lb_bm_suspect2=0 
NODO1 CAIDO => SOSPECHOSO 
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=2 lb_bm_suspect2=0 
NODO1 CAIDO => SOSPECHOSO SEGUNDA VEZ  
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=2 lb_bm_suspect=2 lb_bm_suspect2=2
NODO1 CAIDO => PASA A NO CONECTADO  
 1337:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_init=0 lb_bm_suspect=2 lb_bm_suspect2=2 


 1337:lb_fd.c:unpack:117:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1331				ICMP_DEST_UNREACH ICMP_HOST_UNREACH
 1337:lb_fd.c:unpack:117:Server node1: len=64 icmp_type=0 icmp_code=0 icmp_id=1331 lb_pid=1331			OK
 1337:lb_fd.c:unpack:117:Server node1: len=16 icmp_type=9 icmp_code=0 icmp_id=513 lb_pid=1331			ICMP_ROUTERADVERT  ICMP_UNREACH_NET 
 1337:lb_fd.c:unpack:117:Server node1: len=64 icmp_type=0 icmp_code=0 icmp_id=1331 lb_pid=1331			OK 
 1337:lb_fd.c:unpack:117:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=1331				ICMP_DEST_UNREACH ICMP_HOST_UNREACH
 1337:lb_fd.c:unpack:117:Server node1: len=64 icmp_type=0 icmp_code=0 icmp_id=1331 lb_pid=1331			OK 


 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=2 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=0 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=2 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=2 lb_bm_suspect2=0 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=2 lb_bm_suspect=2 lb_bm_suspect2=2 
 1429:lb_fd.c:lb_fd_receiver:287:Server node1(1): INPUT lb_bm_nodes=2 lb_bm_suspect=2 lb_bm_suspect2=2 
 1429:lb_fd.c:lb_fd_receiver:353:Server node1(1): OUTPUT lb_bm_nodes=0 lb_bm_suspect=2 lb_bm_suspect2=2 

LISTO: 	SI HAY PAQUETES QUE LLEGAN AL PROXY ENTONCES 
			AUTOMATICAMENTE EL NODO ESTA ACTIVO lb_bm_active
			y el PROXY esta activo lb_bm_init 

==========================================================================================
20220227:
LISTO : MODIFICAR EL FD PARA QUE SE ESTABLEZCA EL PERIODO DE MONITOREO DE UN NODO 
		Y QUE EL NODO SE MONITOREE SI:
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=0 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=1 remainder=1
 1403:lb_fd.c:lb_fd_sender:250:CHECK n=1 lb_period=2 i=1 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=2 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=3 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=4 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=5 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=6 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=7 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=8 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=9 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=10 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=11 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=12 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=13 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=14 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=15 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=16 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=17 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=18 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=19 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=20 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=21 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=22 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=23 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=24 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=25 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=26 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=27 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=28 remainder=1
 1403:lb_fd.c:lb_fd_sender:246:n=1 lb_period=2 i=29 remainder=1




root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "tv_sec" lb_dvs.out 
 2201:lb_fd.c:lb_echo_request:228:Server node1 (1): tv_sec=1646014279
 2202:lb_fd.c:lb_echo_reply:305:Server node1(1): INPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646014279
 2202:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646014279
 2197:lb_dvs.c:send_hello_msg:753:Server node1: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2197:lb_dvs.c:send_hello_msg:753:Server node1: c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2197:lb_svrpxy.c:svr_Sproxy_serving:811:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2197:lb_svrpxy.c:svr_Sproxy_sndhdr:881:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2197:lb_svrpxy.c:svr_Sproxy_serving:811:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2197:lb_svrpxy.c:svr_Sproxy_sndhdr:881:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1646014278
 2201:lb_fd.c:lb_echo_request:228:Server node1 (1): tv_sec=1646014310
 2202:lb_fd.c:lb_echo_reply:305:Server node1(1): INPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646014310
 2202:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646014310
 2201:lb_fd.c:lb_echo_request:228:Server node1 (1): tv_sec=1646014340
 
 hay 96 SEGUNDOS DE DIFERENCIA !!!
 2202:lb_fd.c:lb_echo_reply:305:Server node1(1): INPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646014340   <<<< 340
 2202:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646014436  <<<< 436 
 

 hay 126 SEGUNDOS DE DIFERENCIA !!!
 2201:lb_fd.c:lb_echo_request:228:Server node1 (1): tv_sec=1646014437
 2202:lb_fd.c:lb_echo_reply:305:Server node1(1): INPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646014437    <<<< 437
 2202:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646014563    <<< 563
 
 hay 127 SEGUNDOS DE DIFERENCIA !!!
 2201:lb_fd.c:lb_echo_request:228:Server node1 (1): tv_sec=1646014564
 2202:lb_fd.c:lb_echo_reply:305:Server node1(1): INPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646014564	<<<< 564	
 2202:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646014691	<<<<<681 


==========================================================================================
20220228:
		root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
CON NODE1 CONECTADO LO DETECTA BIEN
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077177
 
 AQUI HAY ALGUNOS ECHO_REPLY QUE NO LLEGAN Y LO MANTIENEN COMO SOSPECHOSO TESTEANDO CADA 1 SEGUNDO HASTA TENERLO CONECTADO OTRA VEZ 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077207
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077208
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077209
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077210
 
 AQUI HAY ALGUNOS ECHO_REPLY QUE NO LLEGAN Y LO MANTIENEN COMO SOSPECHOSO TESTEANDO CADA 1 SEGUNDO HASTA TENERLO CONECTADO OTRA VEZ 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077237
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077238
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077239
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077240
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077241
 
 AQUI HAY ALGUNOS ECHO_REPLY QUE NO LLEGAN Y LO MANTIENEN COMO SOSPECHOSO TESTEANDO CADA 1 SEGUNDO HASTA TENERLO CONECTADO OTRA VEZ 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077267
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077268
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077269
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077270
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077271
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077272
 
 AQUI HAY ALGUNOS ECHO_REPLY QUE NO LLEGAN Y LO MANTIENEN COMO SOSPECHOSO TESTEANDO CADA 1 SEGUNDO HASTA TENERLO CONECTADO OTRA VEZ 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077297
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077298
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077299
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077300
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077301
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077302
 
 AQUI SE DESCONECTA NODE1
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077327
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077328
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077329
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077330
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077331
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077333
 PASA A SUSPECTED2 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646077334
 LO MARCA COMO DESCONECTADO 
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646077335
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646077358
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646077388
 
CUANDO SE CONECTA NODE1 NUEVAMENTE, TODO VUELVE A LA NORMALIDAD  
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077418
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646077449
 2096:lb_fd.c:lb_echo_reply:374:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646077450

------------------------------------------------------------------------------------------------------------------------
FUNCIONA BIEN PERO ES INESTABLE 
	CON NODE1 CONECTADO EN VARIAS OPORTUNIDADES SE PONE EN SUSPECTED Y UNA VEZ EN SUSPECTED2 !!! 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078840
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078870
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078871
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078872
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078873
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078901
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078902
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078903
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078904
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078905
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078931
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078932
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078933
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078934
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078935
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078936
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078961
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078962
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078963
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078964
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078965
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078966
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078991
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078992
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078993
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078994
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078995
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646078996
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646078997 <<<<<<<< SUSPECTED2 
 2227:lb_fd.c:lb_echo_reply:378:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646078998

----------------------------------------------------------------------------------------------------------------------------
CON NODE1 CONECTADO 

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "send_packet_FD" lb_dvs.out | grep  svr_icmp_seq
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=1
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=2
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=3
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=4
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=5
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=6
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=7
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=8
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=9
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=10
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=11
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=12
 2499:lb_fd.c:send_packet_FD:179:Server node1: packetsize=64 svr_dstaddr.sin_addr.s_addr=6500A8C0 svr_icmp_seq=13
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "recv_packet_FD" lb_dvs.out | grep  svr_icmp_rcvd
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=1 svr_icmp_rcvd+1=1
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=2 svr_icmp_rcvd+1=2
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=3 svr_icmp_rcvd+1=3
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=4 svr_icmp_rcvd+1=4
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=5 svr_icmp_rcvd+1=5
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=6 svr_icmp_rcvd+1=6
 2500:lb_fd.c:recv_packet_FD:206:Server node1: seq=7 svr_icmp_rcvd+1=7
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082336
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082366
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082367
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082368
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082369
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082396
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082426
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082456
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082486
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082487
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082517
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082518
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082547
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082577
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646082578
 2500:lb_fd.c:lb_echo_reply:381:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646082579


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
DESDE QUE SE DESCONECTA PASA DEMASIADO TIEMPO 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646083767
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083797
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083798
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083799
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646083800
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083828
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646083829
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083858
 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646083859 <<< NODE1 DISCONNECT 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083908 <<< SUSPECTED 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083915
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083924
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083927
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083930
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646083933
 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646083940 <<<< SUSPECTED2 
 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646083973 <<<< DISCONNECTED 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646083998

EL PROBLEMA DEL TIEMPO ESTA EN EL RECVFROM 
 2700:lb_fd.c:lb_echo_reply:324:Server node1(1): INPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646083941   <<< INGRESA 41
 2700:lb_fd.c:lb_echo_reply:326:MTX_LOCK svr_ptr->svr_mutex 
 2694:lb_dvs.c:main:283:MTX_LOCK lb_ptr->lb_mtx 
 2700:lb_fd.c:recv_packet_FD:200:Server node1: recvfrom n=88 tv_sec=1646083973											<<<<<< SALE 73
 2700:lb_fd.c:unpack:103:Server node1
 2700:lb_fd.c:unpack:116:Server node1: len=68 icmp_type=3 icmp_code=1 icmp_id=0 lb_pid=2694
ERROR: lb_fd.c:unpack:132: rcode=-515
 2700:lb_fd.c:recv_packet_FD:202:Server node1: seq=-515 
ERROR: lb_fd.c:recv_packet_FD:214: rcode=-515
 2700:lb_fd.c:lb_echo_reply:350:Server node1(1): FAULTY 
 2700:lb_fd.c:lb_echo_reply:374:MTX_UNLOCK svr_ptr->svr_mutex 
 2700:lb_fd.c:lb_echo_reply:380:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646083973

PARA NO ESPERAR 
fnctl
If no messages are available at the socket, the receive calls wait for a message to arrive, 
unless the socket is nonblocking (see fcntl(2)), 
in which case the value -1 is returned and the external variable errno is set to EAGAIN or EWOULDBLOCK. 
The receive calls normally return any data available, up to the requested amount, 
rather than waiting for receipt of the full amount requested.

setsockopt 
SO_KEEPALIVE
Enable sending of keep-alive messages on connection-oriented sockets. Expects an integer boolean flag.

SO_RCVTIMEO and SO_SNDTIMEO
Specify the receiving or sending timeouts until reporting an error. The argument is a struct timeval. If an input or output function blocks for this period of time, and data has been sent or received, the return value of that function will be the amount of data transferred; if no data has been transferred and the timeout has been reached then -1 is returned with errno set to EAGAIN or EWOULDBLOCK, or EINPROGRESS (for connect(2)) just as if the socket was specified to be nonblocking. If the timeout is set to zero (the default) then the operation will never timeout. Timeouts only have effect for system calls that perform socket I/O (e.g., read(2), recvmsg(2), send(2), sendmsg(2)); timeouts have no effect for select(2), poll(2), epoll_wait(2), and so on

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
NODE1 DESCONECTADO 
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646091869
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646091899
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646091929
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646091959
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646091990

NODE1 CONECTADO 
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092020
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092050
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092080
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092081
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092082
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092083
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092084
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092085
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646092086 <<< SUSPECTED2 ZAFO
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092087
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092110
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092140
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092170
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092171
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092172
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646092173
 
 NODE1 DESCONECTADO 
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092200
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092202
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092204
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092206
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092208
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646092210
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646092212
 2782:lb_fd.c:lb_echo_reply:386:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646092214 <<< DETECTADA DESCONECCION
 
 SI APARECE 515 ENTONCES NO DEBERIA DESCONTAR 
 
 =================================================================================
 1646697756 desconecte
 1646697811 conecte 
 
 root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
 NODE1 CONECTADO 
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646697723
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697753
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697754
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697755
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646697756
 NODE1 DESCONECTADO 
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697783
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697784
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697786
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697788
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697790
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697792
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697793
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697795 << DETECTO DESCONEXION (13)
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697797
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697799
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697801
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697803
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697805
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697807
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697809
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697811
 NODE1 CONECTADO 
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697813
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697815
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697817
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697819
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697821
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697823
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697825
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697827
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697829
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697831
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697833
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697835
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697837
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=2 lb_bm_suspect2=2 tv_sec=1646697840
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646697841 << DETECTO CONEXION (32)
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 lb_bm_suspect2=0 tv_sec=1646697871
 1117:lb_fd.c:lb_echo_reply:327:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 lb_bm_suspect2=0 tv_sec=1646697872
 
 LUEGO MODIFIQUE ALGO Y DEJO DE FUNCIONAR 

=================================================================================
20220308:

1646850522 desconexion
1646850610 conexion 

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
ARRANCA CON NODE1 CONECTADO 
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850369
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850399
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850400
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=2 tv_sec=1646850401
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850402
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850429
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850430
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850459
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850460
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850489
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850520
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850521
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850522
 NODE1 DESCONECTADO 
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850550
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850551
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=2 tv_sec=1646850552
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=1 tv_sec=1646850553
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=0 tv_sec=1646850554
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850555 <<< DETECTA DESCONEXION (6)
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850580
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850610
 NODE1 CONECTADO 
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850640 <<< DETECTA CONEXION (30)
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850670
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850671
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=2 tv_sec=1646850672
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850673
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850700
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850701
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850730
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850731
 1174:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850732

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# grep "OUTPUT" lb_dvs.out 
ARRANCA CON NODE1 DESCONECTADO 
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850817
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850847
 NODE1 CONECTADO 
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850878 << DETECTA CONEXION (31)
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850908
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850909
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850938
 NODE1 DESCONECTADO 
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1646850968
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1646850969
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=2 tv_sec=1646850970
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=1 tv_sec=1646850971
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=0 tv_sec=1646850972
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850973 << DETECTA DESCONEXION (35)
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646850998
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646851028
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646851058
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646851088
 NODE1 CONECTADO 
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646851119
 1201:lb_fd.c:lb_echo_reply:346:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1646851149 << DETECTA CONEXION (30)


LISTO: ATENCION, CUANDO LLEGA UN MENSAJE CORRECTO AL PROXY RECEIVER INDICA QUE EL NODE ESTA CORRECTO 
		POR LO QUE SE PUEDE EVITAR QUE SE TESTEE NUEVAMENTE POR EL FAILURE DETECTOR 
	NO PROBADO !!!!

	
==========================================================================================
20220311:
	TODO: IMPLEMENTAR LA SINCRONIZACION DE CONEXION Y DESCONEXION DE LOS PROXIES RECEIVER Y SENDER 
			AL IGUAL QUE SE HIZO EN MULTI_PROXY

		MTX_LOCK(svr_ptr->svr_conn_mtx);
		SET_BIT(lb_ptr->lb_bm_rconn, svr_ptr->svr_nodeid);
		COND_SIGNAL(svr_ptr->svr_conn_scond);
		COND_WAIT(svr_ptr->svr_conn_rcond, svr_ptr->svr_conn_mtx);
		MTX_UNLOCK(svr_ptr->svr_conn_mtx);

		if( TEST_BIT(lb_ptr->lb_bm_sconn, svr_ptr->svr_nodeid) == 0){
			USRDEBUG("SERVER_RPROXY(%s): lb_bm_sconn==%0lX lb_bm_rconn==%0lX\n",
           		svr_ptr->svr_name, lb_ptr->lb_bm_sconn, lb_ptr->lb_bm_rconn);
			close(spx_ptr->lbp_csd);
			sleep(1);
			continue;
		}

Active Internet connections (servers and established)  <<<<<<<<<<<<<<<<<<<<<<<<< ESTA FALTANDO PROXY SENDER 
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 192.168.0.100:3001      0.0.0.0:*               LISTEN     
tcp        0      0 192.168.137.100:3011    0.0.0.0:*               LISTEN     
tcp        0      0 192.168.0.100:3001      192.168.0.101:48300     ESTABLISHED <<< PROXY RECEIVER OK 
tcp        0      1 192.168.137.100:39408   192.168.137.111:3000    SYN_SENT   

root@node1:~# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State         
tcp        0      0 192.168.0.101:3000      0.0.0.0:*               LISTEN  <<<<<<<<<<< PROXY RECEIVER LISTEN    
tcp        0      0 192.168.0.101:48300     192.168.0.100:3001      ESTABLISHED <<<<<<< PROXY SENDER OK 
  
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   
TODO: A los recv habria que ponerle TIMEOUT asi vuelven a probar 
LISTO, tanto en multiproxy como en el LB 
NO COMPILADO
NO PROBADO 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	
MSG_DONTWAIT | MSG_NOSIGNAL 

PROBAR CON KEEP_ALIVE 
	
How can I set a custom timeout value for a TCP or UDP socket?
It depends on your system. You might search the net for SO_RCVTIMEO and SO_SNDTIMEO (for use with setsockopt())
to see if your system supports such functionality.	
The Linux man page suggests using alarm() or setitimer() as a substitute.
	
	TODO: MEJORAR LOS PROXIES PARA QUE CUANDO SE CAE LA CONEXION VUELVA A LEVANTAR !!!
		CLOSE-WAIT 
		https://support.hpe.com/hpesc/public/docDisplay?docId=ns10.0.7791273.2540675en_us&docLocale=en_US

EN LB 
ERROR: lb_svrpxy.c:svr_Rproxy_rcvhdr:573: rcode=-11
 661:lb_svrpxy.c:svr_Rproxy_getcmd:466:MTX_LOCK lb_ptr->lb_mtx 
 661:lb_svrpxy.c:svr_Rproxy_getcmd:471:MTX_UNLOCK lb_ptr->lb_mtx 
ERROR: lb_svrpxy.c:svr_Rproxy_getcmd:474: rcode=-11
 661:lb_svrpxy.c:svr_Rproxy_loop:183:SERVER_RPROXY(node1): Message processing failure [-11]
 661:lb_svrpxy.c:svr_Rproxy_getcmd:458:SERVER_RPROXY(node1): 
 661:lb_svrpxy.c:svr_Rproxy_getcmd:463:SERVER_RPROXY(node1): About to receive header
	
https://tldp.org/HOWTO/html_single/TCP-Keepalive-HOWTO/#codeneeding
		
==========================================================================================
20220315:
		se implemento el KEEP_ALIVE en los servers y se detecta la caida de los enlaces
		
		NO CONECTA!!!! 

https://superuser.com/questions/240456/how-to-interpret-the-output-of-netstat-o-netstat-timers
 
==========================================================================================
20220320:
		Se implementó el incremento de retries dinámico en función de la cantidad de intentos fallidos
		de un nodo conectado.
		Si el nodo es sospechoso, y en la ultima oportunidad se lo ve conectado, se incrementan 
		la cantidad maxima de retries para no correr el resto de informar un falso_desconectado.
		para ello se agregó el campo svr_max_retries en los servers ya que este campo es individual 

 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1647816793
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=4 tv_sec=1647816844
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=3 tv_sec=1647816845
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=2 tv_sec=1647816846
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=1 tv_sec=1647816847
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=2 lb_bm_suspect=2 svr_icmp_retry=0 tv_sec=1647816848
 663:lb_fd.c:lb_echo_reply:382:Server node1(1): OUTPUT lb_bm_active=0 lb_bm_suspect=0 svr_icmp_retry=5 tv_sec=1647816849


NO ENTIENDO POR QUE
NODE0 
root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 192.168.0.100:3001      0.0.0.0:*               LISTEN     
tcp        1      0 192.168.0.100:34382     192.168.0.101:3000      CLOSE_WAIT 
NODE1
tcp        0      0 192.168.0.101:3000      0.0.0.0:*               LISTEN     
tcp        1      0 192.168.0.101:48866     192.168.0.100:3001      CLOSE_WAIT 

HUSTON WE HAVE A PROBLEM
El SPROXY queda colgado esperando un mensaje en la MSGQ 
 659:lb_svrpxy.c:svr_Sproxy_serving:866:SERVER_SPROXY(node1): Reading message queue..
 659:lb_svrpxy.c:svr_Sproxy_mqrcv:1020:SERVER_SPROXY(node1): reading from mqid=0   QUEDA BLOQUEADO AQUI 
SOLUCION 
 Enviar un mensaje a la cola que diga que se desconecto o un HELLO 
 pero que antes de enviarlo se fija la condicion de la conexion 
 DEBERIA ENVIAR PERIODICAMENTE  send_load_threadholds
  

==========================================================================================
20220321:

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State    
tcp      240      0 192.168.0.100:3001      192.168.0.101:46830     ESTABLISHED
tcp        0      0 192.168.0.100:47678     192.168.0.101:3000      ESTABLISHED

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State   
tcp      240      0 192.168.0.100:3001      192.168.0.101:46830     ESTABLISHED
tcp        1      0 192.168.0.100:47678     192.168.0.101:3000      CLOSE_WAIT 

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp      240      0 192.168.0.100:3001      192.168.0.101:46830     ESTABLISHED

root@node0:/usr/src/dvs/dvs-apps/dvs_lb# netstat -nat
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp      240      0 192.168.0.100:3001      192.168.0.101:46830     ESTABLISHED


 675:lb_svrpxy.c:svr_Sproxy_mqrcv:1073:SERVER_SPROXY(node1): 120 bytes received
 675:lb_svrpxy.c:svr_Sproxy_serving:914:SERVER_SPROXY(node1): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=672
 675:lb_svrpxy.c:svr_Sproxy_serving:916:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1647893140
 675:lb_svrpxy.c:svr_Sproxy_send:940:SERVER_SPROXY(node1): 
 675:lb_svrpxy.c:svr_Sproxy_sndhdr:984:SERVER_SPROXY(node1): send header=120 
 675:lb_svrpxy.c:svr_Sproxy_sndhdr:988:SERVER_SPROXY(node1): cmd=0x0 dcid=-1 src=27342 dst=27342 snode=0 dnode=1 rcode=0 len=0 PID=672
 675:lb_svrpxy.c:svr_Sproxy_sndhdr:990:SERVER_SPROXY(node1): c_batch_nr=0 c_flags=0x0 c_snd_seq=0 c_ack_seq=0 c_timestamp.tv_sec=1647893140
ERROR: lb_svrpxy.c:svr_Sproxy_sndhdr:996: rcode=-1
ERROR: lb_svrpxy.c:svr_Sproxy_sndhdr:1002: rcode=-32   RETORNO -32 
 675:lb_svrpxy.c:svr_Sproxy_send:945:SERVER_SPROXY(node1): rcode=0  PERO VE 0 !!! 
 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 reemplace el ERROR_RETURN por 
 			}else{
				ERROR_PRINT(rcode);
//				ERROR_RETURN(rcode);
				return(rcode);
			}
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

No utilizar ERROR_XXXXXX(-errno) ya que dentro de la macro se utilizan llamadas al sistema que modifican errno 
Hacer:
		rcode = -errno;
		ERROR_XXXXX(rcode)
		
		
			
 
=========================================================================================
ULTIMO_LOG 

==========================================================================================
==========================================================================================
	

TODO: LOAD BALANCER DISTRIBUIDO 
https://github.com/envoyproxy/envoy
	
TODO: Se podría enviar una SIGNAL al LB para que fuerce el envio de HELLO a TODOS los proxies 

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
ATENCION: no se si cuando le pase info desde el LB a un NODO no tendré que incluir entre el listado
de nodos activos al mismo LB. De todos modos, en general, dentro del LB se evita considerase a si mismo un nodo.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


MULTI_PROXY
	Deberia tener un mpa_px_mtx y un mpa_scond y un mpa_rcond
	Esto permite sincronizar al SPROXY y RPROXY para saber que ambos 
	estan en modo CONNECTED 
	
	Que hacer cuando uno de los proxies SALE de estado de CONNECTED ?
	El otro proxy deberia salir de estado CONNECTED haciendo un close
		SOLUCION: Se deberia hacer con un SIGNAL 
				En el SPROXY antes del GET2RMT 
				En el RPROXY antes de leer del socket - IMPLEMENTAR TIMEOUT !! COMO EN ICMP 
				
	Esto permite detectar el estado del nodo remoto, que para el nodo local seria NOT_CONNECTED 
	Luego, el ping periodico setea el estado de los nodos 
	

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
TODO: HABRIA Q OPTIMIZAR EL FAILURE DETECTOR 
 https://github.com/dgibson/iputils/blob/master/ping.c
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


TODO:
CUANDO SE CAE EL NODE1, EL SERVER PROXY (RECEIVER) NO ES NOTICADO DE ESTO!!
sigue en el recv() sin salir por error 

TODO: MODIFICAR EL FD PARA QUE SE ESTABLEZCA EL PERIODO DE MONITOREO DE UN NODO 
		Y QUE EL NODO SE MONITOREE SI:

NIVELES DE DETECCION DE FALLO:
		ping:		NODO 			lb_bm_nodes
		proxy:		AGENTE 			lb_bm_init	
		getprocsts:	PROCESO/ENDPOINT 
				
 
https://sites.uclouvain.be/SystInfo/usr/include/netinet/ip_icmp.h.html

#define ICMP_ECHOREPLY                0        /* Echo Reply                        */
#define ICMP_DEST_UNREACH        3        /* Destination Unreachable        */
#define ICMP_SOURCE_QUENCH        4        /* Source Quench                */
#define ICMP_REDIRECT                5        /* Redirect (change route)        */
#define ICMP_ECHO                8        /* Echo Request                        */
#define ICMP_TIME_EXCEEDED        11        /* Time Exceeded                */
#define ICMP_PARAMETERPROB        12        /* Parameter Problem                */
#define ICMP_TIMESTAMP                13        /* Timestamp Request                */
#define ICMP_TIMESTAMPREPLY        14        /* Timestamp Reply                */
#define ICMP_INFO_REQUEST        15        /* Information Request                */
#define ICMP_INFO_REPLY                16        /* Information Reply                */
#define ICMP_ADDRESS                17        /* Address Mask Request                */
#define ICMP_ADDRESSREPLY        18        /* Address Mask Reply                */
#define NR_ICMP_TYPES                18


/* Codes for UNREACH. */
#define ICMP_NET_UNREACH        0        /* Network Unreachable                */
#define ICMP_HOST_UNREACH        1        /* Host Unreachable                */
#define ICMP_PROT_UNREACH        2        /* Protocol Unreachable                */
#define ICMP_PORT_UNREACH        3        /* Port Unreachable                */
#define ICMP_FRAG_NEEDED        4        /* Fragmentation Needed/DF set        */
#define ICMP_SR_FAILED                5        /* Source Route failed                */
#define ICMP_NET_UNKNOWN        6
#define ICMP_HOST_UNKNOWN        7
#define ICMP_HOST_ISOLATED        8
#define ICMP_NET_ANO                9
#define ICMP_HOST_ANO                10
#define ICMP_NET_UNR_TOS        11
#define ICMP_HOST_UNR_TOS        12
#define ICMP_PKT_FILTERED        13        /* Packet filtered */
#define ICMP_PREC_VIOLATION        14        /* Precedence violation */
#define ICMP_PREC_CUTOFF        15        /* Precedence cut off */
#define NR_ICMP_UNREACH                15        /* instead of hardcoding immediate value */

/* Codes for REDIRECT. */
#define ICMP_REDIR_NET                0        /* Redirect Net                        */
#define ICMP_REDIR_HOST                1        /* Redirect Host                */
#define ICMP_REDIR_NETTOS        2        /* Redirect Net for TOS                */
#define ICMP_REDIR_HOSTTOS        3        /* Redirect Host for TOS        */

/* Codes for TIME_EXCEEDED. */
#define ICMP_EXC_TTL                0        /* TTL count exceeded                */
#define ICMP_EXC_FRAGTIME        1        /* Fragment Reass time exceeded        */



 root@node0:/usr/src/dvs/dvs-apps/dvs_lb# tcpdump -i eth0 icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
22:44:27.990131 IP node0 > node1: ICMP echo request, id 43786, seq 256, length 64
22:44:27.990366 IP node1 > node0: ICMP echo reply, id 43786, seq 256, length 64

Pablo Pessolani, Marcelo Taborda, Franco Perino
		


		
TODO: LB 
		Cambiar la forma de operar.
		Solo deben exitir los receiver proxies
		El main thread abre los sockets emisor con los distintos nodos 
		Al recibir un mensaje el RPROXY, una vez seleccionado el destino
		hace un write sobre el socket emisor correspondiente al nodo 
			
TODO:		FUNCIONES REMOTAS A IMPLEMENTAR 
			MT_RMTBIND	 		(dcid, endpoint, nodeid, name)
								typedef struct {int m3i1, m3i2; char *m3p1; char m3ca1[M3_STRING];} mess_3;
								m3_i1 = dcid 
								m3_i2 = endpoint 
								m3_p1 = (char *) nodeid
								strncpy(m3_ca1,name, M3_STRING-1) 
			MT_GET_DVSINFO
			MT_GET_DCINFO
			MT_GET_PROCINFO
			MT_RUN_COMMAND
			MT_MIGR_START
			MT_MIGR_COMMIT
			MT_MIGR_ROLLBACK
					
TODO:		Hacer que el main loop no haga un sleep por un tiempo fijo sino de 1 segundo 
			e ir rotando un bitmap tanto de clientes como de servers 
			lb_bm_nodes = lb_bm_servers | lb_bm_clients
			como trabaja un planificador FIJO de tiempo real
			
			while(TRUE){
				for( j = 0; j < lb_ptr->lb_period; j++){
					for( i = 0; i < nr_nodes; i ++){
						if( j%i == 0){ // aplica a todos los nodos multiplos del periodo en curso 
							if( TEST_BIT(lb_bm_nodes, i) != 0){ // Aplica solo a los nodos CONFIGURADOS 
								do_ping(i);	
							}
							if( TEST_BIT(lb_bm_init, i) != 0){	// Aplica solo a los nodos INICIALIZADOS  
								/// AQUI HACER LAS TAREAS PERIODICAS SOBRE EL NODO 
							}					
						}
					}
				}
				sleep(1);
			}
			
			SI HACE FALTA MAS PRECISION, ENTONCES SE TIENE QUE TRABAJAR CON DIFERENCIAS DE TIEMPOS
			Y SI UN NODO NO PUDO SER ATENTIDO EN SU CICLO CORRESPONDIENTE,POR EJEMPLO
			gettime(ts0)
			bm_zero= lb_bm_nodes		// nodos esperando ser atendidos 
			
			//////// SIN TERMINAR !!!!!!!!! 
			
			for( j = 0; j < lb_ptr->lb_period; j++){
				sleep(1)
				gettime(ts)
				diff = ts.sec - ts0.sec		
				// La idea es que si la diff corresponde 5, todos los nodos no atendidos de 0-4 se atienden 
				for( i = 0; i < diff; i++){
					if( j%diff){
						if( TEST_BIT(lb_bm_nodes, i) != 0){ // Aplica solo a los nodos CONFIGURADOS 
						do_ping(j);	
					}
					
					bm_zero &= lb_bm_nodes;
					if( TEST_BIT(bm_zero, i) != 0){	// Aplica solo a los nodos INICIALIZADOS  
						/// AQUI HACER LAS TAREAS PERIODICAS SOBRE EL NODO 
						CLR_BIT(bm_zero, i);
					}					
				}
			}
			
			
			
			
			
			
				
			
			
			
TODO:		
En el mensaje HELLO se podría acarrear info sobre el nodo y comandos  
		- uso de CPU
		- uso de memoria 
También se puede utilizar el estado proxy de cada NODO como failure detector.
De esta forma no se utiliza el agente para colectar la info ni como detector de fallos
Habria que ver la forma en que desde el LB se le mandan ordenes a los proxies 
El multiproxy tiene un thread que captura la info de CPU cada 30 segundos 
y es la que los proxies individuales reportan en los HELLO
PROBLEMA: Que pasa si los proxies estan saturados??? no va a haber HELLO!! 
	SOLUCION: Como insertar HELLO en los proxies 
				Hacer una cuenta de tiempo, si pasaron 30 segundos 
				Enviar HELLO 
VER EN LOS PROXIES DEL LB QUE DIFERENCIAS TIENE EL SOCKET 
	EN TIPO DE ERROR PARA CUANDO
		1- EL EQUIPO REMOTO ESTA APAGADO
			ERROR: lb_cltpxy.c:clt_Sproxy_connect:968: rcode=-110		Connection timed out
			ERROR: lb_svrpxy.c:svr_Sproxy_connect:728: rcode=-110		Connection timed out
		2- EL EQUIPO REMOTO ESTA ENCENDIDO PERO EL SU PROXY NO ARRANCO 
		3- EL EQUIPO REMOTO Y PROXY ESTAN ARRANCADOS
		4- SE CAE EL PROXY
			EL LB NO DETECTO NADA!!!
				EL LB MAIN DEBERIA ENVIAR PERIODICAMENTE PAQUETES HELLO POR CADA PROXY 
				DEBERIA TENER UN TIEMPO DE LAST_HELLO.
				PARA QUE SALTE EL ERROR.
		5- SE CAE EL NODO 
		
TODO:	HACER UNA MACRO CON UN BITMAP QUE INDIQUE CUALES SON LOS SERVIDORES 
		QUE SE TESTEARON DE A CICLOS DE UN SEGUNDO 
		LA IDEA ES QUE CADA SEGUNDO SE TESTEA 1 NODO (O MAS DE UNO)
		Y SINO ESTA ACTIVO SE SLEEP 1 
		SI, EL LB ESTA SOBRECARGADO, PUEDE QUE SE SALTEE EN EL TIEMPO MAS DE UN SEGUNDO
		DEJANDO A SERVIDORES SIN TESTEAR 
		POR ESO EN EL CICLO CORRESPONDIENTE AL PROXIMO A TESTEAR, SE DEBE VERIFICAR 
		QUE NO HAYA NODOS PREVIOS SIN TESTEAR CUYOS CICLOS FUERON SALTEADOS 
				
TODO:	DESCRIBIR LOS DIFERENTES TIPOS DE TRANSFERENCIAS 

		SERVER TYPE 
			type	[P2P,MP-P-MP,MP-P-P2P]
					P2P: Peer to peer 
					MP-P-MP: 	Multipoint Point Multipoint 
					MP-P-P2P: Multipoint Point - Peer to peer  

	TIPO PEER2PEER
		CLIENT0-------------------------->SERVER0							Configurar solo extep		
		CLIENT1-------------------------->SERVER1							min_ep = extep 
		CLIENT2-------------------------->SERVER2							max_ep = extep 
	
	TIPO UNO PARA TODOS FIJO (UN ENDPOINT PARA TODOS LOS CLIENTES)
		LOS CLIENTES SIEMPRE VAN AL MISMO ENDPOINT EXTEP
		PERO EL PROXY LE ASIGNA UN ENDPOINT ENTRE MINEP Y MAXEP 
		EL CLIENTE SIGUE ENVIANDO SUS MENSAJES A EXTEP 
		CLIENT0----------SERVER0---------------->SERVER1					Configurar  min_ep <= ext_ep <= max_ep
		CLIENT1----------SERVER0---------------->SERVER2
		CLIENT2----------SERVER0---------------->SERVER0

	TIPO PRIMERO PARA TODOS LUEGO PEER2PEER (MMFTP3) 
		LOS CLIENTES EN SU PRIMER MENSAJE VAN AL MISMO ENDPOINT EXTEP
		LUEGO Q RECIBEN LA RESPUESTA AL 1ER MENSAJE 
		ENVIAN SUS MENSAJES AL ENDPOINT QUE SE LE INDICO					Configurar ext_ep fuera de rango de [min_ep, max_ep]
	PRIMER MENSAJE 
		CLIENT1----------SERVER0---------------->SERVER0
		CLIENT2----------SERVER0
		CLIENT3----------SERVER0

	SIGUIENTES MENSAJE 
		CLIENT1-------------------------->SERVER1
		CLIENT2-------------------------->SERVER2
		CLIENT3-------------------------->SERVER3


TODO:	Cuando se CAE un SERVER habria que verificar si estan levantada la cantidad minima de servers necesarios
		lb_min_servers
		ver algo parecido si la cantidad de servers es mayor a la maxima 

TODO:	Cuando llegan a un proxy receiver multiples comandos BATCH , estos podrian ir a 
		diferentes endpoints destinos que pueden estar redirigidos a diferentes nodos 
		por lo que hay que desbatchearlos y volverlos a batchear individualmente. 
		POR AHORA NO USAR BATCH 
				
TODO:	DOCUMENTAR TIPO MAN EL LB_DVS Y SU ARCHIVO DE CONFIGURACION
		Porque la verdad no me acuerdo de como hacer ciertas cosas
		
TODO:
	MEJORAR EL SCRIPT DE STARTUP 
	init.d/dvs 
	que invoca a 
	/usr/src/dvs/dvk-tests/test_lb.sh   
		Este script es valido tanto para el LB, CLIENT y SERVERS ya que los distingue por su nombre
	Luego hay que correr 
		sudo update-rc.d dvs defaults
	https://unix.stackexchange.com/questions/20357/how-can-i-make-a-script-in-etc-init-d-start-at-boot
	- terminar todos los DCs
		- Terminar todos los procesos dentro del DC 
			a) leyendo la info del /proc
			b) utilizando un programa que rastree todos los procesos del DC 
	2- hace un kill de los proxies 
	4- terminar el DVS 
	

CUANDO MUERE EL AGENTE EN EL SERVER se lo quita del lb_bm_init pero no del lb_bm_nodes
que hacemos con un SERVER que esta arrancado pero que el agente murio ??
- se lo trata de arrancar ? es el mas sencillo, habria que monitorear el tiempo
- se rearranca la VM? Es bastante drástico x q puede haber servicios corriendo
	Posible solucion:
		- se trata de arrancar el AGENT
		- si despues de un tiempo dado no hizo el JOIN 
			Se mata la VM 
			
En el mensaje HELLO o en el CMD_T en general se podría acarrear info sobre el nodo 
		- uso de CPU
		- uso de memoria 
También se puede utilizar el estado proxy de cada NODO como failure detector.
De esta forma no se utiliza el agente para colectar la info ni como detector de fallos
Habria que ver la forma en que desde el LB se le mandan ordenes a los proxies 
El multiproxy tiene un thread que captura la info de CPU cada 30 segundos 
y es la que los proxies individuales reportan en los HELLO
PROBLEMA: Que pasa si los proxies estan saturados??? no va a haber HELLO!! 
	SOLUCION: Como insertar HELLO en los proxies 
				Hacer una cuenta de tiempo, si pasaron 30 segundos 
				Enviar HELLO 
VER EN LOS PROXIES DEL LB QUE DIFERENCIAS TIENE EL SOCKET 
	EN TIPO DE ERROR PARA CUANDO
		1- EL EQUIPO REMOTO ESTA APAGADO
			ERROR: lb_cltpxy.c:clt_Sproxy_connect:968: rcode=-110		Connection timed out
			ERROR: lb_svrpxy.c:svr_Sproxy_connect:728: rcode=-110		Connection timed out
		2- EL EQUIPO REMOTO ESTA ENCENDIDO PERO EL SU PROXY NO ARRANCO 
		3- EL EQUIPO REMOTO Y PROXY ESTAN ARRANCADOS
		4- SE CAE EL PROXY
			EL LB NO DETECTO NADA!!!
				EL LB MAIN DEBERIA ENVIAR PERIODICAMENTE PAQUETES HELLO POR CADA PROXY 
				DEBERIA TENER UN TIEMPO DE LAST_HELLO.
				PARA QUE SALTE EL ERROR.
		5- SE CAE EL NODO 
		
		
				
==========================================================================================
==========================================================================================

		
SEGURAMENTE, cuando se termine una VM hay que dejar el server en estado STOPPING 
El cual finaliza no bien se hace el DISCONNECT de spread 
Pero tambien cuando termina la VM!! 
Y tambien hay que terminar el ssh y sshpass
			
TODO_: Modificar test_lb.sh para que soporte parametros
		start y stop y restart 
	
TODO:	Cambiar para que el agente utilice 
			- fork y exec para ejecutar el comando remoto
			- kill en lugar de system(KILL)

ERROR EN DVK 
	Cuando mato a los procesos, lo procesos remotos bindeados quedan iguales 


 ARRANCA ASI 	
root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name
 0  21    21   736/736    1    8   20 31438 27342 27342 27342 m3ftpd         
 0  22    22   737/737    1    8   20 31438 27342 27342 27342 m3ftpd         
 0  23    23   738/738    1    8   20 31438 27342 27342 27342 m3ftpd          
 0  60    60    -1/-1     2 1000    0 27342 27342 27342 27342 m3ftp0         
 0  61    61    -1/-1     2 1000    0 27342 27342 27342 27342 m3ftp1         
 0  62    62    -1/-1     2 1000    0 27342 27342 27342 27342 m3ftp2 
 
 MURIERON EL 20,21 Y 22 y sin embargo no fueron eliminados del getf
 root@node1:/usr/src/dvs/dvk-tests# cat /proc/dvs/DC0/procs
DC pnr -endp -lpid/vpid- nd flag misc -getf -sndt -wmig -prxy name 
 0  60    60    -1/-1     2 3000    0    21 27342 27342     2 m3ftp0         
 0  61    61    -1/-1     2 3000    0    22 27342 27342     2 m3ftp1         
 0  62    62    -1/-1     2 3000    0    23 27342 27342     2 m3ftp2

#define BIT_REMOTE		12
#define BIT_RMTOPER		13	
sumando los 2 da 0x3000 que es el estado del proxy 2
De todos modos no removio el campo getf 

TODO: Agregar como parametro al LB un min_servers que indica la cantidad minima de 
servers a conservar y max_servers 
lb node0 {
	nodeid 			0;   // nodeid 
	lba_unloaded	30;  // limite inferior de carga 
	lba_saturated	70;  // limite superior de carga 
	lba_period		30;  // periodo de sensado 
	start_period	30;	 // periodo para lanzar un nuevo server node si todos estan saturados 
	shutdown_period 120; // periodo para bajar un server node si no tiene actividad suficiente
	min_servers 	1;
	max_servers		4;
};

TODO: EL LB se deberia lanzar a ejecutar con el SPREAD y el modulo DVK arrancados
	Para luego hacer estas verificaciones contra los parametros del DC y no contra las constantes.
	ATENCION HAY QUE CORRER EL AGENT EN LOS NODOS SERVER 

	// check that src endpoint be a CLIENT ENDPOINT
	if( hdr_ptr->c_src < (dvs_ptr->d_nr_sysprocs-dvs_ptr->d_nr_tasks)
	||  hdr_ptr->c_dst < 0  
	||  hdr_ptr->c_dst >= (dvs_ptr->d_nr_sysprocs-dvs_ptr->d_nr_tasks)) {
		rcode = clt_Rproxy_error(clt_ptr, EDVSENDPOINT);
		if( rcode < 0) ERROR_PRINT(-errno);
		return(NULL);
	}

TODO: MODIFICAR m3ftp y m3ftpd para que manejen buffers de MAXCOPYBUF
	LISTO 
	
TODO: + ver si funcionan los proxies con dynamic binding, batched y compresion

ERROR 
	ATENCION PROBLEMA DEL PROXY EN NODE1 !!!
	root@node1:/usr/src/dvs/dvs-apps/dvs_lb# top
	top - 01:30:40 up  2:33,  2 users,  load average: 2,00, 2,32, 2,68
	Tasks:  97 total,   2 running,  95 sleeping,   0 stopped,   0 zombie
	%Cpu(s): 44,4 us, 55,6 sy,  0,0 ni,  0,0 id,  0,0 wa,  0,0 hi,  0,0 si,  0,0 st
	KiB Mem :  1028268 total,   245860 free,    55932 used,   726476 buff/cache
	KiB Swap:   522236 total,   522236 free,        0 used.   328040 avail Mem 
	  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                    
	  739 root      20   0   11532    140     12 R 99,0  0,0  68:36.67 lz4tcp_proxy_ba <<<<<<<<<<<<<<<<
  
    SEGURAMENTE DEBE SER EN UN WHILE DESPUES DE FALLAR LA COMUNICACION

COMO SUSPENDER UNA VM EN VMWARE WORKSTATION
CD C:\Program Files (x86)\VMware\VMware Workstation>
vmrun -T ws suspend "J:\Virtual Machines\Debian 9.4 NODE1\Debian 9.4.vmx" hard 
para rearrancar
vmrun -T ws start "J:\Virtual Machines\Debian 9.4 NODE1\Debian 9.4.vmx"  

PARA ARRANCAR VM EN VMWARE PLAYER 
@echo off
PATH "C:\Program Files (x86)\VMware\VMware Player\"
START vmplayer.exe "D:\PAP\Virtual Machines\Debian 9.4\Debian 9.4.vmx"


COMO MEDIR LA CPU 
Try reading /proc/loadavg. 
The first three numbers are the number of processes actually running (i.e., using a CPU),
averaged over the last 1, 5, and 15 minutes, respectively.

cat /proc/loadavg | awk '{c=$1}END{print 100*(c)}';



TODO:	Los test a realizar son?
			- latencia  con server persistente
			- throughput con server persistente 
			- startup    con server efimero 
			
LATENCIA:
		ELIMINAR LOS DEBUGS DE:
			- la libreria 
			- los proxies 
			- las aplicaciones
		Revisar el tests.sh que los proxies arranquen SIN AUTOBIND y SIN BATCH 
			/usr/src/dvs/dvk-proxies/lz4tcp_proxy_bat -ZP -n node$rmtA -i $rmtA > /dev/shm/node$rmtA.txt 2> /dev/shm/error$rmtA.txt &	
			/usr/src/dvs/dvk-proxies/lz4tcp_proxy_bat -ZP -n node$rmtB -i $rmtB > /dev/shm/node$rmtB.txt 2> /dev/shm/error$rmtB.txt &	

		LOCAL: 
			Arrancar el dvs en NODO0 (CLIENT) 
				cd /usr/src/dvs/dvk-tests
				./tests.sh 0 0 
				. /dev/shm/DC0.sh 
			ejecutar el server en endpoint 9 
				 nsenter -p -t$DC0 ./latency_server 0 9 
			ejecutar el client en endpoint 50 indicando 1000 loops
				nsenter -p -t$DC0 ./latency_client 0 50 9 1000 > latency_local.out 
			Para obtener TODAS las latencia
				grep elapsed latency_local.out
			
		REMOTO: 
			Arrancar el dvs en NODO1 (SERVER) 
				cd /usr/src/dvs/dvk-tests
				./tests.sh 1 0 
				. /dev/shm/DC0.sh 
			Usage: ./test_rmtbind <dcid> <p_nr> <nodeid> <name>
				./test_rmtbind 0 50 0 client 
			Ejecutar el server en NODE1  
				nsenter -p -t$DC0 ./latency_server 0 10
			En NODE0 bindear el server remoto en NODE1
				./test_rmtbind 0 10 1 server 
			ejecutar el client en endpoint 50 indicando 1000 loops
				nsenter -p -t$DC0 ./latency_client 0 50 9 100 > latency_remote.out 
				
		CON LOAD BALANCER Y SERVER PERSISTENTE
		
		CON LOAD BALANCER Y SERVER EFIMERO 
		
//////////////////////////////////////////////////////////////////////////////////
FALTA RESOLVER COMO PROBAR LAS CAPACIDADES DE LOAD BALANCING  
Y LAS DE AUTOSCALING 		
//////////////////////////////////////////////////////////////////////////////////
						
TODO:	Agregar un contador de sesiones por nodo SERVER para no cambiar de server hasta
		que el primero se satura y no LIBERAR un server hasta que su carga es < lowwater
		LISTO 
		
TODO:	Los clientes y servers podrian trabajar en redes IP diferentes para no comunicarse entre si
		y en DVSs diferentes.
		El LB debaria ser el NODE0 de ambos DVS
		Para ello habria que configurar al LB con 2 placas de red 
		

TODO: Quizas tenga que dividir entre SERVER NODE y SERVER SERVICE 
		En un mismo NODO pueden correr varios SERVICEs.
		LISTO 
		
TODO: Como enviar un UNICAST 
		
TODO:  Como obtener el uso de CPU ? LISTO 

TODO: VER thread multijoin 
https://man7.org/tlpi/code/online/dist/threads/thread_multijoin.c.html

TODO: Cuando se conecta el CLIENT y no hay ningun SERVER retornar EDVSNOTBIND
#define EDVSNOTBIND 	(_SIGN 310)  /* The process has not BINDed */
#define EDVSDEADSRCDST  (_SIGN 105)  /* source or destination is not alive */
#define EDVSNOTREADY    (_SIGN 106)  /* source or destination is not ready */
#define EDVSSRCDIED     (_SIGN 108)  /* source just died */
#define EDVSDSTDIED     (_SIGN 109)  /* destination just died */

ATENCION MANEJAR LOS DIFERENTES ERRORES 
- CLIENT SE CAE DESPUES DE HABAR INICIADO 
- SERVER SE CAE 
- NODO CLIENTE SE CAE
- NODO SERVER SE CAE 
- SERVER MIGRA 
- SERVER SE REPLICA 

				////////////////////////////////////////////////
				// DEBERIA ENVIAR A LA COLA DE MENSAJES DEL
				// SERVER PROXY SENDER UN MENSAJE DE TIPO 
				//  cmd  | MASK_ACKNOWLEDGE
				//  CON CODIGO DE ERROR  EDVSDEADSRCDST
				////////////////////////////////////////////////
				

config file Kees J. Bot

 


Diego Ongaro , John Ousterhout,"In Search of an Understandable Consensus Algorithm",2014 USENIX Annual Technical Conference, ISBN 978-1-931971-10-2, 2014, 









 				

		









	
	